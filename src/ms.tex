% Define document class
\documentclass[12pt,dvipsnames]{report}
\usepackage{aas_macros}

% Citations
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear}

% Language 
\usepackage[utf8x]{inputenc}

% Layout and margins 
\usepackage[margin=1in]{geometry}
\usepackage{layout}
\usepackage{lastpage}
\usepackage{titling}

% Figures & tables 
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % much better looking tables

% Math 
\usepackage{amsmath,amssymb,bm}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{siunitx} % for proper units 
\usepackage{esint} % various fancy integral symbols
%\usepackage[bbgreekl]{mathbbol}

% Algorithms 
\usepackage{algorithm}
\usepackage{algpseudocode}

% Other packages
\usepackage{url}
\usepackage{epstopdf}
\usepackage[font=scriptsize]{caption} % smaller font for fig captions
\usepackage[section]{placeins}
%\usepackage{import}
% of references  

% Links 
\hypersetup{
    hidelinks,
    colorlinks=true,
    allcolors=NavyBlue,
}

% Settings
\hfuzz=6pt % tolerance on hbox overflows

% Custom command
\newcommand{\ud}{\,\mathrm{d}}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\hquad}{~~}

% Begin!
\begin{document}

% Title
\title{\vspace{-15pt}\begin{center}\includegraphics[width=0.4\linewidth]{static/misc/logo.jpg}\end{center}
    \vspace{20pt}
    \begin{center}
        School of Physics \& Astronomy
    \end{center}
    \vspace{20pt}
    \begin{center}
        \LARGE\textbf{Probabilistic modeling of astrophysical time series:
            gravitational microlensing and occultation mapping of planets and moons}
    \end{center}
    \vspace{20pt}
    \begin{center}
        Supervisor: Dr. Martin Dominik
    \end{center}
}

\maketitle
\tableofcontents

% Abstract 
\chapter*{Abstract}

Scientific progress in modern astronomy research commonly relies on gathering
large quantities of data using exceedingly precise instruments. The process
which ``generates'' these data consists of the physical phenomenon of interest
-- for instance, an exoplanet blocking or twisting the light of a distant star,
and the noise introduced by the measurement process and the presence of an
atmosphere The task for an astronomer is to first construct a \emph{model}
which describes the entire process which generated the data and to then ``fit''
that model to data. All models only approximate reality and the researcher has
to make a series of decisions during the model building process, everything
from how to process the raw data to which results to put in the abstract of a
paper. Advancements in computational statistics and machine learning in the
past decade or so have made it possible to fit ever more complex models to
data. These models are generally expressed in computer code which may contain
complex numerical algorithms, such as iterative solvers and numerical
integrals. In this thesis, I mostly focused on developing methods which enable
\emph{statistical inference} with these kinds of complex models in two
particular domains within astronomy, gravitational microlensing and occultation
mapping. The common theme between these two topics is that both deal with
accurately measuring the brightness of distant stars as a function of time with
the goal of inferring properties of exoplanets and stars. Broadly speaking, I
believe the biggest contribution of this thesis is providing a new lens for
looking at a particular set of old problems, a lens which incorporates recent
advancements from statistics, machine learning and computer science. More
specifically, I have developed a an open-source software package
\textsf{caustics}\footnote{\url{https://github.com/fbartolic/caustics}} which
enables fast and accurate computation of binary lens and triple lens
microlensing light curves and simultaneously provides exact \emph{gradients} of
the code outputs with respect to all its inputs. This is significant because it
for the first time enables the use of modern gradient based statistical
inference algorithms such as Hamiltonian Monte Carlo with microlensing light
curves. Microlensing is one of the major goals for the upcoming \emph{NASA
    Roman} telescope and the existing modeling methods are completely inadequate
for dealing with the scale of data which will come from \emph{Roman}. I also
propose a framework for dealing with issues which have plagued the field for
decades -- various pathologies in microlensing models and questions about the
interpretation of statistical results. Besides microlensing, I have also delved
into the field of occultation mapping of Solar System objects and exoplanets.
Together with collaborators, I have developed a novel statistical method for
reconstructing spatial maps of volcanic emission on Jupiter's moon Io using
infra-red occultation light curves. I applied the same method to exoplanets to
explore the exciting possibility of detecting weather changes on Hot Jupiters
by reconstructing two dimensional maps of the emission surface from simulated
\emph{JWST} secondary eclipse light curves. I found that planetary scale
changes in the emission pattern should be detectable with \emph{JWST}.

\newpage
\thispagestyle{empty}

% CHAPTER 1: Introduction
\chapter{Introduction}
\section{Context}

The key idea of the scientific revolution in the 16th century was to carefully
observe the world, build \emph{models} which describe some aspect of the
observed phenomenon, and finally and most importantly -- test those models to
see if they provide an accurate description of reality. Since the time of
Galileo, this process gave rise to the modern world and revolutionised our
understanding of the universe. Today we have amazingly accurate models of the
universe. Einstein's theory of \emph{General Relativity} which describes the
universe at a large scale and the \emph{Standard Model} which describes the
universe at the atomic and subatomic scales, both of which were developed in
the 20th century. In my opinion, the two most important questions for the
physical sciences in the 21st century and beyond are the following:
\begin{itemize}
    \item How do we combine the Standard Model and General Relativity to get a complete
          physical theory of the Universe?
    \item What is the origin and distribution of complex life in the Universe?
\end{itemize}

It used to be the case that conducting experiments with the potential to to
change our understanding of fundamental physics was relatively straightforward
and could be conducted by a single person or a small group of researchers. I
have in mind, for instance, Kepler's observations of planetary orbits, Ernest
Rutherford's experiments with atoms or Arthur Eddington's observations of the
Solar eclipse with the goal of testing General Relativity. The data gathering
process consisted of writing notes in a physical notebook and the analysis
hardly required complex statistics. Today the situation is more complicated
because much of the ``low-hanging fruit'' of scientific discovery (in the
physical sciences) has been exhausted and the use of computers is absolutely
central to the process. Progress today usually (but not always) requires
coordination between many scientists, engineers and software engineers building
highly complex experiments. Consider this list of some of the most notable
scientific discoveries in the past few decades:
\begin{itemize}
    \item  Discovery of accelerated expansion of the Universe
    \item  First sequencing of the human genome
    \item  Detection of the Higgs Boson
    \item  Paleogenomics studies of the origin of Homo Sapiens
    \item  Detection of gravitational waves by LIGO
    \item  Reconstruction of the first image of a Black Hole
\end{itemize}
All of these discoveries required gathering substantial amounts of data and the use of
relatively complex statistical analysis techniques.
Computation and statistical analysis are now absolutely central the process of scientific
discovery.

Simultaneously, in the past decade there's been a complete revolution in the
field of machine learning/AI thanks to the advent of deep learning and neural
networks. Besides the incredible progress on predicting patterns in language
and vision, deep learning has also been used in science for things like solving
the protein folding problem \citep{2021Natur.596..583J} and in mathematics for
the purpose of discovering novel conjectures and theorems
\citet{2021Natur.600...70D}. Deep learning has been less useful in physics and
astronomy so far but as I will argue in this thesis, some of the technologies
which underlie deep learning such as automatic differentiation and GPU
computing are going to be (if they aren't already) crucial for processing and
understanding complex datasets in physics and astronomy.

\begin{figure}
    \begin{centering}
        \includegraphics[width=0.8\linewidth]{figures/ads_keyword_bayesian.pdf}
        \caption{Number of NASA/ADS entries containing the keyword ``Bayesian'' per year.}
        \label{fig:ads_keyword_bayesian}
    \end{centering}
\end{figure}

There is another revolution worth mentioning. It is slightly less obvious than
the one in machine learning but nevertheless significant. In the past two
decades there has been a substantial increase in popularity of Bayesian
statistics. Fig~\ref{fig:ads_keyword_bayesian} shows that number of entries in
the NASA/ADS database containing the keyword ``Bayesian'' each year is growing
almost exponentially. One of the reasons that these methods are so popular now
even though they have been invented many decades ago is that they used to be
very computationally expensive and the algorithms necessary to do proper
Bayesian analysis were somewhat underdeveloped. This has changed drastically in
the past decade. I will discuss these methods in detail in
\S~\ref{sec:bayesian_statistics}. Much of this thesis is about applying
Bayesian methods to problems in astronomy.

Having situated the work presented this thesis in the present moment and point
out some scientific and technological changes that I think are relevant, I will
now focus on astronomy in particular. The kinds of questions in astronomy that
most excite me are those which lead us closer to answering one of those two
fundamental questions I stated at the beginning of this chapter. The question
about the origin of life in the universe in particular. Since the first
discoveries of planets outside of our Solar System in the early 90s
\citep{1992Natur.355..145W,1995Natur.378..355M} thousands more have been
confirmed\footnote{At the time of writing the
    \href{https://exoplanetarchive.ipac.caltech.edu/}{NASA Exoplanet Archive}
    contains more than 5000 exoplanets.} using methods such as transits, radial
velocity, microlensing and direct imaging. Thanks to gravitational microlensing
we now know \citep{2012Natur.481..167C} that there is an average one planet per
star in the Milky Way. In addition to detecting the presence of the planets and
inferring their properties such as mass, radius, and orbital period; it is now
possible to measure the transmission and emission spectra of their atmospheres
and even reconstruct crude maps of their surfaces\footnote{The subject of
    Chapter~\ref{ch:mapping_exoplanets}.}
\citep{2007Natur.447..183K,2012ApJ...747L..20M}. With the James Webb Space
Telescope, we might even be able to detect biosignatures in the atmospheres of
Earth size planets.

Answering a grand question such as ``are there biologically produced complex
molecules in this exoplanet atmosphere'' will not be easy even with
cutting-edge instruments such as the James Webb Space Telescope. It will also
almost certainly not be a clear yes or no kind of answer; rather, it will
require a deep understanding (i.e. good \emph{models}) of the physics of
exoplanet atmospheres, stellar variability, the response of the instrument,
sophisticated statistical analysis of the drawing on multiple independent
pieces of evidence and clear definitions of what it means to have detected
something. To build a good model for the thing we really care about requires
understanding also the the things we may intrinsically care less about
(instrumental systematics, details of stellar variability, variations in
Earth's atmosphere etc.).

A large fraction of this thesis (with the exception of
Chapter~\ref{ch:mapping_exoplanets}) is focused on building the necessary
infrastructure (computation, statistics, interpretation) which should enable
new discoveries in exoplanet (and planetary) science. Wherever possible I try
to approach these problems from first principles thinking but with a heavily
computational/statistical approach with a healthy dose of pragmatism. In the
next two sections I will provide a brief introduction to specific areas I
worked on and end the chapter with an outline of the following chapters.

\section{Gravitational microlensing}
\emph{``Do not bodies act upon light at a distance and by their action bend its rays,
    and is not this action strongest at the least distance?''} asked \citet{Newton1704}.
Much later in 1911, even before he published his theory of General Relativity (GR), Einstein
considered the deflection of light from a distant star passing close
to a massive object in the foreground and derived an expression for the deflection angle of the light ray
which was off by a factor of 2 relative to the correct value.
He corrected the error in 1915 using GR.
The same year Albert Einstein published his General theory of Relativity (1916),
English physicist Sir Oliver Lodge suggested the light bending phenomenon could produce
a \emph{gravitational lens}.
He warned that it wouldn't be a lens in the usual sense,  because it does not have a
focal length.

The first experimental confirmation of the deflection of light by a mass came
in 1919 during the Solar eclipse when the astronomer Arthur Eddington famously
measured the deflection of light from distant stars passing close to the limb
of the Sun \footnote{He concluded that the observed deflection was in agreement
    with GR, although it is doubtful that the data was actually good enough to
    distinguish between the GR prediction for the deflection of light and the
    Newtonian prediction which is derived using the equivalence principle and is
    equal to one half the GR value.}. Much later in 1936 a young amateur scientist
R.W. Mandl convinced Einstein to write a short a paper on the lensing effect by
a massive star acting as the lens instead of the Sun. In the paper
\citep{1936Sci....84..506E} Einstein derived an expression for the
magnification of the distant star when it is closely aligned to the foreground
lens star relative to an observer and he predicted that a luminous ring would
form if the two stars were perfectly aligned. He considered this effect curious
but useless, stating that \emph{``Of course, there is no hope of observing this
    phenomenon directly. First, we shall scarcely ever approach closely enough to
    such a central line. Second, [the angles] will defy the resolving power of our
    instruments''}.

In this matter, Einstein was wrong. The famous astronomer Fritz Zwicky noticed
the phenomenon is likely to be observable at galactic scales if the lens was a
massive galaxy \citep{1937PhRv...51..290Z,1937PhRv...51..679Z} and that one
could use the measurement of the deflection angle to weigh the lens. Zwicky was
right and the first definitive observation came in 1979 by
\citet{1980ApJ...241..507Y} who observed a double image of the quasar Q0957+561
and concluded that the two images correspond to the same object whose light was
distorted by a massive galaxy. Many other observations followed, including
visible Einstein rings. Far from being just a curious phenomenon, galactic
scale lensing is now one of the key methods of observational cosmology, used
for inferring the parameters of the standard model and the distribution of dark
matter in the universe.

\begin{figure}
    \begin{centering}
        \includegraphics[width=0.5\linewidth]{static/microlensing/crowded_field.jpg}
        \caption{
            Stellar field of a microlensing event GLE-2012-BLG-0406 (centered),
            imaged by one of Las Cumbres Observatory's 2m telescopes,
            showing the high density of stars typical in microlensing observations.
            Credit: Y. Tsapras. Taken from
            \url{http://microlensing-source.org/pictures/}.}
        \label{fig:crowded_field}
    \end{centering}
\end{figure}

That the same effect could be used to detect the presence of planets orbiting
around the lens star was first theorized by \citet{1964PhRv..133..835L} who
wrote that \emph{``the primary effect of planetary deflectors bound to stars
    other than the Sun will be to slightly perturb the lens action of these
    stars''} although he was also sceptical about the possibility of detection,
saying that \emph{``associated pulses would be so weak and infrequent and of
    such fleeting duration – perhaps a few hours – as to defy detection''}.
Gravitational lensing as a method for discovering exoplanets really took off
with the work of Paczy\'nski
\citep{1986ApJ...304....1P,1986ApJ...301..503P,1991ApJ...374L..37M} who also
coined the term \emph{microlensing}, which refers to gravitational lensing in a
regime where the images of the background object cannot be resolved but one can
nevertheless measure its magnification as a function of time. Microlensing as a
method for detecting exoplanets has some unique aspects. First, it is a one-off
event which happens on a timescale of a few minutes up to several months
depending on the distances to the background star and the lensing star and the
mass of the lens. In addition to the fact that there's only one chance to
observe such an event for it to happen at all requires extremely precise
alignment between the lens and the background star and the chance of observing
a stellar microlensing event is about one-in-a-million (CITE) for a typical
star within the Milky Way. To observe a planetary signal is about an order of
magnitude less likely than that. Hence, obtaining a decent sample of planetary
events requires continuous monitoring on the order of $10^{8}$ stars at the
time which way microlensing observations that to focus on the densest region of
the Milky Way -- the galactic bulge. Figure~\ref{fig:crowded_field} a picture
from of such a dense stellar field. Finally, in the vast majority of cases we
do not detect any light from the lens itself, the collected photons are from a
background star completely unrelated and distant from the lensing star. This is
very different from other exoplanet discovery methods and it means that we only
obtain dynamical properties of planets such their masses and periods.

The aspects that make microlensing events that make them difficult to observe
also mean that it provides a unique lens onto exoplanet systems. Relative to
other methods such as transits and radial velocity, microlensing is sensitive
to planets located at substantially greater distances, well outside of our
Solar System neighborhood and even potentially to Milky Way's satellite
galaxies such as the Magellanic Clouds and the nearby Andromeda galaxy (CITE).
Microlensing is also sensitive to very to very small planets and planets which
are further out from the star than those typically detected using transits and
radial velocity. Microlensing surveys such as OGLE \citep{1993AcA....43..289U}
and MOA \citep{1999PThPS.133..233M} have been continuously monitoring crowded
stellar fields in the Milky Way since the 90s\footnote{Initially the focus was
    on finding dark matter candidate particles -- so called MACHOs (Massive Compact
    Halo Objects).} discovering thousands of stellar events and dozens of planetary
events. In order to detect planetary deviations in the observed light curve it
essential to have high cadence observations of the source star. The way surveys
have traditionally worked is that once a particular star started to become
magnified many additional small telescopes would start observing it to obtain
denser coverage and sometimes space-based observatories get involved as well.
The vast majority of microlensing events analyzed so far consist of
observations from multiple observatories, each with its own unique aspects such
as noise properties, cadence and photometric quality.

Future surveys such as the ground based Rubin Observatory
\citep{2019ApJ...873..111I} telescope and the space based Roman Telescope
\citep{2019ApJS..241....3P} and Euclid \citep{2022arXiv220209475B} will detect
tens of thousands of events in total. Although most of past work in the field
focused on characterizing individual events binary lens events, answering
questions about \emph{populations} with these new (but also existing) datasets
requires scalable data analysis methods and algorithms and a clear set of
guidelines on how to interpret the analysis products. This is a substantial
challenge because microlensing events are notoriously difficult to model. Even
though the datasets are relatively simple, consisting of multiple time series
photometric light curves in different bands, the parameter space of even the
simplest models is highly non-linear, correlated, relatively high dimensional
and there are often near perfect degeneracies in the solutions.

The assumptions that existing methods for modeling microlensing events rely on
are often opaque and unquestioned. Discussions on model ``degeneracies''
\citep{2014MNRAS.437.4006S,2019AJ....157...23H,2018AcA....68...43S,2009MNRAS.393..816D},
correlated noise \citep{2015ApJ...812..136B,2019MNRAS.488.3308L} and model
comparison \citep{2018AJ....155..259H,2019MNRAS.484.5608D} have been ongoing in
the microlensing literature for decades without a clear solution and proper
framing of the issue. In this thesis I will revisit these kinds of questions
while taking into account many recent developments in the fields of
computational statistics and machine learning. My main contributions are the
following:
\begin{itemize}
    \item I wrote an open-source Python package \textsf{caustics} which enables fast and
          accurate computation of binary and triple lens microlensing light curves using
          contour integration. The code runs on both CPU and GPU architectures, and
          crucially, for reasons that I will elaborate in subsequent chapters, it
          supports \emph{automatic differentiation} of all outputs with respect to all
          input parameters which enables the use of statistical methods which are orders
          of magnitude more efficient than existing approaches. The code can easily be
          extended to support quadruple lensing and arbitrary intensity profiles for the
          source star.
    \item I have revisited the topic of degeneracies in the posterior probability
          distributions which appear in the context of microlensing and propose solutions
          to these problems.
    \item I have extended the complex polynomial root solver from \citet{Cameron2021}
          such that it can be executed on GPU architectures and that it supports
          automatic differentiation. This enables the evaluation of ~1M lens equation
          solutions for point source binary and triple lenses in seconds using a CPU and
          miliseconds using a GPU.
    \item I propose an approach to statistical inference on a population of microlensing
          events which different from existing methods and has some advantages.
    \item I briefly discuss the issue of correlated noise in microlensing light curves
          and propose ways to account for it using Gaussian processes.
\end{itemize}

\section{Occultation and phase curve mapping}
Interestingly, the second topic I will cover in this thesis has a history not
unlike that of microlensing. It was proposed in the early 20th century and it
was only much later that technology caught up with the idea. Back in 1906,
\citet{1906ApJ....24....1R} pointed out that certain features in light curves
of Solar System satellites may be attributed to inhomogeneities of their
surfaces. The key idea is that although at any given time we only observe the
total light from an unresolved satellite or planet, different portions of the
surface are visible at different times so we may expect that some of the
information about the surface intensity (be it emission from heat or reflected
Sunlight) gets imprinted onto the light curve. \citet{1906ApJ....24....1R} also
considered the inverse problem -- can we learn something about the surface of
these objects starting from a light curve? The method he proposed is now known
as \emph{phase curve mapping} and it was first attempted by
\citet{1972ApJ...174..449L} who analyzed photometric light curves of Pluto in
reflected light attempting to constrain variations in the \emph{albedo} of the
surface with inconclusive results.

Later works such as
\citet{1986AJ.....92.1201D,1992Icar...97..211B,1993Icar..102..134Y,1999AJ....117.1063Y}
went a step further by using not only phase curves but also light curves of
mutual \emph{occultations} of Pluto by its moon Charon to reconstruct albedo
maps with greater success. The major advantage of occultations relative to just
phase curves is that more information about the surface is encoded into the
light curve because of the sharp limb of the occultor sweeping over the disc of
an occulted body and blocking the reflected light. More importantly for this
thesis, \citet{1994Icar..107..195S} was the first to observe the occultations
of Jupiter's moon Io by another of Jupiter's moons -- Europa, and also
occultations of Io by Jupiter itself. These observations were conducted using
near-infrared telescopes such as NASA's Infrared Telescope Facility (IRTF) to
observe emitted light from Io's surface which is covered with many time-varying
and bright volcanic features. The observing campaign of Io has yielded insights
into the the nature of its volcanic activity and it continues to this day. I
will return to the subject of Io in great detail in
Chapter~\ref{ch:mapping_io}.

A natural question arises, can we do this with objects outside of the Solar
System? The answer is yes. \citet{2007Natur.447..183K},
\citet{2012ApJ...747L..20M}, and \citet{arXiv:1202.3829} used Spitzer
mid-infrared observations of secondary eclipses of the Hot Jupiter HD189733b
and found that surface emission is best described by the presence of a large
hot spot on the dayside of the planet which is longitudinally offset from the
substellar point. Similarly, \citet{2014Sci...346..838S}, produced temperature
maps of the Hot Jupiter WASP-43b, \citet{2013ApJ...776L..25D} mapped the Hot
Jupiter Kepler-7b in reflected light and \citet{2016Natur.532..207D} mapped the
thermal emission from the Super Earth 55 Cancri e. These studies were only able
to capture longitudinal variations in intensity. Real exoplanet atmospheres of
are certain to have three-dimensional spatial inhomogeneities in emission more
complex than a single hot spot due to the presence of clouds, zonal jets,
storms, waves etc. \citep{2020SSRv..216..139S}.

In recent years there have been significant advances in statistical modeling of
phase curves and eclipse light curves. Most notably,
\citet{2019AJ....157...64L} introduced the \textsf{starry} code which enables
analytic computation of phase curves and occultation light curves for bodies
with arbitrary emission maps expressed in a spherical harmonic basis (an idea
dating back to \citet{1906ApJ....24....1R} ) and \citet{2021arXiv210306275L}
expanded the algorithm for the (considerably more complicated) case of
reflected light. In this thesis, I will present the work I've done in
collaboration with other researchers from the planetary science and exoplanet
communities on using \textsf{starry} to map the surface of Io and investigate
the prospects for detecting fine spatial structure in the atmospheres of Hot
Jupiters using JWST phase curves and eclipse light curves. My contributions to
this area are the following:
\begin{itemize}
    \item I have developed a novel model for inferring emission maps of Io from
          occultation light curves. First part of this project is published in
          \citet{2022PSJ.....3...67B} though I haven't managed to complete the second
          part.
    \item I have investigated the problem of eclipse mapping of exoplanets using the
          \textsf{starry} framework, focusing in particular on the possibility of
          detecting planetary scale storms on Hot Jupiters using JWST. I found that it
          extremely difficult to infer maps of higher resolution than a dipole order but
          that should still be sufficient for detecting large scale weather and climate
          change in the atmospheres of these planets.
\end{itemize}
The application of the occultation/eclipse mapping method to Io can be seen as the
best case scenario for the application of the same method to exoplanets.
I will show that even in the case of observing a bright object right in our neighborhood,
this is by no means and easy task. Our ability to reconstruct the surface features of
Io sets a sort of upper limit on what is possible with exoplanets.

%\section{Outline}
%The outline of the thesis is as follows. Chapter~\ref{ch:theoretical_min}
%contains the theoretical background needed to understand the subsequent
%chapters, covering the theory behind microlensing and occultation mapping, a
%summary of Bayesian statistics in particular and statistical inference more
%generally, and probabilistic programming which ties everything together.
%Chapter~\ref{ch:microlensing} is the most extensive chapter in the thesis
%covering all of my work on microlensing. I start by discussing the case of
%single lens microlensing models, showing that it contains many of the
%properties of more complex models. In particular I focus on the issue of
%multi-modal posterior distributions which is a ubiquitous problem in
%microlensing. I propose several solutions to this and demonstrate their
%effectiveness. I then move on to the considerably more complex case of binary
%and triple lens models starting with numerical methods for solving the lens
%equation (finding roots of a complex polynomial equation). I demonstrate the
%advantage of the Ehrlich-Aberth algorithm over previous methods and explain how
%to make this algorithm differentiable and orders of magnitude faster through
%the use of GPUs. Next, I cover the contour integration algorithm for computing
%the magnification of a limb-darkened source star and its implementation in
%\textsf{caustics}.

% CHAPTER 2: The theoretical minimum
\chapter{The theoretical minimum}
\label{ch:theoretical_min}

\section{Gravitational microlensing}
\label{sec:microlensing}
Gravitational lensing is generally divided into multiple classes depending
on weather the effect is discernible at the level of individual objects or a
a statistical sample of object. The main division is between \emph{weak lensing}
and \emph{strong lensing}.
The former refers to lensing by galaxies and clusters of galaxies on cosmological
scales where the deflection  is impossible to detect in a single background source
but one can tease out the effect in a statistical sense.
Strong lensing refers to lensing of individual objects and it is further divided into
\emph{macrolensing} -- lensing of galaxies where the multiple images of the source
are resolved, and \emph{microlensing}  -- the images are generally not resolved,
and the observable is the total magnification of the source as a function of time (a light curve).
In case of microlensing the light source is either a quasar or a star (sometimes a binary star)
and the lenses are stars, brown dwarfs, planets, and compact objects such as black holes, neutron
stars and white dwarfs. In this thesis we focus specifically on microlensing
with stars as the light source instead of quasars\footnote{Quasar microlensing
    is a somewhat separate community from the rest of the microlensing community.}.

\subsection{Deflection of light by gravity}
Gravitational lensing is a phenomenon fully described by Einstein's General
Theory of Relativity (GR) in which light changes its direction of propagation
when passing close to a massive body. GR predicts the following:
\begin{itemize}
    \item the presences of mass changes the spacetime geometry from the flat (Minkowski)
          metric $\eta_{\mu\nu}$ to a curved metric, specified by the tensor $g_{\mu\nu}$
    \item massless particles such as photons follow the null geodesics -- minimum
          distance paths in curved spacetime which are obtained by solving the
          \emph{geodesic equation}
\end{itemize}
If we restrict ourselves to the regime where the metric is time independent
and the particles are allowed to travel at any velocity less than $c$,
also known as the \emph{weak field} or \emph{linearized approximation }
of GR, the metric tensor
can be written as
\begin{equation}
    g_{\mu\nu}=\eta_{\mu\nu}+h_{\mu\nu}\quad,
\end{equation}
where $h_{\mu\nu}=
    \textrm{diag}(-2\Phi,-2\Phi,-2\Phi,-2\Phi)$\footnote{Assuming the
    the metric sign convention $(-,+,+,+)$} \citep{carroll_2019}.
The (static) Newtonian gravitational potential $\Phi$ obeys the Poisson equation
\begin{equation}
    \nabla^2\Phi=4\pi G\rho\quad,
\end{equation}
where $\rho$ is the mass density of the distribution of mass located between the observer
and the light source.
The geometry of the lensing system consisting of a distant source of light, the
observer, and a distribution of mass with density $\rho$ located between the
source and the observer is shown in Figure~\ref{fig:deflection_angle}.
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{static/microlensing/deflection_angle.pdf}
    \caption{Geometry of gravitational lensing.
        A geodesic curve $x^\mu(\lambda)$ is deflected
        by an angle $\hat\alpha$ from its initial trajectory
        due to the presence of a massive body with mass density $\rho$. Figure
        adapted from \citet{carroll_2019}.}
    \label{fig:deflection_angle}
\end{figure}
A photon is deflected as it travels from a source to an observer by a
\emph{deflection angle} $\hat\alpha$, a vector in the
plane  perpendicular to the wave vector $\vec{k}$ pointing in the direction of
photon propagation.
The deflection angle is given by \citep{carroll_2019}
\begin{equation}
    \hat\alpha=2\int\nabla_\perp\Phi \ud s\quad,
    \label{eq:deflection_angle_general}
\end{equation}
where $\nabla_\perp\Phi$ is the gradient of the potential in the direction transverse
to the path of the photon and $s$ is the spatial distance traveled.
For a point mass $M$, the potential is given by the potential is
\begin{equation}
    \Phi=- \frac{GM}{(b^2+x^2)^{1/2}}\quad,
\end{equation}
where $x$ parametrizes the straight line distance between the observer and the lens and
$b$ is the impact parameter of the light ray.
Integrating from $-\infty$ to $\infty$ (i.e. assuming that both
the source and the observer are located far away from the deflecting mass), we obtain
the deflection angle:
\begin{equation}
    \hat\alpha= \frac{4GM}{c^2b}= \frac{2R_s}{b}\quad,
\end{equation}
where $R_s=2GM/c^2$ is the Schwarzschild radius.
$\hat\alpha$ is directly proportional to the mass of the lens and
it does not depend on the wavelength of the light. It also increases with the proximity of
the light ray to the lensing mass which is the opposite behavior to that of a
classical convex lens for which the deflection angle vanishes at the center of the lens.
The gravitational lens does not have a focal length and so it is not a lens in the usual
sense.
The resulting deflection angle is very small, for example, for the Sun we have
$GM/c^2=1.48\times 10^5\si{\centi\meter}$ (2.95 km), $R=6.96\times
    10^{10}\si{\centi\meter}$ and $\hat\alpha=1.75$ arc seconds. This is the angle
that was claimed to have been observed by Eddington when during the 1919 total
solar eclipse.

Equation~\ref{eq:deflection_angle_general} is only valid when the linearized
approximation of GR is sufficient. In this approximation, the deflection angles
are additive and the total deflection caused by a group massive bodies is just
the sum of the individual deflection angles. The linearized approximation
breaks down when the impact parameter $b$ approaches the Schwarzschild radius
of the lensing mass and the linearized metric is no longer sufficient. Except
for modeling lensing in the close vicinity of a black, the linearized
approximation is sufficient.

\subsection{The magnification of a point source by a point lens}
Consider a system consisting of an observer $O$, a point mass $M$, and a point
light source $S$. The geometry of this system is shown in
Figure~\ref{fig:lens_geometry}. The lens is located in the \emph{lens plane}, a
plane perpendicular to the observer--lens axis, at a distance $D_L$ away from
the source. The light source is located in the \emph{source plane}
perpendicular to the observer--source axis, at a distance $D_S$ away from the
observer, and the observer is in the \emph{observer plane}. The assumption that
the location of the source, observer, and the lens can be parametrized using
angular coordinates (or Cartesian coordinates if the distances to the lens and
the source are known) in their respective planes is justifiable because the
deflection angles involved are very small and the distance between the observer
and any point of interest in the lens plane is approximately constant.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{static/microlensing/lens_geometry.pdf}
    \caption{The geometry of a system consisting of a point mass lens $M$ located at distance
        $D_L$ at angular separation $\beta$ from the observer, and a single light source $S$ located at distance $D_S$, emitting a light ray which gets
        deflected by an angle $\hat\alpha$ from its
        original trajectory. As a result the observer sees the source at angular separation
        $\theta$. Figure adapted from \citet{1992grle.book.....S}.}

    \label{fig:lens_geometry}
\end{figure}
Let $\theta$ be the apparent angular position of the source in the sky with respect to the
observer--lens axis and $\beta$ be the actual angular position of the source.
Assuming a metric for spacetime in between the source and the observer that is
approximately Euclidian\footnote{This is a valid assumption for galactic sources but not for
    extragalactric source such as quasars and galaxies which require a cosmological
    model for the metric.} the distance from the lens to the
source is $D_S-D_L$.

From Figure~\ref{fig:lens_geometry} it follows that the relationship between
the observed and the actual location of the source is given by
\begin{equation}
    \beta=\theta-2 R_s \frac{D_S-D_L}{D_LD_S}
    \frac{1}{\theta}\quad,
    \label{eq:lens_equation}
\end{equation}
which is known as the \emph{lens equation} or sometimes also the \emph{ray-tracing}
equation. If the source and the lens are perfectly aligned with respect to the observer,
then $\beta=0$ and $\theta\equiv\theta_E$ where
\begin{equation}
    \theta_E= \sqrt{ \frac{4GM}{c^2} \left( \frac{1}{D_L} - \frac{1}{D_S} \right)}=
    \sqrt{\kappa M\pi_{LS}}
    \label{eq:angular_einstein_radius}
\end{equation}
where $\kappa \equiv \frac{4 G}{c^{2} \mathrm{au}} \simeq 8.1 \frac{\mathrm{mas}}{M_{\odot}}$
and
\begin{equation}
    \pi_{LS}\equiv\pi_L - \pi_S=\frac{1\mathrm{au}}{D_L}-\frac{1\mathrm{au}}{D_S}
\end{equation}
is the relative lens-source parallax.

The image of the source forms a ring (the Einstein ring) around the lens star
with angular radius $\theta_E$ -- the \emph{Einstein radius}. $\theta_E$
depends on the mass of the lens and the distances to the lens and the source.
The angular Einstein radius sets the characteristic scale of a microlensing
event, it is a natural unit for most microlensing parameters. For an M-dwarf
star lens in the galactic disc ($D_L\sim 3\,\textrm{kpc}$) and a source star in
the galactic bulge ($D_S\sim 8\,\textrm{kpc}$) we have $\theta_E\sim
    1\,\textrm{mas}$ so the typical scale of microlensing is on the order of
miliarcseconds. The images of the source star are only rarely observable using
the most advanced optical interferometers such as the GRAVITY instrument on
ESO's Very Large Telescope whose lower resolution limit is $2\,\textrm{mas}$
\citep{arXiv:1705.02345}. \cite{2019ApJ...871...70D} were the first to resolve
the images of a microlensing event, finding the value of
$\theta_E=1.87\,\textrm{mas}$ using the GRAVITY instrument.

We can rewrite Equation~\ref{eq:lens_equation} by defining new dimensionless
angular coordinates scaled by the angular Einstein radius:
\begin{equation}
    x= \frac{\theta}{D_L\theta_E}, \quad y=\frac{\beta}{D_S\theta_E}
\end{equation}
The lens equation then takes the very simple form
\begin{equation}
    y= x- \frac{ 1}{x}
    \label{eq:lens_equation_dimensionless}
\end{equation}
Equation~\ref{eq:lens_equation_dimensionless} is a nonlinear mapping between the
source plane $y$ and the image plane $x$.
If we solve for $x$, we obtain two solutions for the position of the images, given by

\begin{equation}
    x_\pm= \frac{1}{2} \left( y\pm\sqrt{y^2 +4}\right)
    \label{eq:images_location}
\end{equation}
For non zero $y$ the so called \emph{minor image} is always located
within the Einstein radius ($\lvert x_-\rvert<1$) while the \emph{major image}
is located outside of it ($\lvert x_+\rvert>1$).

Since gravitational lensing does not involve any additional emission or
absorption along a deflected ray of light and since the wavelength of the light
ray does not change when it encounters the lens, the \emph{surface brightness}
(flux density per unit angular area) $I$ of the source image is identical to
the surface brightness of the unlensed source. The flux of an infinitesimal
source is a product of the surface brightness and the solid angle $\Delta
    \omega$ subtended by the source in the sky which changes throughout the
microlensing event because the source and the lens are in motion relative to an
observer. The ratio of the original and the lensed source flux, the
\emph{magnification} $A$, is then given by the ratio of the two solid angles
\begin{equation}
    A= \frac{\Delta \omega}{(\Delta\omega)_0}\quad,
    \label{eq:magnification}
\end{equation}
where $0$ denotes the unlensed solid angle.
Assuming an infinitesimal source size at angular location $\mathbf y =(y_1,y_2)$ subtending an angle
$\Delta (\omega)_0$, and an image of the source at location $\mathbf x=(x_1,x_2)$ subtending an angle
$\Delta \omega$; the ratio between the solid angle of the source and the image
is given by the determinant of the Jacobian matrix of the
lens mapping $\mathbf{x}\rightarrow\mathbf{y}$, evaluated at the location of the images
\begin{equation}
    \frac{(\Delta\omega)_0}{\Delta\omega} =\left\lvert\textrm{det}
    \frac{\partial \mathbf y}{\partial \mathbf x} \right\rvert
\end{equation}
The magnification $A$ is then given by the inverse Jacobian determinant of the lens mapping:
\begin{equation}
    A= \left\lvert\det
    \frac{\partial \mathbf y}{\partial \mathbf x} \right\rvert^{-1} \label{eq:magnification_general}
\end{equation}
The images for which the determinant of the Jacobian of the lens mapping is
positive are said to have positive \emph{parity} and vice versa.

Looking at Equation~\ref{eq:magnification_general}, we see that the
magnification diverges when the Jacobian determinant of the lens mapping
vanishes. Curves in the lens plane for which the determinant of the lens
mapping vanishes, that is,
\begin{equation}
    \left\lvert\det
    \frac{\partial \mathbf y}{\partial \mathbf x} \right\rvert=0
\end{equation}
are called \emph{critical curves}.
The critical curves in the lens plane can be mapped to the source plane using
the lens equation $\mathbf{x}\rightarrow \mathbf{y}$ and these curves are
are called \emph{caustic curves}.
Although the magnification factor in Equation~\ref{eq:magnification_general} formally
diverges at the points of critical curves or caustics,
the divergence is not physical because light sources aren't truly point like.
If the finite angular size of the source is taken into
account, the magnification is an integral of Equation~\ref{eq:magnification_general} over the
extent of the source, weighted by the source brightness and that integral is always finite.
Both the critical and the caustic curves are closed and one can prove that \textbf{the
    number of images changes by two if and only if the source crosses a caustic curve}
\citep[][Chapter 6]{1992grle.book.....S}.

We can derive a simpler expression for Equation~\ref{eq:magnification} by
switching to polar coordinates $(v,\phi)$ in the lens plane. The lens equation
for the two vector components of the apparent source position is given by
\begin{align}
    y_1= & x_1- \frac{x_1}{x_1^2+x_2^2} \\
    y_2= & x_2- \frac{x_2}{x_1^2+x_2^2}
\end{align}
Introducing polar coordinates $x_1=v\cos\phi,\;x_2=v\sin\phi$, we have
\begin{equation}
    \left\lvert\textrm{det}
    \frac{\partial \mathbf y}{\partial \mathbf x} \right\rvert =1- \frac{1}{v^4}\quad,
\end{equation}
By substituting in the locations of the source images Equation~\ref{eq:images_location} into the
above expression, we obtain the magnification of the source for both images
\begin{equation}
    A_\pm=\left(1- \frac{1}{x^4_\pm} \right)^{-1}
\end{equation}
The total magnification is then the sum of the two magnifications, and is given by
\citep{1936Sci....84..506E}
\begin{equation}
    A(u)=\lvert A_-\rvert+\lvert A_+\rvert= \frac{u^2+2}{u\sqrt{u^2+4}}\quad,
    \label{eq:magnification_point_lens}
\end{equation}
where $u = \sqrt{y_1^2+y_2^2}$ is the the magnitude of the position
vector of the source star.
The critical curve of the point lens is the Einstein
ring corresponding to $v=\sqrt{x_1^2+x_2^2}=1$ and the caustic curve is mapped to a
a single point in the source plane at $u=0$.
For source separations much smaller than the angular Einstein radius ($u\ll 1$), the
magnification is approximately $A(u)\simeq 1/u$, in the opposite case ($u\gg 1$)
we have $A(u)\simeq 1+ 2/u^4$, that is, the magnification falls off rapidly the further
away the source is from the lens.

To be able to evaluate Equation~\ref{eq:magnification_point_lens} in practice
we have to parametrize the position of the source on the sky $u$ as a function
of time. To derive an expression for $u(t)$ we assume the the motion of the
observer, lens and the source is rectilinear (acceleration can be neglected).
We take $\vec{x}_S$ to be the angular position of the source star on the plane
of the sky and $\boldsymbol{\mu}_L$ to be its proper motion vector. Likewise
for the lens. We thus have
\begin{align}
    \vec{x}_S(t) & =\vec{x}_{S, 0}+
    \left(t-t_{0}\right) \boldsymbol{\mu}_S
    \label{eq:source_position}      \\
    \vec{x}_L(t) & =\vec{x}_{L, 0}
    +\left(t-t_{0}\right) \boldsymbol{\mu}_L\quad,
    \label{eq:lens_position}
\end{align}
where $t_0$ is some reference time.
The relative position vector of the lens with respect to the source is then
\begin{equation}
    \boldsymbol{u}(t) \equiv \frac{\vec{x}_L(t)
        -\vec{x}_S(t)}{\theta_E}=
    \frac{\vec{x}_{LS, 0}}{\theta_E}
    +\frac{t-t_{0}}{\theta_E} \boldsymbol{\mu}_{LS}
    \label{eq:relative_trajectory_no_parallax}
\end{equation}
where
$\vec{x}_{L S, 0}\equiv\vec{x}_{L,0}-\vec{x}_{S, 0}$
is the relative position at $t_0$ and
$\boldsymbol{\mu}_{LS}\equiv \boldsymbol{\mu}_{L}- \boldsymbol{\mu}_{S}$ is the
relative proper motion.
Since $\boldsymbol{\mu}_{LS}$ and $\vec{x}_{L S, 0}$ are
perpendicular to each other, it follows that the magnitude of the relative
separation is
\begin{equation}
    u(t)=\sqrt{u_0^2+ \left(\frac{t-t_0}{t_E}\right)^2}
\end{equation}
where we have defined $u_0\equiv |\vec{x}_{L S, 0}|/\theta_E$
and $t_E\equiv \theta_E/|\boldsymbol{\mu}_{LS}|$.
The magnification is then a function of three parameters $(t_0, u_0, t_E)$
and the resulting curve as a function of time is often called the Paczy\'nski curve
\citep{1986ApJ...304....1P,1986ApJ...301..503P}.
The magnification as a function of time for different impact parameters $u_0$
is shown in Figure~\ref{fig:paczynski_curve}. Notice the very steep fall off in
magnification as as we move away from the Einstein ring ($A(u)\propto
    1+2u^{-4}$).
In the limits of $u_0\rightarrow 0$ and $u_0\gg 1$  there is a continuous
mathematical degeneracy between the parameters $t_0$, $u_0$ and $t_E$
\citep{1997ApJ...487...55W}.

\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.7\linewidth]{figures/paczynski_curve.pdf}
        \caption{
            Magnification of a point source by a point lens as a function of time. The
            various magnification curves correspond to different impact parameters $u_0$.
            The inset figure
            shows the source trajectories in the lens plane, the dashed circle corresponds
            to the Einstein radius $v=1$.}
        \label{fig:paczynski_curve}
    \end{centering}
\end{figure}

\subsection{Observed flux}
The magnification $A(t)$ is not a direct observable in microlensing events. We
can only measure the flux of the source star which is usually contaminated with
the flux of other nearby stars in the dense stellar field and potentially also
with light from the lens and possible companions to the lens. The observed flux
is thus given by
\begin{equation}
    F(t)=F_S\,A(t)+F_B
    \label{eq:flux_observed_micro}
\end{equation}
where $F_s$ is the flux of the source star and $F_B$ is the contamination flux also
called the \emph{blending flux}. To quantify the strength of the blending e can
define the emph{source flux fraction} $b_S$ as the fraction of total (unmagnified)
flux that is coming from the source star:
\begin{equation}
    b_S\equiv F_S/(F_S + F_B)
\end{equation}
For highly blended events $b_S\approx 0$ and in absence of blending $b_S=1$.
The source flux $F_S$ and the blending flux $F_B$ are generally highly correlated
and it is often preferable to use a slightly different parametrization proposed by
\citet{2009MNRAS.393..816D}. In this parametrization we use the
\emph{baseline flux} $F_\mathrm{base}$ and the difference between
the baseline flux and the flux at peak magnification $\Delta F\equiv F_S[A(u_0)-1]$
as the new parameters.
Equation~\ref{eq:flux_observed_micro} then takes the form
\begin{equation}
    f=\Delta F\frac{A(t) - 1}{A(t_0)-1}+F_\mathrm{base}
    \label{eq:flux_dominik}
\end{equation}
It is generally straightforward to measure the flux difference $\Delta F$, the
baseline flux $F_\mathrm{base}$ and the center of the curve $t_0$ but getting a
good estimate of $t_E$ even outside of the regime $u_0\rightarrow 0$ and
$u_0\gg 1$ requires good photometric sampling along the ``wings'' of the light
curve \citep{2009MNRAS.393..816D}.

\subsection{Magnification of an extended source}
\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/single_lens_images.pdf}
        \caption{Images of an extended limb-darkened source star lensed by a single
            point lens for varying position of the source star. The top panels show the magnification map with a logarithmic scale
            colormap  consisting of a single point caustic. The semi-transparent circle
            is the source star disc with radius $\rho_\star=0.15$. The bottom row shows
            the two images merging into the Einstein ring as the source moves over the caustic.}
        \label{fig:single_lens_images}
    \end{centering}
\end{figure}
For a relatively small subset of microlensing events the source star passes
very close to the caustic and the variation of magnification over the extent of
the source star disc is non-negligible. For those events the point source
approximation breaks down and we have to take into account the finite angular
radius $\theta_\star$. This will generally happen when $u_0 \lesssim
    \rho_\star/2$ \citep{1997ApJ...477..580G} where
$\rho_\star=\theta_\star/\theta_E$. The effect of finite source effects on the
magnification curve thus matters only near the peak of an event and it results
in a rounder top of the curves shown in Figure~\ref{fig:paczynski_curve}.
Unfortunately there are no analytic solutions for the magnification of an
extended source lensed by a single point lens with a general surface brightness
profile. \citet{1994ApJ...421L..71G} derived an approximate solution valid for
uniform brightness sources with $\rho_\star\lesssim 0.1$ and
\citet{1994ApJ...430..505W} derived a general solution (also for a uniform
brightness source) involving solutions to elliptic integrals of the first,
second and third kind. A general solution involving Fast Fourier Transforms
which is applicable to any source profile was recently proposed by
\citet{2022arXiv220306637S}.

In Figure~\ref{fig:single_lens_images} we show the magnification map and the
images of an extended limb-darkened source with $\rho_\star=0.15$ as it
approaches the point caustic for a single lens. The top panels show the
magnification map in the source plane using a logarithmic scale colormap. The
source disc is shown (semi-transparent grey circle) at different positions
relative to the caustic. The bottom panels show the resulting images in the
image plane, this is what we would see on the sky if we could resolve the
microlensing event. As the source moves closer to the caustic the area (and
hence the magnification) of the two images increases and eventually they merge
forming an Einstein ring.

To produce these plots I evaluated the lens mapping
(Eq.~\ref{eq:lens_equation_dimensionless}) on a regular grid in the image plane
and then plotted the surface brightness of the corresponding point on the
source disc in the source plane. I will discuss all of this in far greater
detail In Chapter~\ref{ch:microlensing} but for now I should mention a few key
points:
\begin{itemize}
    \item Every point inside the two images maps to a single point on the source disc via
          the lens equation. The inverse mapping is one-to-many because each point on the
          source disc maps to two points (point lens images) in the image plane. The
          points on the limbs of the two images correspond to the points on the limb of
          the source disc. We cannot predict the location of the images without inverting
          the lens mapping
    \item To numerically compute the magnification of an extended source, we could either
          integrate the magnification function convolved with the source brightness
          profile over the entire source disc (a two-dimensional integral), or we could
          do the same integral in the image plane. The first approach is highly
          numerically unstable because we would need to integrate over a function which
          diverges as $1/r$ approaching the caustic and this would require a very large
          number of lens equation evaluations. Hence it is much better to integrate in
          the image plane where the function is smooth.
    \item Decreasing the source radius $\rho_\star$ results in narrower arcs of the
          images (smaller radial extent).
\end{itemize}

\subsection{Annual parallax}
\label{ssec:single_lens_parallax}
In Equation~\ref{eq:relative_trajectory_no_parallax} we have assumed that the
relative motion of the lens with respect to the source on the plane of the sky
is rectilinear.
This is a reasonable approximation in a barycentric frame of
reference if the acceleration of the source and the lens can be neglected.
However, most microlensing events are observed from a non-inertial geocentric
frame (Earth) and sometimes also from multiple locations simultaneously. In those
cases we have to take into account parallax effects.
We differentiate between two kinds of parallax. First is the \emph{annual
    parallax} (sometimes also called the orbital parallax) which modifies the
lens-source relative motion vector $\boldsymbol\mu_{LS}$ due to the local
acceleration of Earth in its orbit. It is important for long timescale events
when the event timescale is equal to some substantial fraction of a year. The
effect of annual parallax is usually a slight modification of the shape of the
Paczy\'nski curve.

The second kind of parallax is the \emph{satellite parallax} which refers to a
difference in viewpoint when conducting simultaneous observations of the source
star from different locations separated by a substantial fraction of the
Einstein ring projected onto the observer plane $\tilde{r}_E\equiv
    D_{LS}\theta_E$ (where $D_{LS}^{-1}=D_L^{-1} - D_S^{-1}$). In practice this
means observing a microlensing event from Earth and a space based telescope
simultaneously. It was first proposed by \citet{1966MNRAS.134..315R}. There is
also the ``terrestrial parallax'' which is identical to the satellite parallax
except that it involves only ground-based observations separated by a large
fraction of Earth's diameter. A measurement of these parallax effects allows
for a partial breaking of the degeneracy in the event timescale $t_E$ by
providing a relationship between the mass and the distance to the lens
(assuming one can estimate the distance to the source star).

In this section I will describe the annual parallax effect. We start by
modifying Equations~\ref{eq:source_position} and \ref{eq:lens_position} to
include the projected motion of the Sun relative to the Earth on the plane of
the sky. We project $\mathbf s$ -- the position vector of the Sun relative to
Earth, onto a plane perpendicular to the line of sight towards the source star
(plane of the sky) which is defined by the unit vector $\boldsymbol{\hat n}$
normal to the plane. The unit vector $\boldsymbol{\hat n}$ depends on the sky
coordinates of the source star $(\alpha,\delta)$ (right ascension and
declination). We choose to work in geocentric equatorial coordinates defined by
the spherical unit vectors $\hat{\mathbf e}_n$, pointing North, and
$\hat{\mathbf e}_e$ pointing eastward such that the coordinate system is
right-handed. The unit vectors are defined by
\begin{align}
    \hat{\mathbf e}_e & = \hat{\mathbf z}\times \hat{\mathbf n}  \\
    \hat{\mathbf e}_n & = \hat{\mathbf n}\times\hat{\mathbf e}_e
\end{align}
The two components of $\mathbf s$ projected onto the plane of the sky are then
\begin{align}
    \zeta_e(t;\alpha,\delta)\equiv \mathbf s\cdot \hat{\mathbf e}_e \\
    \zeta_n(t;\alpha,\delta)\equiv \mathbf s\cdot \hat{\mathbf e}_n
\end{align}
In practice, we can retrieve $\mathbf{s}$ using NASA's JPL Horizons system and compute
the projected separation at any time $t$.
The angular positions of the source and the lens
(Equations~\ref{eq:source_position} and \ref{eq:lens_position}) then become
\begin{align}
    \vec{x}_S(t) & = \vec{x}_{S,0}+(t-t_0)\boldsymbol{\mu}_S
    +\pi_S\,\boldsymbol{\zeta}(t)                            \\
    \vec{x}_L(t) & = \vec{x}_{L,0}+(t-t_0)\boldsymbol{\mu}_L
    +\pi_L\,\boldsymbol{\zeta}(t)
\end{align}
where  $\pi_S\equiv 1\,\mathrm{au}/D_S$ is the source parallax, and $\pi_L$ is the
lens parallax.
The relative separation is then
\begin{equation}
    \boldsymbol{u}(t)= \frac{\boldsymbol\theta_{LS, 0}}{\theta_E}
    +\frac{t-t_0}{\theta_E}\boldsymbol{\mu}_{LS}+\pi_{E}\,\boldsymbol{\zeta}(t)
    \label{eq:relative_separation_parallax}
\end{equation}
where
\begin{equation}
    \pi_E\equiv \frac{\pi_{LS}}{\theta_E}
    \label{eq:pi_E}
\end{equation}

Since annual parallax is a higher order effect affecting the apparent
trajectory of the source on the sky, it makes sense to decompose the trajectory
as a sum of rectilinear motion plus a deviation due to parallax. Following
\citet{2004ApJ...606..319G} we define the position offset of the Sun on the
plane of the sky relative to its position at a particular time $t_0'$ as

\begin{equation}
    \delta\boldsymbol \zeta (t)=\boldsymbol \zeta (t)-\boldsymbol \zeta (t_0')-(t-t_0')
    \boldsymbol{\dot \zeta} (t_0')
\end{equation}
By construction, we have $\delta\boldsymbol \zeta (t_0')=0$ and
$\delta\dot{\boldsymbol \zeta} (t_0')=0$.
Equation~\ref{eq:relative_separation_parallax} then takes the form
\begin{equation}
    \boldsymbol{u}(t)=\mathbf{u}_0 + (t-t_0')\,\dot{\mathbf{u}}_0 +
    \pi_E\,\delta\boldsymbol \zeta(t)
    \label{eq:relative_separation_parallax_decomposed}
\end{equation}
where
\begin{align}
    \mathbf{u}_0       & \equiv \mathbf{u}(t_0) =\frac{\boldsymbol\theta_{LS, 0}}{\theta_E}
    + \frac{(t_0'-t_0)}{\theta_E}\boldsymbol\mu_{LS}+
    \pi_E\,\boldsymbol \zeta(t_0)                                                           \\
    \dot{\mathbf{u}}_0 & \equiv\dot{\mathbf{u}}(t_0)=
    \frac{\boldsymbol\mu_{LS}}{\theta_E}  + \pi_E\,\dot{\boldsymbol \zeta}
    (t_0')
\end{align}
In what follows, we set the reference time to $t_0'\equiv t_0$ in order to ensure that
the vectors $\mathbf{u}_0$ and $\dot{\mathbf{u}}_0$ are perpendicular to each other,
which is the case at $t=t_0$ because at $t_0$ the trajectory has a local extremum
\footnote{In CITE, $t_0'$ is fixed to a particular value. It is not clear
    to me how this ensures that the two vectors are perpendicular to each other
    (TODO: show that this is true).}

\subsubsection{$\mathbf{u}(t)$ in the $(\mathbf{\hat e}_\bot,
        \mathbf{\hat e}_\parallel)$ coordinate system}%

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{static/microlensing/microlensing_parallax_coordinates.pdf}
    \caption{A coordinate system
        $(\mathbf{\hat e}_\bot,\mathbf{\hat e}_\parallel)$ parallel to the source trajectory at time $t_0$. The orange curve
        represents the source trajectory relative to the lens at the origin. The
        coordinate system $(\mathbf{\hat e}_\bot,\mathbf{\hat e}_\parallel)$ is related to the equatorial coordinates $(\mathbf{\hat
                e}_e,\mathbf{\hat e}_n)$ by a rotation through an angle $\psi$.}
    \label{fig:parallax2}
\end{figure}

To evaluate Equation~\ref{eq:relative_separation_parallax_decomposed}, we need
to choose a suitable basis. A natural coordinate system for describing the
trajectory of the source relative to the lens is one defined by unit vectors
$(\mathbf{\hat e}_\bot,\mathbf{\hat e}_\parallel)$ where $\mathbf{\hat e}_\parallel$ is parallel to the trajectory $\mathbf{u}(t)$ at time $t_0$
(Figure~\ref{fig:parallax2}). We define the unit vectors as
\begin{equation}
    \mathbf{\hat e}_\bot\equiv \frac{\mathbf{u}_0}{|\mathbf{u}_0|}\quad,
    \mathbf{\hat e}_\parallel\equiv \frac{\mathbf{\hat n}\times\mathbf{u}_0}{|\mathbf{u}_0|}\quad
\end{equation}
and the coordinate system $(\mathbf{\hat e}_\bot,\mathbf{\hat e}_\parallel)$ is related to ecliptic coordinates via a simple rotation through an
angle $\psi$
\begin{equation}
    \begin{pmatrix}
        \mathbf{\hat e}_\bot \\
        \mathbf{\hat e}_\parallel
    \end{pmatrix}
    =
    \begin{pmatrix}
        \cos\psi & -\sin\psi \\
        \sin\psi & \cos\psi
    \end{pmatrix}
    \begin{pmatrix}
        \mathbf{\hat e}_e \\
        \mathbf{\hat e}_n
    \end{pmatrix}
    \label{eq:ecliptic_to_parallel}
\end{equation}
By construction, at time $t_0$ we have
$\mathbf{u}_0\,\bot\,\dot{\mathbf{u}}_0$ and the two components of
$\mathbf{u}(t)$ are then
\begin{align}
    u_\bot(t)      & \equiv \mathbf{u}(t)\cdot \mathbf{\hat e}_\bot= u_0 +
    \pi_E\,\delta\boldsymbol \zeta(t)\cdot\mathbf{\hat e}_\bot                                                                                                                                       \\
    u_\parallel(t) & \equiv \mathbf{u}(t)\cdot \mathbf{\hat e}_\parallel= (t-t_0)\,\dot{\mathbf{u}}_0\cdot\mathbf{\hat e}_\parallel+ \pi_E\,\delta\boldsymbol \zeta(t)\cdot\mathbf{\hat e}_\parallel
\end{align}
using the definitions of the unit vectors and
$\delta\boldsymbol \zeta(t)=
    \delta \zeta_e(t)\,\mathbf{\hat e}_e+\delta \zeta_n(t)\,\mathbf{\hat e}_n$, we have
\begin{align}
    u_\bot(t)      & = u_0 + \pi_E\,\cos\psi\,\delta \zeta_e(t) - \pi_E\,\sin\psi\,\delta \zeta_n(t)
    \label{eq:u_t_parallel1}                                                                         \\
    u_\parallel(t) & =(t-t_0)/t_E' + \pi_E\,\sin\psi\,\delta \zeta_e(t) +
    \pi_E\,\cos\psi\,\delta \zeta_n(t) \label{eq:u_t_parallel2}
\end{align}
where $t_E' =|\dot{\mathbf{u}}_0|=|\boldsymbol\mu_{LS}/\theta_E  + \pi_E\,\dot{\boldsymbol \zeta}(t_0')|$.
The model parameters which determine the magnification $A(t)$ are then
$\left(u_0,t_0,t'_E,\pi_E,\psi\right)$. Notice that $u_0$ in this case can also be negative.

An alternative parametrization which is more commonly used is obtained by
defining components of a "microlensing parallax vector" as
\begin{align}
    \pi_{E,N} & \equiv \pi_E\cos\psi \\
    \pi_{E,E} & \equiv \pi_E\sin\psi
\end{align}
in which case we have
\begin{align}
    u_\bot(t)      & = u_0 + \pi_{E,N}\,\delta \zeta_e(t) - \pi_{E,E}\,\delta \zeta_n(t) \\
    u_\parallel(t) & =(t-t_0)/t_E' + \pi_{E,E}\,\delta\zeta_e(t) +
    \pi_{E,N}\,\delta\zeta_n(t)
\end{align}
and the model parameters are $\left(u_0,t_0,t_E',\pi_{E,E},\pi_{E,N}\right)$.

For reference, we can also write down the components of $\mathbf{u}(t)$ in
equatorial coordinates by inverting the rotation matrix in
Equation~\ref{eq:ecliptic_to_parallel} and applying it to
Equations~\ref{eq:u_t_parallel1} and \ref{eq:u_t_parallel2}, the two components
are then
\begin{align}
    u_e & =u_0\cos\psi + (t-t_0)/t_E'\sin\psi + \pi_E\,\delta\zeta_e(t)  \\
    u_n & =-u_0\sin\psi + (t-t_0)/t_E'\cos\psi + \pi_E\,\delta\zeta_n(t)
\end{align}

\subsubsection{$\mathbf{u}(t)$ in the $(\mathbf{\hat e}_\parallel, \mathbf{\hat e}_\bot)$ coordinates using acceleration parameters} There
exist another parametrization of the trajectory in which we fit for the local
acceleration of the lens at $t_0$ instead of $\pi_E$ and the angle $\psi$. We
define the position, velocity and acceleration such that at $t=t_0$ we have
\begin{align}
    \mathbf{u}(t_0)       & \equiv \widetilde{u}_0 \,\hat{\mathbf e}_\bot
    \label{eq:u_primed_def}                                               \\
    \dot{\mathbf{u}}(t_0) & \equiv \frac{1}{\widetilde{t_E'}}\,
    \hat{\mathbf e}_\parallel \label{eq:u_dot_primed_def}                 \\ \ddot{\mathbf{u}}(t_0) & \equiv
       a_\bot\,\hat{\mathbf e}_\bot + a_\parallel\,\hat{\mathbf e}_\parallel \label{eq:u_ddot_primed_def}
\end{align}
where $a_\parallel$ and $a_\bot$ are the two components of the instantaneous acceleration of
the lens at $t_0$. From Equations~\ref{eq:u_t_parallel1} and
\ref{eq:u_t_parallel2} it follows that
\begin{align}
    \mathbf{u}(t_0)       & = u_0\,\hat{\mathbf e}_\bot                    \\
    \dot{\mathbf{u}}(t_0) & =\frac{1}{t_E'}\,\hat{\mathbf e}_\parallel     \\ \ddot{\mathbf{u}}(t_0) &
       =\left[\pi_E\cos\psi\,\delta\ddot{\zeta}_e(t_0)
    -\pi_E\sin\psi\,\delta\ddot{\zeta}_n(t_0)\right]\,\hat{\mathbf e}_\bot \\  &
       +\left[\pi_E\sin\psi\,\delta\ddot{\zeta}_e(t_0)
           +\pi_E\cos\psi\,\delta\ddot{\zeta}_n(t_0)\right]\,\hat{\mathbf e}_\parallel
\end{align}
By equating the components of the position, velocity and acceleration vectors, we obtain the expressions for the old parameters in terms of the new parameters as
\begin{equation}
    \widetilde{u}_0=u_0,\quad \widetilde{t_E'}=t_E'\,\quad
    \pi_E=\sqrt{\frac{a_\parallel^2 + a_\bot^2}{1} \left[\delta\ddot{\boldsymbol{\zeta}}_e(t_0)\right]^2 +
        \left[\delta\ddot{\boldsymbol{\zeta}}_n(t_0)\right]^2}
\end{equation}
Similarly, we have
\begin{align}
    \pi_{E,N}=\pi_E\cos\psi & = \frac{a_\parallel\,\delta\ddot{\zeta}_n(t_0) + a_\bot\,\delta\ddot{\zeta}_e(t_0)
    }{[\delta\ddot{\zeta}_e(t_0)]^2 + [\delta\ddot{\zeta}_n(t_0)]^2}                                              \\
    \pi_{E,E}=\pi_E\sin\psi & = \frac{ a_\parallel\,\delta\ddot{\zeta}_e(t_0) - a_\bot\,\delta\ddot{\zeta}_n(t_0)
    }{[\delta\ddot{\zeta}_e(t_0)]^2 + [\delta\ddot{\zeta}_n(t_0)]^2}
\end{align}
To obtain the trajectory in terms of these new parameters, we plug in the above expressions
into Equations~\ref{eq:u_t_parallel1} and \ref{eq:u_t_parallel2}.
The new parameter set is then $\left(u_0,t_0,t_E',a_\parallel,a_\bot\right)$. This parametrization makes it obvious that the parallax
effect depends on the apparent local acceleration of the lens at time $t_0$.

\subsection{Measuring the lens mass}
Notice that only quantity of physical interest in single lens microlensing --
the lens mass $M$, is buried inside of the definition of the angular Einstein
radius $\theta_E$ which, when combined with the magnitude of the relative
proper motion $|\boldsymbol\mu_{LS}|$ forms the observable $t_E$. In practice,
depending on the microlensing event there are several possible channels which
enable an estimate of the lens mass. For example, a measurement of $\theta_E$
from finite-source effects or from a direct measurement of
$|\boldsymbol\mu_{LS}|$ can be combined with a measurement of $\pi_E$ to yield
(via Equations \ref{eq:angular_einstein_radius} and \ref{eq:pi_E})
\begin{equation}
    M = \frac{\theta_E}{\kappa \pi_E}
\end{equation}
With an estimate of the distance to the source star one can also obtain the distance
to the lens.
An alternative, less direct approach is to use a measurement of $\theta_E$ combined
with the measurement of the lens flux to estimate the mass and distance to the lens
\citep{2007ApJ...660..781B}.
It should be noted that either of these approaches is possible for only a small subset
of all detected microlensing events.

\subsection{A system with $N$ lenses}
Microlensing with more than one lens is considerably more complex than the case
of a single lens. Consider a system with $N$ point mass lenses with a total
mass $M=\sum_{i=1}^Nm_i$. As before, we have the angular source position in the
source plane, given by the the dimensionless vector
$\mathbf{y}=\boldsymbol\beta/(D_\textrm{S}\theta_E)$ and the position of the
images in the lens plane, given by given by
$\mathbf{x}=\boldsymbol\theta/(D_\textrm{L}\theta_E)$, where $\theta_E$ now
refers to the angular Einstein radius corresponding to the total mass $M$. The
lens equation then contains a sum over the deflection angles
$\boldsymbol\alpha_i$ corresponding to each point mass $m_i$\footnote{This is
    because we are working within the linearized GR framework so the deflection
    angles are additive.}:
\begin{equation}
    \mathbf{y}=\mathbf{x}-\sum_{i=1}^N\boldsymbol\alpha_i(\mathbf{x},\mathbf{x}_i)
\end{equation}
where $\boldsymbol\alpha_i$ is the deflection angle due to the $i$th lens.
Using Equation~\ref{eq:lens_equation_dimensionless}, we obtain
\begin{equation}
    \mathbf{y}=\mathbf{x}-\sum_{i=1}^N\epsilon_i\frac{\mathbf{x}-\mathbf{x}_i}
    {\lvert \mathbf{x}-\mathbf{x}_i\rvert^2}
    \label{eq:lens_equation_general}
\end{equation}
where $\epsilon_i\equiv m_i/M$.

It is useful to rewrite Equation~\ref{eq:lens_equation_general} in complex form
by introducing complex variables \citep{1990A&A...236..311W}
\begin{equation}
    w= y_1+iy_2,\quad z=x_1+ix_2
\end{equation}
The lens equation then takes the form
\begin{equation}
    w=z-\sum_{i=1}^N \frac{\epsilon_i}{\bar z-\bar z_i}
    \label{eq:lens_equation_complex}
\end{equation}
The mapping from the image plane $z$ to the source plane $w$ is one-to-one and it is
straightforward to compute using Equation~\ref{eq:lens_equation_complex}. The inverse
mapping is not as easy. It turns out that it is possible to rewrite Equation~\ref{eq:lens_equation_complex}
as a complex polynomial of degree $N^2 + 1$ (see Appendix~\ref{app:complex_poly}).
From the Fundamental Theorem of Algebra, it follows that such a polynomial has exactly $N^2+1$ complex roots.
However, not all of these roots are necessarily solutions to the lens equations so in practice one first has to
compute all of the polynomial roots and then discard those which do not satisfy
Equation~\ref{eq:lens_equation_complex}.
In the case of binary lens, there are always either 3 or 5 real images and for $N\geq 2$ the maximum number
of images is $5(N-1)$ \citep{arXiv:astro-ph/0103463,astro-ph/0305166,arXiv:math/0401188v2}.
As before, the magnification is given by the inverse Jacobian determinant of the mapping $z\rightarrow w$
evaluated at the images. We have
\begin{equation}
    \mathbf{J}=
    \renewcommand\arraystretch{2}
    \begin{pmatrix}
        \frac{\partial w}{\partial z} & \frac{\partial w}{\partial \bar{z}} \\ \frac{\partial \bar{w}}{\partial z} & \frac{\partial \bar{w}}{\partial \bar{z}}
    \end{pmatrix}
\end{equation}
and
\begin{equation}
    \mathrm{det}\,\mathbf{J}=\frac{\partial w}{\partial z}\frac{\partial \bar{w}}{\partial \bar{z}} - \frac{\partial w}{\partial \bar{z}}\frac{\partial \bar{w}}{\partial z} = \left|\frac{\partial w}{\partial z}\right|^2 - \left|\frac{\partial w}{\partial \bar{z}}\right|^2
\end{equation}
where we have used the identities
$\overline{\frac{\partial\bar a}{\partial{\bar b}}}=\frac{\partial a}{\partial{b}}$ and $a\bar a=|a|^2$. Finally, by evaluating the partial derivatives
using Equation~\ref{eq:lens_equation_complex} we obtain
\begin{equation}
    \mathrm{det} \,\mathbf J=1-\left|\sum_{i=0}^{N} \frac{\epsilon_{i}}{\left(\bar{z}-\bar{z}_i\right)^{2}}\right|^{2}
\end{equation}
so the magnification is given by
\begin{equation}
    A = \sum_j \frac{1}{\left|\mathrm{det}\,\mathbf J\right|_j}
\end{equation}
where $j$ denotes the $j$-th image. The points in the image plane where the Jacobian determinant vanishes
($\mathrm{det}\,\mathbf J=0$) are the critical curves:
\begin{equation}
    \left|\sum_{i=0}^{N} \frac{\epsilon_{i}}{\left(\bar{z}-\bar{z}_i\right)^{2}}\right|^{2}=1
    \label{eq:critical_curves_complex}
\end{equation}
The points on the critical curve mapped to the source plane via
the lens equation (Equation~\ref{eq:lens_equation_complex}) are the caustic curves.
The difference compared to the case of a single lens lens equation is that the caustics are closed curves
rather than a single point. The caustic curves are comprised of concave segments called \emph{folds} which are connected
at points called \emph{cusps}.
Equation~\ref{eq:critical_curves_complex} can also be written as a complex polynomial, of
degree $2N$ (see Appendix~\ref{app:complex_poly_crit}) so there are at most $2N$ critical and caustic curves.
The complexity of microlensing events involving multiple lenses is a direct consequence of the
non-smooth nature of caustic curves.

\subsection{Binary lens}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/binary_lens_topology.pdf}
    \caption{The three different topologies of the binary lens. The left panels show the
        critical curves in the lens plane, the right panels show the caustic curves in the
        source plane. The figure shows all three topologies and the transitions between
        them for an equal mass binary lens with $q=1$. In this case the transition between
        the close and intermediate topology occurs at $s=1/\sqrt{2}$ and the one between
        the intermediate and wide topology at $s=2.0$. Figure adapted from
        \citet{dominik1999}.
    }
    \label{fig:binary_lens_topology}
\end{figure}
In this section we will focus specifically on the binary lens. We choose a
coordinate system whose origin is at the midpoint of the line connecting the
two lenses and both lenses are on the real axis. If the first lens is at
distance $a$ ($z_1=a$) then the second lens is at $-a$ and the lens equation
takes the form
\begin{equation}
    w=z+\frac{\epsilon_{1}}{\bar{z} - a}+\frac{1 - \epsilon_{1}}{\bar{z} + a}
\end{equation}
The above equation can be rewritten as a 5th order complex polynomial
(see Appendix~\ref{app:complex_poly}). Either 3 or 5 of the complex roots of that
polynomial are also solutions to the lens equation
(5 inside the caustics and 3 outside).
It is common to use the mass ratio $q\equiv \epsilon_2/\epsilon_1$ with
$\epsilon_2 \leq \epsilon_1$ instead  of $\epsilon_1$ as a parameter and the
separation between the lenses $s\equiv 2a$ instead of half the separation $a$.
As a reminder all angular quantities are expressed in the units of the angular Einstein
radius of the total mass of the system.

Depending on the separation between the two lenses, $s$, the critical and the
caustic curves can have three distinct topologies which are labeled
\emph{close}, \emph{intermediate}, and \emph{wide}. These are shown in
Figure~\ref{fig:binary_lens_topology} for a system with $q=1$ where on the left
we see the critical curves in the lens plane, and on the right the caustic
curves in the source plane. The critical and caustic curves are symmetric with
respect to the x-axis. The sharp structure of the caustic curves compared to
the smooth critical curves is due to the nonlinearity of the lens mapping. The
number of different topologies does not depend on the mass ratio.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/planetary_caustics.pdf}
    \caption{Caustic structure for a binary lens with $q=0.003$, shown for different
        values of the separation $s$. The orange star denotes the position of the star and
        the blue dot designates the planet.
        The transition between the close and intermediate
        topologies occurs at $s\approx 0.92$ and the one between intermediate and wide
        topologies at $s\approx 1.21$. The vertical gray dashed line shows the angular
        Einstein radius at $y_1=1$. }
    \label{fig:planetary_caustics}
\end{figure}

Caustics shown in Figure~\ref{fig:binary_lens_topology} corresponds to an equal
mass binary lens, systems with $q\ll 1$ are of more interest because these
correspond to planetary systems. The lower mass ratio does not change the
topological structure of the caustics (except for shifting the boundaries
between the different topologies) but it does change their shape and size. In
Figure~\ref{fig:planetary_caustics} we show point source \emph{magnification
    maps} (computed using the \textsf{caustics} code which is the subject of
Chapter~\ref{ch:microlensing}) for a binary lens with $q=5\times 10^{-3}$,
roughly corresponding to a gas giant planet orbiting an M dwarf. For the close
topology (first two panels from the top in Figure~\ref{fig:planetary_caustics})
we see one caustic centered on the star which is called a \emph{central
    caustic} and two additional caustics on the opposite side of the planet,
symmetric with respect to the star-planet axis, which are called
\emph{planetary caustics} because they are associated with the planet rather
than the star. Notice that the magnification in the vicinity of these planetary
caustics is significantly smaller than the magnification around the central
caustic. Because of this it is easier to search for binary events with
trajectory passing close to the central caustic rather than the planetary
caustics. As the separation $s$ approaches $s_c$, the critical value between
the close and wide topologies, the two planetary caustics merge with the
central caustic into a single caustic, often called the \emph{resonant
    caustic}, with a much larger cross section than either the central or the
planetary caustics (third panel from the top). Finally, as the separation $s$
increases further beyond the intermediate/wide boundary, the resonant caustic
separates into a smaller central caustic centered on the star and a single,
larger, planetary caustic located on the star-planet axis (bottom panel). The
size of this planetary caustic scales as $q^{1/2}s^{-2}$ \citep[see references
    in review by][]{Gaudi2012}.

Given caustics such as the ones shown in Figure~\ref{fig:planetary_caustics}
the magnification as a function of time depends on the source trajectory in the
source plane. Absent parallax, it is just a slice through the magnification map
convolved with the source star brightness profile. \textbf{Herein lies the
    complexity of microlensing. The trajectory of the source star over the caustic
    patterns is completely random and the magnification response is highly
    non-linear}. The consequence of this fact is there is a large variety of
possible light curve morphologies and the parameter space of the models is
highly complex.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/close_wide_degeneracy.pdf}
    \caption{Illustration of the ``close/wide'' degeneracy in binary microlensing events.
        The two panels on the left show the magnification maps (on a logarithmic scale) for
        a binary lens with $q=5\times 10^{-3}$, $s=0.7$ (top panel), and $s=1/0.7$
        (bottom panel). The dashed grey line indicates the source trajectory. The panel
        on the right shows the corresponding magnification as a function of time.
        We see that trajectories through two very different configurations of the binary lens
        result in nearly identical magnification curves. In absence of good
        photometric coverage near the central caustic crossing this approximate degeneracy
        can become exact.}
    \label{fig:close_wide_degeneracy}
\end{figure}

One notable feature of the binary lens is that in the limit $q\ll 1$ and
$\lvert s-1\rvert \gg q$, the structure of the caustic curves are invariant to
the $s\rightarrow s^{-1}$ transformation \citet{dominik1999} which is known in
the literature as the \emph{close/wide degeneracy} and it often arises for
microlensing events involving planets. Figure~\ref{fig:close_wide_degeneracy}
illustrates this (approximate) degeneracy. The two panels on the left show the
magnification maps of a binary lens with $q=5\times 10^{-3}$ and two different
values of the separation $s$ related by the transformation $s\rightarrow
    s^{-1}$ . The dashed grey line shows the trajectory of the source passing close
to the central caustic. The panel on the right shows that the magnification as
a function of time is nearly identical for the two different configurations of
the binary lens. In the absence of good photometric coverage near the peak of
the event such approximate degeneracies can become exact. These kinds of data
dependent degeneracies (rather than exact mathematical symmetries) are a very
common feature of microlensing \citep{erdl1993}.

\subsubsection{Parametrizing the trajectory}
To parametrize the trajectory $\mathbf u(t)$ including annual parallax effect
we have use an additional angle to specify the orientation of the axis
containing both lenses. We can build on the work done in
Section~\ref{ssec:single_lens_parallax} and instead of interpreting the origin
of the coordinate system as the location of the single lens, we interpret it as
the midpoint between the two lenses which lie on the same axis. $\mathbf u_0$
is then the point on the trajectory closest to the midpoint and $\dot{\mathbf
        u}_0$ is the velocity vector at that point. To specify the trajectory of the
source in this new coordinate system we just need to rotate the coordinate
system $(\hat{\mathbf e}_\parallel, \hat{\mathbf e}_\bot)$ by $\alpha$ where $\alpha$ is the angle between
the $\hat{\mathbf e}_\parallel$ and the axis containing the two lenses. Using
Equations~\ref{eq:u_t_parallel1} and~\ref{eq:u_t_parallel2} we have
\begin{equation}
    \mathbf u(t)  =
    \mathbf R(\alpha)
    \begin{pmatrix}
        u_\bot (t) \\
        u_\parallel (t)
    \end{pmatrix}
    =
    \mathbf R(\alpha)
    \renewcommand\arraystretch{2}
    \begin{pmatrix}
        u_0 + \pi_E\,\cos\psi\,\delta \zeta_e(t) - \pi_E\,\sin\psi\,\delta \zeta_n(t) \\
        (t-t_0)/t_E' + \pi_E\,\sin\psi\,\delta \zeta_e(t) +
        \pi_E\,\cos\psi\,\delta \zeta_n(t)
    \end{pmatrix}
\end{equation}
where $\mathbf R (\alpha)$ is the rotation matrix  given by
\begin{equation}
    \mathbf R (\alpha)=
    \begin{pmatrix}
        \cos\alpha & -\sin\alpha \\
        \sin\alpha & \cos\alpha
    \end{pmatrix}
\end{equation}
The magnification of a binary lens is then fully parametrized by the following parameters
\begin{equation}
    (u_0,t_0,t'_E,\pi_E,\psi, a, q, \alpha)
\end{equation}

\subsection{Triple lens}
The first detected triple lens microlensing event was OGLE-2006-BLG-109Lb,c
\citep{2008Sci...319..927G,2010ApJ...713..837B}, a system consisting of two
massive planets and a star, similar to Jupiter and Saturn in the Solar System.
A circumbinary planet has also been detected \citep{2016AJ....152..125B} and
there is a also possibility of detecting exomoons although none have been
confirmed so far \citep{2010A&A...520A..68L}. To parametrize a triple lens
system, we use the same coordinate system as for the binary lens with the
addition of a third lens at an arbitrary location $z_3$ in the source plane.
The lens equation is then
\begin{equation}
    w=z+\frac{\epsilon_{1}}{\bar{z} - a}+\frac{\epsilon_{2}}{\bar{z} + a} +
    \frac{1 - \epsilon_1 - \epsilon_2}{\bar{z} - \bar{z}_3}
\end{equation}
The complex polynomial derived from the triple lens lens equation is a 10th degree
polynomial.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/triple_lens_caustics.pdf}
    \caption{Caustic structure for a triple lens with
        $\epsilon_1=0.9$, $\epsilon_2=\epsilon_e=0.05$, $s=0.8$ and $z_3=0.3-i0.8$.
        The caustics are considerably more complex than in the binary case and they
        can be nested and self-intersecting.
    }
    \label{fig:triple_lens_caustics}
\end{figure}

The caustic/critical curve structure of a triple lens system is vastly more
complex than the binary lens system. \citet{2019ApJ...880...72D} investigate
three different kinds of systems: equal masses for all three lenses, two equal
mass lenses and a low-mass third component and a hierarchical combination of
the three masses. They find 11 different kinds of topologies of the caustics.
Depending on the mass ratios, there could even be more kinds of topologies. In
Figure~\ref{fig:triple_lens_caustics} we illustrate an example caustic
structure for a triple lens system with $\epsilon_1=0.9$,
$\epsilon_2=\epsilon_e=0.05$, $s=0.8$ and $z_3=0.3-i0.8$. The key features to
note are that the caustic pattern is no longer symmetric about the x-axis and
the caustics can be nested and self-intersecting.

\subsection{Other effects}
There are a few other effects which often need to be taken into account when
modeling real microlensing events. I won't cover these in detail except for the
first one because the focus on this thesis is on core but I describe them below
for completeness.
\begin{itemize}
    \item \textbf{Finite source effects:} accurately computing the magnification
          of a limb-darkened extended source is highly non-trivial, this is the subject
          of Chapter~\ref{ch:microlensing}.
    \item \textbf{Multiple sources:} instead of a single source star we could have
          a multiple source stars \citep{1998A&A...333..893D}.  Events with a binary source
          star can sometimes mimic binary lens events with a single source star.
          Modeling these events requires specifying the trajectory and separation of
          the binary and the total flux is then the sum of the flux from the primary and the
          secondary star.
          There is a paucity of binary source events in the microlensing literature, partly due to
          their intrinsic rarity \citep{1998MNRAS.301..231H} and partly because there is a
          bias in the community towards fitting binary lens models and
          simply not putting as much effort into testing binary source models as
          for the binary lens models \citep{2017AJ....153..129J,2019MNRAS.484.5608D}.
    \item \textbf{Orbital motion of the lenses:} if the orbital timescale of
          the lens in binary or triple lens systems is comparable to the event timescale
          (this is true for a small subset of events) then one needs to take into account
          the orbital motion of the lens projected onto the plane of the sky.
          In the general case of a Keplerian orbit five additional parameters are needed
          for the binary lens: the mass of the primary lens, the three components of the
          secondary's velocity relative to the primary and the projected separation
          of the secondary along the line of sight in units of $\theta_E$
          \citep{1998A&A...329..361D}. In practice only two additional parameters
          can be measured: the sky-projection of the two components of the velocity of
          the secondary relative to the primary. These are parametrized by
          $\gamma_\parallel\equiv \dot{s}/s$ -- the fractional rate of change of the projected
          separation between the two lenses, and $\gamma_{\perp} \equiv-\dot{\alpha}$ --
          the angular rotation rate of the projected separation axis. The effect of
          $\gamma_\perp$ is to simply rotate the magnification pattern on the sky while a
          nonzero value of $\gamma_\parallel$ changes the magnification pattern itself. These new parameters are
          partially degenerate with other parameters such as parallax and it is very
          rarely possible to constrain the full Keplerian orbit
          \citep{2011ApJ...738...87S,2020A&A...633A..98W}.
    \item \textbf{Orbital motion of the sources:} the effect of the orbital motion of a binary
          source star is less dramatic than for the orbital motion of lens stars because
          the caustic structure stays fixed. \citet{1998A&A...329..361D} showed that six
          additional parameters are needed to describe the full Keplerian orbit of a
          binary source star. \citet{1992ApJ...397..362G,1993ApJ...407..440G} pointed out
          that such events should be rare although it is not clear if that is still the
          case today with the advent of new microlensing surveys.
          There is also a possibility that the secondary source is not a star but a large
          planet resulting in subtle deviations in the primary
          source position as it orbits the barycentar. This is an alternative channel for
          detecting planets using microlensing and it is often
          called \emph{xallarap} (inverse parallax) in the literature
          \citep{2009MNRAS.392.1193R,2021AJ....162...59R,2021AJ....161...84M}.
    \item \textbf{Fine structure in the source profiles:} microlensing in principle allows for
          the measurement of the intensity and shape profile
          of source stars and their possible planets. \citet{1997ApJ...490...38H} investigates
          microlensing of elliptical sources by a single lens as a model for oblate stars and inclined
          accretion discs. \citet{1999ApJ...513..619G} discusses the sensitivity of binary and single lens
          caustic crossing events to spatial and spectral structure stellar atmospheres.
          \citet{2003ApJ...586..527G} focused specifically on the possibility of inferring
          intensity and shape profiles of caustic crossings \emph{planets} in the source plane. Deviations
          in the light curve due to a non-uniform surface brightness profile or shape of the planet are only
          resolved during a short time window when the planet is within about one radius from the caustic.
          \citet{2003ApJ...586..527G} find that there is a possibility of detecting ring-like structures
          around the planet with a $\sim 30$m class telescope and high cadence observations but
          detecting features such as spots and zonal bands will be very difficult to detect. Detecting
          differences in \emph{phase} of the planet may be easier \citep{2001MNRAS.325..305A}.
\end{itemize}

\section{Occultation and phase curve mapping}
\label{sec:occultations}
In this section I will briefly review the theory behind modeling phase curves and
occultation light curves of spherical bodies for the case of emitted light
(isotropic blackbody emission from the body itself) and reflected light
(starlight scattered from the surface or atmosphere of the body). The following is
mostly a short summary of the papers introducing the \textsf{starry} framework
\citep{2019AJ....157...64L,2021arXiv210306275L}.
\subsection{The starry algorithm}
\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/spherical_harmonics.pdf}
        \caption{Real spherical harmonics up to degree $l=5$ computed from
            Equation~\ref{eq:spherical_harmonics}. Figure adapted from Figure 1 in
            \citet{2019AJ....157...64L}.}
        \label{fig:spherical_harmonics}
    \end{centering}
\end{figure}
\subsubsection{Emitted light}
Consider a spherical body of unit radius emitting light isotropically (a
Lambertian emitter) at each point $(\theta,\phi)$ where $\theta$ is the
inclination angle and $\phi$ is the azimuthal angle. To compute a phase curve
or an occultation light curve we need to be able to integrate the flux over the
entire projected disc of the body given the distribution of specific intensity
$I(\theta, \phi)$, the 3D orientation of the body in space, and the location of
a possible occultor relative to the body. These two-dimensional integrals can
always be computed numerically (see for instance the approaches presented in
\citet{2018AJ....156..146F,2018MNRAS.477.2613L}) but
\citet{2019AJ....157...64L} showed that it is possible to compute them
analytically if the specific intensity is first expanded in a basis of
spherical harmonics and then mapped to a different basis more suitable for
evaluating the integrals.

Following \citet{2019AJ....157...64L}, we start by setting up a cartesian
coordinate system on the units sphere, such that
\begin{align}
     & x=\sin \theta \cos \phi \\
     & y=\sin \theta \sin \phi \\
     & z=\cos \theta
\end{align}
and the observer is located on the $z$-axis at $\infty$ such that the projected disc
of the body is centered at the origin of the $xy$ plane with the unit vector
$\hat{\mathbf{x}}$ pointing to the right and $\hat{\mathbf{y}}$ pointing up.
We introduce spherical harmonics $Y_{l m}(\theta, \phi)$ of degree $l\geq 0$ and order
$m\in [-l, l]$ defined  as
\begin{equation}
    Y_{l m}(\theta, \phi)= \begin{cases}\bar{P}_{l m}(\cos \theta) \cos (m \phi) & m \geqslant 0 \\ \bar{P}_{l|m|}(\cos \theta) \sin (|m| \phi) & m<0\end{cases}
    \label{eq:spherical_harmonics}
\end{equation}
When the spherical harmonics defined above are rewritten in terms of $x$, $y$, and $z$
they become polynomials in these variables (see Appendix A in \citet{2019AJ....157...64L}).
We can expand the specific intensity distribution $I(x,y)$ in a spherical harmonic basis
as
\begin{equation}
    I(x, y)=\tilde{\mathbf{y}}^{\intercal}(x, y) \mathbf{y}
    \label{eq:sh_expansion}
\end{equation}
where $\tilde{\mathbf{y}}$ is the basis vector of spherical harmonics  arranged in
increasing order:
\begin{align}
     & \tilde{\mathbf{y}}=\left(\begin{array}{lllll}
                                    Y_{0,0} & Y_{1,-1} & Y_{1,0} & Y_{1,1} & Y_{2,-2}
                                \end{array}\right. \\
     & \left.\begin{array}{lllll}
                 Y_{2,-1} & Y_{2,0} & Y_{2,1} & Y_{2,2} & \cdots
             \end{array}\right)^{\intercal} \quad,
\end{align}
and $\mathbf{y}$ is a vector of scalar \emph{spherical harmonic coefficients}.
$\mathbf{y}$ is a central quantity which defines the \emph{map} (the specific
intensity at every point) of the body.
\citet{2019AJ....157...64L} shows that we can represent $y$ in a polynomial basis (see
\citet{2019AJ....157...64L} for the general expression)
\begin{equation}
    \tilde{\boldsymbol{p}}=\left(\begin{array}{llllllllll}
        1 & x & z & y & x^{2} & x z & x y & y z & y^{2} & \cdots
    \end{array}\right)^{\intercal}
\end{equation}
such that
\begin{align}
    I(x, y) & =\tilde{\mathbf{p}}^{\intercal}(x, y) \mathbf{p}                    \\
            & =\tilde{\mathbf{p}}^{\intercal}(x, y) \mathbf{A}_1 \mathbf{y}\quad,
    \label{eq:intensity_poly_basis}
\end{align}
and also in the Green's basis
\begin{equation}
    \tilde{\boldsymbol{g}}=\left(\begin{array}{llllllllll}
        1 & 2 x & z & y & 3 x^{2} & -3 x z & 2 x y & 3 y z & y^{2} & \cdots
    \end{array}\right)^{\intercal}
\end{equation}
such that
\begin{align}
    I(x, y) & =\tilde{\mathbf{g}}^{\intercal}(x, y) \mathbf{g}              \\
            & =\tilde{\mathbf{g}}^{\intercal}(x, y) \mathbf{A}_2 \mathbf{p} \\
            & =\tilde{\mathbf{g}}^{\intercal}(x, y) \mathbf{A} \mathbf{y}
\end{align}
where $\mathbf{A}_1$ is the change-of-basis matrix from $\mathbf{y}$ to $\mathbf{p}$,
$\mathbf{A}_2$ is the change-of-basis matrix from $\mathbf{p}$ to $\mathbf{g}$, and
$\mathbf{A}\equiv \mathbf{A}_1 \mathbf{A}_2$.

To compute rotational light curves we also need to be able to specify the
orientation of a surface map with coefficients $\mathbf y$. We can rotate the
map using the Wigner rotation matrix $\mathbf{R}(\mathrm{I}, \Lambda, \Theta)$
that rotates $y$ given the body's inclination $\mathrm{I}$, obliquity $\Lambda$
and rotational phase $\Theta$. The rotated map is then $\mathbf{R}(\mathrm{I},
    \Lambda, \Theta) \mathbf{y}$ and Equation~\ref{eq:intensity_poly_basis} can be
rewritten more generally as
\begin{equation}
    I(x, y)=\tilde{\mathbf{p}}^{\intercal}(x, y) \mathbf{A}_1 \mathbf{R}\mathbf{y}
\end{equation}
The total flux measured by an observer is is given by an integral of the specific
intensity over a region S of the projected disc of the body:
\begin{align}
    F & =\oiint I(x, y) \ud S                                                                 \\
      & =\oiint \tilde{\mathbf{p}}^{\intercal}(x, y) \mathbf{A}_{1} \mathbf{R} \mathbf{y} d S \\
      & =\mathbf{r}^{\intercal} \mathbf{A}_{1} \mathbf{R} \mathbf{y}
\end{align}
where $\mathbf{r}$ is a column vector whose $n$-th element is \citep{2019AJ....157...64L}
\begin{equation}
    r_{n} \equiv \oiint \tilde{p}_{n}(x, y) \ud S
\end{equation}
When the entire disc is visible (no occultor)
\begin{equation}
    r_{n}=\int_{-1}^{1} \int_{-\sqrt{1-x^{2}}}^{\sqrt{1+x^{2}}} \tilde{p}_{n}(x, y) \mathrm{d}y \mathrm{d} x
\end{equation}
and the solution to this integral can be expressed in terms of Gamma functions
\citep[Equation 20 in ][]{2019AJ....157...64L}.
$\mathbf{r}$ and $\mathbf{A}_1$ are independent of the map coefficients $\mathbf{y}$
so they can be pre-computed.

Computing the occultation light curves is more complicated because the the body
is partially occulted by an occultor of radius $r$, centered at $(x_o, y_o)$
and the exposed portion of the disc is a function of $(r, x_o, y_o)$. The
general expression for the flux is
\begin{align}
    F & =\oiint I(x, y) \mathrm{d} S                              \\
      & =\mathbf{s}^\intercal  \mathbf{A} \mathbf{R} \mathbf{y} .
\end{align}
where the vector
\begin{equation}
    \mathbf{s}^\intercal \equiv \oiint \tilde{\mathbf{g}}^{\intercal}(x, y) \mathrm{d} S
\end{equation}
is defined to be the solution to the integral.
\citet{2019AJ....157...64L} solves this integral in two steps.  First, we rotate
the coordinate system about the  $z$-axis  so that the occultor lies at along the
$+y$ axis and its center is at
a distance $b=\sqrt{x_o^{2}+y_o^{2}}$ from the origin. This substantially simplifies
the limits of integration.
The second step is to use Green's theorem to express the surface integral of
$\tilde{\mathbf{g}}_n$ as a line integral of a vector function $\mathbf{G}_n$
along the boundary of the visible portion of the occulted disc.
The $n$-th element of the solution vector $\mathbf{s}^\intercal$ is
\begin{equation}
    s_{n}=\oiint \tilde{g}_{n}(x, y) \mathrm{d} S=\oint \mathbf{G}_{n}(x, y) \cdot \ud \mathbf{r}
\end{equation}
where
$\mathbf{G}_{n}(x, y)=G_{n x}(x, y) \hat{\mathbf{x}}+G_{n y}(x, y) \hat{\mathbf{y}}$
is constructed such that
\begin{equation}
    \mathbf{D} \wedge \mathbf{G}_{n}=\tilde{g}_{n}(x, y)
    \label{eq:G_n}
\end{equation}
were $\mathbf{D} \wedge \mathbf{G}_{n}$ is the exterior derivative of $\mathbf{G}_n$
given by
\begin{equation}
    \mathbf{D} \wedge \mathbf{G}_{n} \equiv \frac{\ud G_{n y}}{\ud x}-\frac{\ud G_{n_{x}}}{\ud y}
\end{equation}
in Cartesian coordinates.
\citet{2019AJ....157...64L} provides one possible expression for $\mathbf{G}_n$
which satisfies Equation~\ref{eq:G_n} and shows that the final solution can be written as
\begin{equation}
    s_{n}=\mathcal{Q}\left(\mathbf{G}_{n}\right)-\mathcal{P}\left(\mathbf{G}_{n}\right)
\end{equation}
where $\mathcal{P}(\mathbf{G}_n)$ is the  line integral along the arc of the occultor
of radius $r$ and $\mathcal{Q}(\mathbf{G}_n)$ is the line integral along the arc of the
occulted body of unit radius. Solutions to these integrals are long but they consist of
analytic functions such as sines, cosines and complete elliptic integrals and importantly
they only need to be evaluated once for an arbitrary map with fixed geometry of an
occultation \citep{2019AJ....157...64L}.

The final expression for the total flux for a particular geometrical
arrangement of the occulted body and the occultor can then be written
succinctly as
\begin{equation}
    f = \mathbf{s}^T\mathbf{A}\mathbf{R}'\mathbf{R}\mathbf{y}
    \label{eq:starry}
\end{equation}
Where $\mathbf{R}'$ is a rotation matrix which rotates the map such that the occultor
is placed symmetrically on the $+y$ axis at distance $b$.
Notice that Equation~\ref{eq:starry} defines a linear operation acting on a vector
of spherical harmonic coefficients $\mathbf{y}$.
It is also trivial to generalize this expression to a case of \emph{spectral map}. In
that case we define a matrix $Y$ whose columns are the spherical harmonic coefficients
at different wavelengths and the total flux (spectrum) is then
\begin{equation}
    \mathbf{f} = \mathbf{s}^T\mathbf{A}\mathbf{R}'\mathbf{R}\mathbf{Y}
    \label{eq:starry_mw}
\end{equation}
where $\mathbf{f}$ is a vector of fluxes, one per wavelength bin.

The formalism above can also be extended to account for limb-darkening which is
important when modeling transits of planets across stars or the light curves of
eclipsing binaries. \citet{2020AJ....159..123A} show how a limb-darkening
profile that is an order $l$ polynomial function of
$\mu=z=\sqrt{1-x^{2}-y^{2}}$ can be expressed in terms of the $m=0$ spherical
harmonics up to order $l$. In the case of quadratic limb-darkening
$(l_\mathrm{max}=2)$, the limb darkening profile is
\begin{equation}
    \frac{I(\mu)}{I(1)}=1-u_{1}(1-\mu)-u_{2}(1-\mu)^{2}
\end{equation}
which can be expressed as a sum of spherical harmonics
\citep[Equation 38 in][]{2019AJ....157...64L}. Limb-darkening needs to be
treated separately from the map coefficients $\mathbf{y}$ because it does not rotate along with the map
when rotations $\mathbf{R}$ and $\mathbf{R}'$ are applied. The limb-darkening
coefficients are applied to the map $\mathbf{y}$ as a multiplicative filter
following any rotations which results in another set of spherical harmonic
coefficients because products of spherical harmonics are also spherical
harmonics. Applying limb-darkening raises the degree of the map by the degree
of the limb-darkening.

\subsubsection{Reflected light}
The results derived in the previous section are valid only for light emitted
from the occulted body. Equation~\ref{eq:starry} is a good model for stellar
light curves and secondary eclipse light curves and phase curves of planets
observed in the mid to far infrared wavelengths where the contribution of
scattered starlight to total flux is negligible. Modeling reflected light phase
curves and occultations is far more complicated because one has to take into
account the the nonuniform illumination of the body and the possible presence
of a terminator line (boundary between day and night).
\citet{2021arXiv210306275L} extends the formalism described in the previous
section to the case of reflected light phase curves and occultations for a body
whose spatial \emph{albedo} (specifically the \emph{spherical albedo} -- the
fraction of power incident on a body at a given wavelength that is scattered
into space in all directions) distribution is expanded in a basis of spherical
harmonics. Thus, analogous to Equation~\ref{eq:sh_expansion} we have
\begin{equation}
    A(x, y)=\tilde{\mathbf{y}}^{\intercal}(x, y) \mathbf{y}
\end{equation}
where $A$ is the albedo. We work under the assumption of isotropic (Lambertian)
scattering of light at every point on the body which means that the illumination
profile $\mathcal{I}$ of the surface is given by Lambert's law:
\begin{equation}
    \mathcal{I}\left(\vartheta_{\mathrm{i}}\right)=\mathcal{I}_{0} \max \left(0, \cos \vartheta_{\mathrm{i}}\right)
\end{equation}
where $\vartheta_i$ is the angle between the incident radiation at the surface normal
and $\mathcal{I}_0$ is the maximum illumination. In case of, for example, a planet illuminated
by its host star \citep[Appendix A.2 in][]{2021arXiv210306275L}
\begin{equation}
    \mathcal{I}_{0}=\frac{f_{s}}{\pi r_{\mathrm{s}}^{2}}
\end{equation}
where $r_s$ is the distance between the planet and the star in units of planet's radius
and $f_s$ is the stellar flux measured at the observer in some arbitrary units.
We assume that $f_s=1$ so that all fluxes are defined as the fraction of the flux
of the illumination source at the observer.
It is the presence of the day/night terminator that complicates the calculation
of total flux the most. The limits of integration end up depending on a
solution to a quartic equation specifying the points of intersection between
the occultor and the day/night terminator line and the solution to those
integrals are a function of \emph{incomplete} elliptic integrals as opposed to
complete elliptic integrals as was the case for occultations in emitted light.
The full derivation of the total flux is provided by \citet{2021arXiv210306275L} and
I will only briefly summarize  the result here for completeness.

Under the Lambertian scattering assumption the observed intensity at any point
of the surface of the body is is proportional to the cosine of the angle
$\vartheta_i$ between the incident light rays and the surface normal. All
points for which $\vartheta_i\geq \pi/2$ have intensity of zero (they're
unilluminated). Let's assume that the point source is placed at sky coordinates
$\left(x_{\mathrm{s}}, y_{\mathrm{s}}, z_{\mathrm{s}}\right)$ in units of the
radius of the illuminated body. The day/night terminator on the body is a
half-ellipse with a semi-major axis equal to unity and a (signed) semi-minor
axis equal to
\begin{equation}
    b=-\frac{z_{\mathrm{s}}}{r_{\mathrm{s}}}
\end{equation}
where $r_{\mathrm{s}}=\sqrt{x_{\mathrm{s}}^{2}+y_{\mathrm{s}}^{2}+z_{\mathrm{s}}^{2}}$
is the distance to the source. The angle by which the semi-major axis of this ellipse is
rotated away from the $+x$-axis is  is
\begin{equation}
    \theta=-\arctan 2\left(x_{\mathrm{s}}, y_{\mathrm{s}}\right)
\end{equation}
Under the assumption that $rs \gg 1$, the illumination $\mathcal{I}$ at point
$(x,y)$ on the projected disc is \citep{2021arXiv210306275L}
\begin{equation}
    \mathcal{I}\left(b, \theta, r_{\mathrm{s}} ; x, y\right)=\max \left(0, I\left(b, \theta, r_{\mathrm{s}} ; x, y\right)\right)
    \label{eq:illumination}
\end{equation}
where
\begin{align}
    I\left(b, \theta, r_{\mathrm{s}} ; x, y\right) & =\frac{1}{\pi r_{\mathrm{s}}^{2}} \cos \vartheta_{\mathrm{i}}                                                      \\
                                                   & =\frac{1}{\pi r_{\mathrm{s}}^{2}}\left(-b_{\mathrm{c}} \sin \theta x+b_{\mathrm{c}} \cos \theta y-b z(x, y)\right)
\end{align}
with  $b_{\mathrm{c}} \equiv \sqrt{1-b^{2}}$  and $z(x, y)=\sqrt{1-x^{2}-y^{2}}$.
The illumination $\mathcal{I}$ is a dimensionless quantity normalized such that the
integral of $A\,\mathcal{I}$ over the units disc is equal to the flux measured by the
observer as a fraction of the flux of the illumination source.
To compute the total flux, we need to weigh each of the terms in the Green's basis
and integrate them over the visible portion of the body's disc to obtain
the reflected light solution vector $\mathbb{s}^\intercal$.
Evaluating these integrals is unfortunately intractable because of the piecewise
function in Equation~\ref{eq:illumination}. It is more tractable to weight the basis
terms by the function $I$ and modify the limits of integration such that the nightside
of the body is excluded. \citet{2021arXiv210306275L} shows that $I$ can be expressed
in the form of a linear operator $\mathbf{I}$ to weight a map vector  in the polynomial
basis $\tilde{\mathbf{p}}$ by the illumination profile. The total flux is then
\begin{equation}
    f=\mathbb{s}^\intercal\left(b, \theta^{\prime}, b_{\mathrm{o}}, r_{\mathrm{o}}\right) \mathbf{A}_{\mathbf{2}} \mathbf{I}\left(b, \theta^{\prime}, r_{\mathrm{s}}\right) \mathbf{A}_{\mathbf{1}} \mathbf{R}^{\prime}\left(x_{\mathrm{o}}, y_{\mathrm{o}}\right) \mathbf{R}(\mathrm{I}, \Lambda, \Theta) \mathbf{y}
\end{equation}
where
\begin{equation}
    \theta^{\prime}=\arctan 2\left(x_{\mathrm{o}}, y_{\mathrm{o}}\right)-\arctan 2\left(x_{\mathrm{s}}, y_{\mathrm{s}}\right)
    \label{eq:starry_ref}
\end{equation}
is the angle of the terminator in frame $\mathcal{F}^\prime$.
In case the illuminated body is not occulted we have
\begin{equation}
    f_{0}=\mathbb{r}^{\intercal}(b) \mathbf{I}\left(b, \theta^{\prime \prime}, r_{\mathrm{s}}\right) \mathbf{A}_{\mathbf{1}} \mathbf{R}^{\prime \prime}\left(x_{\mathrm{s}}, y_{\mathrm{s}}\right) \mathbf{R}(\mathrm{I}, \Lambda, \Theta) \mathbf{y}
    \label{eq:starry_ref_phase}
\end{equation}
where $\theta^{\prime\prime}=0$ is the angle of the terminator in frame
$\mathcal{F}^{\prime\prime}$ by construction. $\mathbf{R}^{\prime\prime}$ rotates the
body through an angle $\arctan 2\left(x_{\mathrm{s}}, y_{\mathrm{s}}\right)$ so the
semi-major axis of the terminator is aligned with the $x^{\prime \prime}$ axis.
The solutions to integrals
$\mathbb{r}^{\intercal}(b)$ and $\mathbb{s}^{\intercal}\left(b, \theta^{\prime}, b_{\mathrm{o}}, r_{\mathrm{o}}\right)$
are anything but trivial, they are derived in the appendices of
\citet{2021arXiv210306275L}. The key point is that the model is once again linear and
the matrices in Equations~\ref{eq:starry_ref} and \ref{eq:starry_ref_phase} need to be
computed only once for a specific occultation/illumination geometry.
\citet{2021arXiv210306275L} also derives a solution for the total flux in case the
light source is extended (the assumption that $r_s\gg 1$ is not satisfied) and
in case the body is not a perfect Lambertian reflector.

\subsection{The starry code}
The method described in the previous section is implemented in the
\textsf{Python} package
\textsf{starry}\footnote{\url{https://starry.readthedocs.io/en/latest/}}.
\textsf{starry} is many orders of magnitude faster and more accurate than
similar codes which rely on numerical integration. It enables computation of
rotational light curves, planetary transits, secondary eclipse light curves and
planet-planet occultations in both emitted and reflected light. Recent
extensions to \textsf{starry}, not mentioned in Section~\ref{sec:occultations},
extended the spherical harmonic formalism to the problem of Doppler imaging
\citep{2021arXiv211006271L} and add support for computing occultation light
curves of oblate bodies \citep{2022ApJ...925..185D}. In
Chapters~\ref{ch:mapping_io} and \ref{ch:mapping_exoplanets} we make extensive
use of \textsf{starry} to explore solutions to the \emph{inverse problem} --
how can we obtain an estimate of the map coefficients $\mathbf{y}$ which best
explain the observed light curve? \textsf{starry} abstracts the complexities of
computing the total flux and allows us to treat this problem as a linear
problem of the form
\begin{equation}
    f = \mathbf{X}\mathbf{y}
    \label{eq:starry_linear_model}
\end{equation}
where  $\mathbf{X}$ is computed by \textsf{starry} depending on the geometry of an
occultation or transit event. The inverse problem is highly degenerate
and making progress requires  imposing some prior structure on the solution
$\mathbf{y}$.

In Chapter~\ref{ch:mapping_io} we use \textsf{starry} to model occultation
light curves of a Solar System object -- Jupiter's moon Io, as it is occulted
by Jupiter. Thanks to the comparatively close distance to Io relative to
objects outside of the Solar System, these light curves are very high quality
we are able to fit very high order maps $l=20$ and test our models in a high
signal-to-noise regime. We also discuss how possible approaches to modeling
time-dependent maps in which case the spherical harmonic coefficients are
time-dependent: $\mathbf{y}=\mathbf{y}(t)$. In
Chapter~\ref{ch:mapping_exoplanets} we tackle a very similar problem but in a
very different regime of exoplanet eclipse mapping where the signal-to-noise is
orders of magnitude worse than in the case of Io. In both cases we mostly focus
on thermal light curves rather than light curves in reflected light.

\section{Statistical inference -- introduction}
\label{sec:inference_intro}
Having covered the basic physics behind gravitational lensing and occultation mapping I
now turn to the subject of statistical inference in general and more specifically --
Bayesian statistical inference. Bayesian (as opposed to frequentist)
provides a straightforward and elegant theoretical framework for solving inverse
problems of the kind we mentioned
in Sections~\ref{sec:microlensing}  and \ref{sec:occultations}. These are problems
where we have little data per object of interest, the data is very noisy,
the physical models are relatively well understood but their parameters don't map
to observables in a straightforward way and there are multiple competing, sometimes
physically  quite different, models which provide a good explanation for the data.
Having said that, I am not particularly dogmatic about the use of Bayesian methods
and I will occasionally steer away from the classical approach and point out where
a less Bayesian approach is often superior in practice.  What is most important is the
use of \emph{probabilities} for encoding uncertainty about the physical world.

I start by reviewing basic probability theory and briefly mention the
frequentist interpretation. I then introduce two key concepts, the likelihood
function -- a recipe for generating plausible datasets given model parameters,
and priors -- what we know about the model parameters prior to gathering any
data. Next, I briefly summarize fundamental concepts such as the curse of
dimensionality and the no-free-lunch-theorem. Finally I end the section by
comparing machine learning to statistical inference within the physical
sciences.

\subsection{Probability theory}
There are two different interpretations of probability. In the
\emph{frequentist} interpretation probabilities are \emph{frequencies of
    events} which are repeated in a large number of trials (think of a repeated
coin tosses). An alternative to the frequentist interpretation is the
\emph{Bayesian} interpretation in which probabilities represent \emph{degrees
    of belief} about the subject of interest. In both cases probabilities are
necessarily positive numbers between zero and one. The major advantage of the
Bayesian interpretation is that it allows one to reason about the probability
of one-off events such as ``person X wins the election'' or ``this microlensing
event is a binary lens event''. The differences between the two interpretations
are not just philosophical because which interpretation one subscribes to can
determine how one approaches a particular statistical inference problem and
which sets of tools one uses. Both approaches are commonly used within the
physical sciences depending on the field and the problem at hand. For instance,
in case of particle physics frequentist methods are much more common because
particle physics experiments often involve billions of repeated experiments in
particle accelerators. One the other hand, Bayesian methods are more popular in
astronomy because we cannot do true repeated experiments with astrophysical
objects.

The mathematical rules for manipulating probabilities are quite
straightforward. In probability theory we use \emph{random variables} which can
be either discrete or continuous and their possible values are described with
probability distributions over all possible values the variable is allowed to
take. For discrete random variables we can talk of probabilities that a given
variable assumes a particular value, say $5$, but with continuous random
variables we can only talk of probabilities that a given variable is in a
particular range of values, say $[0,1]$. Now let's consider a continuous
\emph{random variable} $x$ with an associated probability density function
(pdf) $p(x)$\footnote{The notation $p(x)$ we use is somewhat overloaded. In
    statistics, the proper notation for a probability density function is
    $p_{X}(x)$ where $X$ is a random variable and $x$ is a particular realization
    of that random variable.} Since $x$ has to take on \emph{some values} within
its domain we have the requirement that
\begin{equation}
    \int p(x)\,\ud x=1
    \label{eq:normalization_condition}
\end{equation}
where the integral is taken over the entire domain of $x$.
In order for Equation~\ref{eq:normalization_condition} to hold,
$p(x)$ has to have units of $x^{-1}$ so that
the integral is a dimensionless number.

In general, we are usually dealing with several parameters at once in which
case we are interested in \emph{joint} probability distributions over all the
parameters. Consider random variables $x_1$ and $x_2$ with a joint probability
density function $p(x_1,x_2)$ which when integrated over a particular interval
gives the probability that both $x_1$ and $x_2$ are contained within that
interval. These joint probabilities can be decomposed into \emph{conditional
    probabilities} using the so called product rule:
\begin{align}
    p(x_1,x_2) & =p(x_1)\,p(x_2\lvert x_1) \\
    p(x_1,x_2) & =p(x_1\lvert x_2)\,p(x_2)
    \label{eq:product_rule}
\end{align}
where $p(x_2\lvert x_1)$ is a pdf for $x_2$ \emph{conditional on
    $x_1$ assuming a certain value.} $p(x_2\lvert x_1)$ is every bit as
valid pdf as $p(x_2)$, it has the same units and it has to integrate to 1.
Combining \ref{eq:product_rule} results in the famous \emph{Bayes's theorem}:
\begin{equation}
    p(x_1\lvert x_2)= \frac{p(x_2\lvert x_1)\,p(x_1)}{p(x_2)}
    \label{eq:bayes_theorem}
\end{equation}
Bayes's theorem is simply a rule for converting one kind of a conditional probability
into another.
Given a pdf $p(x_1,x_2)$, we can \emph{integrate out} or
\emph{marginalize} parameters that we are not interested in to obtain a distribution over
parameters of interest:
\begin{equation}
    p(x_1)=\int p(x_1,x_2)\textrm{d}x_2=\int p(x_1\lvert
    x_2)\,p(x_2)\textrm{d}x_2
    \label{eq:marginalization}
\end{equation}
where we applied Bayes's theorem in the second inequality.
In cases where the parameter space is high dimensional these integrals might be
impossible to calculate analytically but they can (sometimes) be estimated using sampling
algorithms as we shall se in subsequent sections.
In some cases, we may be able to factor out $p(x_1,x_2)$ as
If $x_1$ and $x_2$ are \emph{statistically independent} we can factor out $p(x_1,x_2)$ as
\begin{equation}
    p(x_1,x_2)=p(x_1)\,p(x_2)\,.
\end{equation}
Independence is a very common assumption in statistical modelling. For instance, when modeling light curves
of microlensing events or planetary transits we often assume that flux $f_i$ measured at time $t_i$ is independent
of flux $f_j$ measured at time $t_j$  for all measurement times $t_i,t_j$ so that
\begin{equation}
    p(\{f\}_{i=1}^N)=\prod_{i=1}^Np(f_i)
    \label{eq:likelihood_indep}
\end{equation}
If the pdf-s $p(f_i)$ also happen to be equal, we say that the data is independent and
identically distributed (``iid''). If the data is independent but not identically distributed
we say that the uncertainties are \emph{heteroscedastic}.
The iid assumption is not that common in astronomy because we tend to have some idea of per-data-point
uncertainties.

\subsubsection{Expectation values and the mode}
The \emph{mean} or the \emph{expected value} of a distribution $p(\vec{x})$,
usually denoted by $\mu$, is a measure of the ``center'' of a distribution and
it is defined as
\begin{equation}
    \mathbb{E}[x] \equiv\int x \,p(x) \ud x
\end{equation}
Similarly, the \emph{variance} is a measure of the ``spread'' of a distribution:
\begin{align}
    \mathbb{V}[x] & \equiv\mathbb{E}\left[(x-\mu)^{2}\right]=\int(x-\mu)^{2} p(x) d x                                           \\
                  & =\int x^{2} p(x) \ud x+\mu^{2} \int p(x) \ud x-2 \mu \int x p(x) \ud x=\mathbb{E}\left[x^{2}\right]-\mu^{2}
\end{align}
The variance is usually denoted by $\sigma^2$ and the square root of $\sigma^2$ is the
\emph{standard deviation}.
For a \emph{multivariate} random variable with $D$ dimensions then the variance generalizes to
a symmetric, positive semi definite matrix called the \emph{covariance matrix}:
\begin{align}
    \operatorname{Cov}[\vec{x}] & \equiv\mathbb{E}\left[(\vec{x}-\mathbb{E}[\vec{x}])(\vec{x}-\mathbb{E}[\vec{x}])^{\intercal}\right] \equiv \boldsymbol{\Sigma}                                                                                                                                                                           \\
                                & =\left(\begin{array}{cccc}
                                             \mathbb{V}\left[x_{1}\right]                & \operatorname{Cov}\left[x_{1}, x_{2}\right] & \cdots & \operatorname{Cov}\left[x_{1}, x_{D}\right] \\
                                             \operatorname{Cov}\left[x_{2}, x_{1}\right] & \mathbb{V}\left[x_{2}\right]                & \cdots & \operatorname{Cov}\left[x_{2}, x_{D}\right] \\
                                             \vdots                                      & \vdots                                      & \ddots & \vdots                                      \\
                                             \operatorname{Cov}\left[x_{D}, x_{1}\right] & \operatorname{Cov}\left[x_{D}, x_{2}\right] & \cdots & \mathbb{V}\left[x_{D}\right]
                                         \end{array}\right)
\end{align}

In the general case, we can compute expectation values of an arbitrary function
$g(\vec{x})$ under the probability distribution $p(\vec{x})$:
\begin{equation}
    \mathbb{E}\left[g(\vec{x})\right]=\int g(\vec{x})\,p(\vec{x})\ud x
    \ud\theta
\end{equation}
Some examples of $g$ include higher moments of the distribution, the median and
quantile estimates. The \emph{mode} of $p(\vec{x})$
\begin{equation}
    \vec{x}^{*}=\underset{\vec{x}}{\operatorname{argmax}}\,p(\vec{x})
\end{equation}
on the other hand is not an expectation value. It is the global maximum of the
function $p(\vec{x})$.
Estimating the mode involves computing the \emph{derivatives} of the density
$p(\vec{x})$, whereas expectation values require computing
\emph{integrals} over the space $\vec{x}$. Computing either
the mode or expectation values
for nontrivial densities $p(\vec{x})$ is a very hard and often impossible problem
(more on this in Section~\ref{sec:inference_practice}).

\subsubsection{Central Limit Theorem}
Consider $N$ \emph{independent and identically distributed} random variables
with pdf's $p_n(x)$ which are not necessarily Gaussian. Let $S_N$ be the sum
the random variables. The \emph{Central Limit Theorem} (CLT) says that as $N$
increases, the distributions of this sum approaches the Gaussian distribution
\begin{equation}
    p\left(S_{N}=u\right)=\frac{1}{\sqrt{2 \pi N \sigma^{2}}} \exp \left(-\frac{(u-N \mu)^{2}}{2 N \sigma^{2}}\right)
\end{equation}
and the distribution of the quantity
\begin{equation}
    Z_{N} \equiv \frac{S_{N}-N \mu}{\sigma \sqrt{N}}=\frac{\bar{x}-\mu}{\sigma / \sqrt{N}}
\end{equation}
where $\bar{x}$ is the \emph{sample mean} $\frac{1}{N}\sum_{i=1}^Nx_i=S_N/N$
converges to the Gaussian distribution with zero mean and unit variance.
Because lots of random variables representing real world processes can be seen as sums
of i.i.d. independent random variables the Gaussian distribution is all over the place
in statistics. One should keep in mind that the assumptions underlying the CLT are
not always satisfied and also the convergence towards a Gaussian is
more rapid in the bulk of the distribution around the mean than in the tails of the
distribution.

\subsubsection{Change of variables}
Given a univariate random variable $x$ the effect of some deterministic
transformation $y(x)$ is to stretch and shift the distribution $p(x)$. For an
infinitesimal interval the \emph{probability mass} is the same in both
variables so $p(x) \ud x=p(y) \ud y$ which yields the \emph{change of variables
    formula}
\begin{equation}
    p_{y}(y)=p_{x}(x)\left|\frac{\ud x}{\ud y}\right|
\end{equation}
In the multivariate case, if $\vec{f}$ is an invertible function mapping $\mathbb{R}^n$
to $\mathbb{R}^n$ with inverse $\vec{g}$, the pdf of $\vec{y}=\vec{f}(\vec{x})$ is
\citep{murphy_book_2022}
\begin{equation}
    p_{y}(\vec{y})=p_{x}(\vec{g}(\vec{y}))\left|\operatorname{det}\mathbf{J}_{g}(\vec{y})\right|
\end{equation}
where $\vec{J}_{g}=\frac{\ud \vec{g}(\vec{y})}{\ud \vec{y}^{\intercal}}$ is the Jacobian of $\vec{g}$
and $\operatorname{det}|\mathbf{J}(\vec{y})|$ is the determinant of the Jacobian.

\subsubsection{Kullback Leibler divergence}
A common way of measuring ``similarity'' between two distributions $p(x)$ and
$q(x)$ is using the \emph{Kullback-Leibler divergence} (KL divergence for
short):
\begin{equation}
    D_{\mathrm{KL}}(p \| q)=\int_{-\infty}^{\infty} p(x) \ln \left(\frac{p(x)}{q(x)}\right) \ud x
    \label{eq:kl_divergence}
\end{equation}
where $D_{\mathrm{KL}}(p \| q)$ should be read as ``the KL divergence from p to q''.
This is not a measure of ``distance'' between the two distributions because
it is not a symmetric quantity. The KL divergence is best understood as a quantity which
quantifies \emph{how surprised one would be if one expected to encounter distribution $p(x)$
    but instead saw distribution
    $q(x)$}\footnote{\url{https://wiki.santafe.edu/images/a/a8/IT-for-Intelligent-People-DeDeo.pdf}}.
This quantity is important in the context of variational inference which is covered in
Section~\ref{sec:inference_practice}.

\subsection{Bayes' theorem in practice}
\label{ssec:likelihood_function}
Consider a general inference problem where we collect some data $\mathcal{D}$ and we are interested
in the joint probability distribution for parameters $\boldsymbol\theta$ of particular model.
We can then rewrite Bayes' theorem as
\begin{equation}
    p(\vec{\theta} | \mathcal{D})=\frac{p(\vec{\theta}) p(\mathcal{D} | \vec{\theta})}{p(\mathcal{D})}=\frac{p(\vec{\theta}) p(\mathcal{D} | \vec{\theta})}{\int p\left(\vec{\theta}^{\prime}\right) p\left(\mathcal{D} |\vec{\theta}^{\prime}\right) d \vec{\theta}^{\prime}}
    \label{eq:bayes_theorem2}
\end{equation}
In Equation~\ref{eq:bayes_theorem2} the term $p(\mathcal{D}|\vec{\theta})$  is a probability
distribution over the data $\mathcal{D}$ given the parameters of the model $\vec{\theta}$.
When this term is seen as a function of the parameters given a fixed dataset, it is
called the \emph{likelihood function }  or the \emph{likelihood} for short. $p(\boldsymbol\theta)$
is to so called \emph{prior} distribution over $\vec{\theta}$ and it represents our beliefs
about $\vec{\theta}$ prior to having observed the data $\mathcal{D}$. When the prior distribution is
multiplied by the likelihood we obtain, up to a normalizing constant $p(\mathcal{D})$, the posterior
distribution over $\vec{\theta}$ given the data $\mathcal{D}$.
The best way to think about Equation~\ref{eq:bayes_theorem2} is seeing at as a rule
for updating beliefs. Our prior beliefs about the parameters of the model
$p(\mathrm{parameters})$ should be updated to posterior beliefs $
    p(\mathrm{parameters} | \mathrm{data})$ by making use of the likelihood
$p(\mathrm{data} | \mathrm{parameters})$. If the data is useful, the likelihood is generally
much narrower than the prior pdf and the choice of priors does not significantly
influence the final results.

\subsubsection{The likelihood}
Let's cover each of these three terms in more detail, starting with the most
important one -- the likelihood. The likelihood can be seen as a \emph{recipe
    for generating plausible datasets $\mathcal{D}$} (a \emph{generative model})
given a specific realization of the parameters $\vec{\theta}$. For the kinds of
models we work with in astronomy, the likelihood consists of two parts. The
first part is (usually deterministic) function which describes the physical
system of interest. This is for instance the predicted flux for a binary
microlensing event with a particular trajectory. The second part is the
\emph{noise model} which describes the statistical properties of the noise
inherent in any physical measurement. We often assume that the noise is
independent and Gaussian. It is very common to use the likelihood as is and
optimize it with respect to the model parameters $\vec{\theta}$ in a procedure
called \emph{maximum likelihood estimation} (MLE) to find a single point in the
parameter space $\vec{\theta}^\star$ which maximizes the likelihood (more on
this in Section~\ref{ssec:mle_estimation}).

We can also marginalize the likelihood over a subset of parameters. Say that we
have a likelihood function $p(\vec{\theta} ,\vec{\alpha}|\mathcal{D})$ where
$\vec{\alpha}$ is a set of \emph{nuisance parameters} which we are not
particularly interested in. To marginalize over these nuisance parameters we
need to introduce a prior\footnote{Not introducing a prior leads to a
    dimensionally incorrect result.} $p(\vec{\theta}|\vec{\theta})$ which may
potentially also depend on the $\vec{\theta}$ and the marginalized likelihood
is then
\begin{equation}
    p(\mathcal{D} | \vec{\theta})=\int p(\mathcal{D} | \vec{\theta}, \vec{\alpha})\,p(\vec{\alpha} |\vec{\theta}) \ud \vec{\alpha}\quad .
\end{equation}
The problem structure where where $\vec{\alpha}$ is some set of parameters
we don't particularly care about such and $\vec{\theta}$ are the physical
parameters of interest is very common in astronomy. For instance, in single lens gravitational microlensing estimating
the distribution $p(t_E)$ over the parameter of interest $t_E$ requires marginalizing over
parameters such as $F_S, F_B, t_0$ and $u_0$.

Not all statistical models can be written in terms of the likelihood function.
Most machine learning models for example do not have a simple probabilistic
interpretation\footnote{Indeed, finding probabilistic interpretations of
    complex machine learning models such as deep neural networks is an active
    research area.}. If we can write down the likelihood function for a certain
problem, we say that we have a \emph{generative model for the data} because the
likelihood can be used to generate mock data sets.

\subsubsection{Priors}
The second term of interest is the prior $p(\vec{\theta})$. In principle, the
prior pdf-s should capture all of the prior information we might have about the
model parameters prior to gathering new data; in practice, we don't want to
bias our results so we often try to use ``weakly informative'' priors -- priors
that are informative enough that they exclude unreasonable parameter values but
aren't so narrow that they rule out sensible values which may be favored by the
likelihood. For example, we might have prior information such as ``the mass of
the planet should be positive'' in which case a suitable weakly informative
prior on the mass parameter should first of all exclude negative values but
also potentially exclude values which are nonsensically large for the problem
at hand. There are also the so called ``non-informative prior'' such as
Jeffrey's prior and maximum entropy priors. Although these priors can be of use
in some cases, in general \textbf{there no such thing as a truly non
    informative prior}. Although a prior for a particular parameter $\theta$ might
be uninformative in one set of coordinates, it is often informative for a
certain transformation of the parameter $g(\theta)$. For example, a uniform
prior in $\theta$ is not uniform in $\ln\theta$. Having said that, it is often
worth the effort to construct priors which respect certain symmetries of the
problem the we know to be true.

Priors should not be seen in isolation, rather, they should be seen in the
context of the likelihood \citep{arXiv:1708.07487}. To see that this is the
case consider drawing samples from the prior $\vec{\theta}\sim
    p(\vec{\theta})$, plugging those values into the likelihood distribution
$p(\mathcal{D}|\vec{\theta})$ and then generating mock datasets, a process
called \emph{prior predictive checking}. This process makes it easy to spot
priors which in combination with the likelihood yield predictions of what the
data should like like which do not match our expectations. An excellent highly
in depth study on choosing priors is available
\href{https://betanalpha.github.io/assets/case_studies/prior_modeling.html}{here}.

Depending on how informative the data is we differentiate between two different
regimes. We say that the inference process is \emph{data driven} if the data
constrains the model parameters so well that the choice of the priors is
effectively irrelevant. The other regime is said to be \emph{prior driven},
when the data is so weakly informative that the shape of the posterior pdf-s is
similar to the prior pdf. In general, in any statistical analysis involving
priors, one should check the sensitivity of the scientific conclusion to
assumptions of different priors. No statistical inference procedure is free of
assumptions but the use of priors and likelihoods makes those assumptions more
explicit.

Priors are sometimes the goal of the inference. For example, if the prior on
the prior pdf $p(\vec{\theta})$ itself depends on some other set of parameters
$\vec{\beta}$, then we can marginalize out all of the $\vec{\theta}$ parameters
and we are left with a likelihood for the parameters $\vec{\beta}$ of the
prior.
\begin{equation}
    p(\mathcal{D}\lvert\vec{\beta})=\int p(\mathcal{D}\lvert\vec{\theta})\,p(\vec{\theta}\lvert
    \vec{\beta})\ud \vec{\theta}
\end{equation}
This can then be used, using Bayes's theorem, to calculate the posterior
over the prior parameters $\bm\beta$.
\begin{equation}
    p(\bm\beta\lvert\mathcal{D})\propto p(\mathcal{D}\lvert\vec{\beta})
    \,p(\bm\beta)
\end{equation}
The parameters $\bm\beta$ characterizing the prior are usually called
\emph{hyperparameters} or \emph{population level parameters}. Similarly, the priors
for those parameters are called \emph{hyperpriors}.
The whole process is called \emph{hierarchical Bayesian inference}
(in astronomy literature),  or \emph{multilevel modelling, hierarchical modelling,
    nested modelling, mixed models or random effects models}
(in statistics and adjacent fields).
Hierarchical modelling is an very useful method for estimating
population-wide properties given data about individual objects. For example,
in microlensing observations of exoplanets the individual parameters could be the
masses of individual planets while the hyperparameters would describe the
population the shape of the planetary mass function. We shall return to this topic
in Chapter~\ref{ch:microlensing}.

\subsubsection{The evidence}
The final component of Equation~\ref{eq:bayes_theorem2} is the normalization
constant $ p(\mathcal{D})$ in the denominator:
\begin{equation}
    \mathcal{Z}\equiv p(\mathcal{D})=\int p(\mathcal D\lvert\boldsymbol\theta)\,p(\boldsymbol\theta)\textrm{d}\boldsymbol\theta
    \label{eq:evidence}
\end{equation}
is often called the \textsl{fully marginalized likelihood} because
it is a likelihood marginalized over all of the parameters of the model.
Sometimes
it is also called the \emph{Bayesian evidence}.
$\mathcal{Z}$ is general extremely difficult to calculate because it is an
integral over a potentially very high dimensional parameter space $\vec{\theta}$.
Fortunately, in most cases, we can obtain the posterior distribution without having
to calculate $\mathcal{Z}$.

%% mention the likelihood principle

\subsection{Maximum likelihood estimation}
One of the most popular inference methods is called \emph{maximum likelihood
    estimation} (MLE). Maximum likelihood estimation answers the question:
\textbf{what choice of parameters $\vec{x}$ makes my data most likely} as
opposed to the question \textbf{what are the most likely values of the
    parameters $\vec{x}$ having observed the data $\mathcal{D}$}\footnote{The
    distinction between the two questions is subtle, but important. The parameters
    which maximize the likelihood might be very different from those which are most
    ``sensible'' and which best describe the data.} which necessitates the
multiplication of the likelihood with a prior. Since we don't incorporate the
prior, it is not a Bayesian method. The output of MLE is a \emph{point
    estimate} of the parameters $\hat{\vec{\theta}}$. In a Bayesian viewpoint, the
MLE is equivalent to MAP estimation under a uniform prior $p(\vec{\theta})$.
The MLE estimate is obtained by maximizing the logarithm\footnote{Because
    probabilities in most region of the parameter space are very small numbers we
    always use the log transform when optimizing likelihoods or posteriors.} of the
likelihood $p(\mathcal{D}\lvert\vec{\theta})$ (or equivalently, minimizing the
negative log likelihood) with respect to the parameters $\vec{\theta}$:
\begin{equation}
    \hat{\boldsymbol{\theta}}_{\mathrm{mle}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}}
    \ln p\left(\mathcal{D}\lvert \boldsymbol{\theta}\right)
\end{equation}

\subsubsection{MLE for a multivariate Gaussian}
The log-likelihood of multivariate Gaussian distribution without the
unnecessary constants is
\begin{equation}
    L L(\boldsymbol{\mu}, \boldsymbol{\Sigma})=\ln p(\mathcal{D} \lvert \boldsymbol{\mu}, \boldsymbol{\Sigma})=\frac{N}{2} \ln |\vec{\Sigma}^{-1}|-\frac{1}{2} \sum_{n=1}^{N}\left(\boldsymbol{y}_{n}-\boldsymbol{\mu}\right)^{\intercal} \vec{\Sigma}^{-1}\left(\boldsymbol{y}_{n}-\boldsymbol{\mu}\right)
\end{equation}
where $\vec{y}_n$ are the data points and $\vec{\Sigma}^{-1}$ is
the inverse covariance matrix, also knowns as the \emph{precision matrix}.
The MLE for the mean parameter $\hat{\vec{\mu}}$
is given by the sample mean (empirical mean) of the data \citep{murphy_book_2022}:
\begin{equation}
    \hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{y}_{n}=\overline{\boldsymbol{y}}\quad .
\end{equation}
Similarly, the MLE of the the covariance matrix $\hat{\vec{\Sigma}}$ is equal to
the empirical covariance matrix
\begin{equation}
    \hat{\boldsymbol{\Sigma}}=\frac{1}{N} \sum_{n=1}^{N}\left(\boldsymbol{y}_{n}-\overline{\boldsymbol{y}}\right)\left(\boldsymbol{y}_{n}-\overline{\boldsymbol{y}}\right)^{\intercal}
\end{equation}
Thus, whenever we compute an empirical mean and the covariance for a given datasets we
are implicitly assuming that the data is normally distributed and obtaining an MLE
estimate of that distribution.

\subsection{Least squares linear regression}
\label{ssec:least_squares}
In this section we cover the classical non Bayesian formulation of linear
regression, also known as \emph{least squares regression} and in the next
section we generalize to the Bayesian case. In a (multiple) linear regression
model we have (noisy) measurements $\vec{y}\in \mathbb{R}^n$ (\emph{dependent
    variable}), an $n\times p$ matrix $\vec{M}\in \mathbb{R}^{n\times p}$ of inputs
(\emph{design matrix} consisting of \emph{independent variables},
\emph{covariates}, or \emph{features}) and a vector of \emph{regression
    coefficients} or \emph{weights} $\vec{w}\in \mathbb{R}^p$. This is a very
general model because the matrix $\vec{M}$ can consist of basis functions such
as polynomials, spherical harmonics, Fourier modes, wavelets etc. so although
the model is linear in the parameters (regression coefficients), it is not
generally linear in the raw input data. If we assume that the observations
$\vec{y}$ have Gaussian noise we can write down the likelihood of this model as
\begin{equation}
    p(\vec{y}\lvert \vec{w})=\mathcal{N}(\vec{y}\lvert\vec{M}\vec{w}, \vec{C})
\end{equation}
where $\vec{C}$ is the data covariance matrix and $\mathcal{N}$ is shorthand notation for
the multivariate normal distribution. In astronomical application it is usually
assumed to be diagonal and we might have some idea for the scale of the variance
of the diagonal elements (the ``errorbars'').
The negative log likelihood (NLL) is then equal (ignoring constants)  to
\begin{equation}
    \operatorname{NLL}(\vec{w})=\frac{1}{2}(\vec{M} \vec{w}-\vec{y})^{\intercal}\vec{C}^{-1}(\vec{M} \vec{w}-\vec{y})
\end{equation}
which is just the well known least squares objective or (half the) $\chi^2$ as it is known
in astronomy.

\subsubsection{Underdetermined case ($p < n$)}
For now, let's assume that the inverse data covariance matrix $\vec{C}^{-1}$ is
a diagonal matrix with the the same variance $\sigma^2$ for every data point.
In that case the MLE solution for the weights $\vec{w}$ is the solution to the
following (convex) optimization problem:
\begin{equation}
    \hat{\vec{w}}=
    \underset{\boldsymbol{w}}{\operatorname{argmin}}\left|\vec{y}-\vec{M} \vec{w}\right|^{2}
    \label{eq:ols_objective}
\end{equation}
There is a closed form solution for Equation~\ref{eq:ols_objective} as long as the number
of parameters and features $p$ is less than the number of data points $n$ and the solution
is \citep{arXiv:2101.07256}
\begin{equation}
    \hat{\vec{w}}=(\vec{M}^{\intercal} \vec{M})^{-1} \vec{M}^{\intercal} \vec{y}
    \label{eq:ols_solution}
\end{equation}
under the assumption that $\vec{M}\vec{M}^\intercal$ is invertible which is usually the case.
Equation~\ref{eq:ols_solution} is called the \emph{ordinary least squares} (OLS) solution.
For a nontrivial data covariance matrix $\vec{C}$ Equation~\ref{eq:ols_solution} generalizes
to
\begin{equation}
    \hat{\vec{w}}=(\vec{M}^{\intercal}\vec{C}^{-1}\vec{M})^{-1} \vec{M}^{\intercal}\vec{C}^{-1} \vec{y}
    \label{eq:wls_solution}
\end{equation}
which is the solution to the \emph{weighted least squares} (WLS) problem.
It is common to modify the least squares objective functions with the addition of a
\emph{regularization} term to prevent overfitting. For instance, we can modify the OLS
objective (Equation~\ref{eq:ols_objective}) by adding an \emph{L2 regularization} term:
\begin{equation}
    \hat{\vec{w}}=
    \underset{\boldsymbol{w}}{\operatorname{argmin}}\left|\vec{y}-\vec{M} \vec{w}\right|^{2} + \lambda \left|\vec{w}\right|^2
    \label{eq:ols_objective_l2}
\end{equation}
where $\lambda >0$ is a regularization parameter
which penalizes large magnitudes of the coefficients $\vec{w}$. This is also a convex
optimization problem and the solution is
\begin{equation}
    \hat{\vec{w}}=(\vec{M}^{\intercal} \vec{M} + \lambda\vec{I})^{-1} \vec{M}^{\intercal} \vec{y}
    \label{eq:ols_solution_l2}
\end{equation}
where $\vec{I}$ is the identity matrix. Regularized OLS regression with this particular
regularization term is also known as \emph{ridge regression} in the statistics
literature\footnote{One of the many things that make statistics confusing is that very
    similar methods often have special names wich obscures the big picture. The vast majority
    of statistical methods can be seen as a linear model with the data represented in a particular
    basis, and with a particular choice of the likelihood (noise model), priors (regularization)
    and  a method for obtaining the solution (usually a convex optimization problem).}
L2 regularization (as well as most other forms of regularization terms) can be seen
as a particular choice of priors for the regression coefficients in the Bayesian paradigm.
The above also naturally generalizes to the case of weighted least squares.

\subsubsection{Overdetermined case ($p > n$)}
Contrary to popular belief within the astronomy community it is possible and
often desirable to have a model with more parameters than data points (see
Chapter~\ref{ch:mapping_io} for a real world example). In the overparametrized
case of linear regression $(p > n)$ there are multiple choices for $\vec{w}$
which perfectly fit the data. The solution in that case is defined to be a
minimum-norm parameter vector $\vec{w}$ that interpolates the data
($\vec{y}=\vec{M} \vec{w}$), out of possibly many degenerate solutions. The OLS
objective in Equation~\ref{eq:ols_objective} can be modified such that it is
valid for both the $p<n$ and the $p>n$ case by making use of a limit in which
an L2 regularization term tends to zero \citep{arXiv:2101.07256}:
\begin{equation}
    \hat{\vec{w}}=\lim _{\lambda \rightarrow 0^{+}}\left[
    \underset{\boldsymbol{w}}{\operatorname{argmin}}\left|\vec{y}-\vec{M} \vec{w}\right|^{2} + \lambda \left|\vec{w}\right|^2\right]\quad .
\end{equation}
In this case the OLS solution becomes
\begin{equation}
    \hat{\vec{w}}=\vec{M}^{\intercal} (\vec{M} \vec{M}^{\intercal})^{-1} \vec{y}
    \label{eq:ols_solution_overdet}
\end{equation}
Equations~\ref{eq:ols_solution} and \ref{eq:ols_solution_overdet} can be unified into a
single equation by making use of the \emph{Moore-Penrose pseudoinverse} $\vec{M}^\dagger$.
The pseudoinverse is defined by taking the singular-value decomposition (SVD)
of $\vec{M}$:
\begin{equation}
    \vec{M}\equiv\vec{U} \vec{S} \vec{V}
\end{equation}
Then $\vec{M}^\dagger =  \vec{V}^{\intercal} \vec{S}^{\dagger} \vec{U}^{\intercal}$.

\subsection{Bayesian linear regression}
In the Bayesian paradigm we can write down the posterior over the regression
weights $\vec{w}$ assuming a Gaussian likelihood and a Gaussian prior over the
weights. The posterior is then
\begin{align}
    p(\vec{w}|\vec{y}) & = \frac{p(\vec{y}\lvert \vec{w})\,p(\vec{w})}{\int p(\vec{y}\lvert \vec{w})p(\vec{w}) \ud\vec{w}}                                                                                                                                                 \\
                       & = \frac{    \mathcal{N}(\vec{y} \lvert\vec{M} \vec{w}, \vec{C}) \,\mathcal{N}(\vec{w} \lvert\vec{\mu}, \vec{\Lambda})}{\int \mathcal{N}(\vec{y} \lvert\vec{M} \vec{w}, \vec{C}) \,\mathcal{N}(\vec{w} \lvert\vec{\mu}, \vec{\Lambda}) \ud\vec{w}}
    \label{eq:bayes_lin_regression}
\end{align}
where $\vec{C}$ is the data covariance matrix as before, $\vec{\mu}$ is the mean
of the Gaussian prior on coefficients and $\vec{\Lambda}$ is the the prior covariance matrix.
Using the fact that the product of two multivariate Gaussian distributions is also a
multivariate Gaussian distribution, one can show that
the solution to Equation~\ref{eq:bayes_lin_regression} is
\citep[see for example][]{arXiv:2005.14199}
\begin{equation}
    p(\vec{w}|\vec{y}) =\mathcal{N}(\vec{w} \lvert\vec{a}, \vec{A})
    \label{eq:linear_regression_posterior}
\end{equation}
where
\begin{align}
     & \vec{A}^{-1}   =\vec{\Lambda}^{-1}+\vec{M}^{\intercal} \vec{C}^{-1} \vec{M}                                                     \\
     & \vec{a}     =\vec{A} \left(\boldsymbol{\Lambda}^{-1} \boldsymbol{\mu}+\vec{M}^{\intercal} \mathbf{C}^{-1} \boldsymbol{y}\right) \\
\end{align}
Since all \textsf{starry} models for occultation light curves are linear
(Equation~\ref{eq:starry_linear_model}) we can use Equation~\ref{eq:linear_regression_posterior}
to directly obtain the posterior over the map coefficients $\mathbf{y}$ but the cost
of doing so is imposing a Gaussian prior on the map coefficients.

\subsection{Marginalizing a likelihood over linear parameters}
A model structure which is quite common in astronomy and physics is a model
which depends on some linear, and some nonlinear parameters. This is the
structure of the equation for the observed flux in microlensing
(Equations~\ref{eq:flux_observed_micro} or \ref{eq:flux_dominik}) where the
flux parameters are linear and the magnification parameters are nonlinear. The
likelihoods for such models can be written as
\begin{equation}
    p(\vec{y}\lvert \vec{w}, \vec{\theta})
\end{equation}
where $\vec{w}$ are the linear parameters and $\vec{\theta}$ are the nonlinear parameters.
In some circumstances, it is possible to analytically marginalize over the linear
parameters $\vec{w}$. Doing so is often advantageous because it reduces the size of the
parameter space  and the computational cost of doing inference.
The marginalized likelihood is given by (Equation~\ref{eq:marginalization})
\begin{equation}
    p(\vec{y}\lvert \vec{\theta})=\int p(\vec{y}\lvert \vec{w}, \vec{\theta})p(\vec{w}\lvert \vec{\theta}) \ud\vec{w}
\end{equation}
Marginalization requires the introduction of a prior  $p(\vec{w}\lvert \vec{\theta})$
which can potentially also depend on the nonlinear parameters $\vec{\theta}$.
The alternative to doing this integral is re-factorizing the expression in the integrand
using Bayes' theorem:
\begin{equation}
    p(\vec{y}\lvert \vec{w}, \vec{\theta})p(\vec{w}\lvert \vec{\theta})  = p(\vec{w}\lvert \vec{y},\vec{\theta})\,p(\vec{y}\lvert \vec{\theta})
\end{equation}
which yields the marginalized likelihood without us having to do the integral:
\begin{equation}
    p(\vec{y}\lvert \vec{\theta}) = \frac{    p(\vec{y}\lvert \vec{w}, \vec{\theta})p(\vec{w}\lvert \vec{\theta})}{p(\vec{w}\lvert \vec{y},\vec{\theta})}
    \label{eq:likelihood_factorization}
\end{equation}
If both the prior and the likelihood are multivariate Gaussians with the following form:
\begin{align}
    p(\vec{y}\lvert \vec{w}, \vec{\theta}) & = \mathcal{N}(\vec{y}\lvert \vec{M}\vec{w},\vec{C})  \\
    p(\vec{w}\lvert \vec{\theta})          & = \mathcal{N}(\vec{w}\lvert \vec{\mu},\vec{\Lambda})
\end{align}
then Equation~\ref{eq:likelihood_factorization} becomes
\citep[for the proof see][]{arXiv:2005.14199}
\begin{equation}
    \mathcal{N}(\vec{y} \lvert \vec{M} \vec{w}, \vec{C})\, \mathcal{N}(\vec{w} \lvert\vec{\mu}, \vec{\Lambda})=\mathcal{N}(\vec{w} \lvert\vec{a}, \vec{A})\, \mathcal{N}(\vec{y} \lvert\vec{b}, \vec{B})
\end{equation}
where
\begin{align}
     & \vec{A}^{-1}   =\vec{\Lambda}^{-1}+\vec{M}^{\intercal} \vec{C}^{-1} \vec{M}                                                     \\
     & \vec{a}     =\vec{A} \left(\boldsymbol{\Lambda}^{-1} \boldsymbol{\mu}+\vec{M}^{\intercal} \mathbf{C}^{-1} \boldsymbol{y}\right) \\
     & \mathbf{B}=\mathbf{C}+\vec{M} \vec{\Lambda} \vec{M}^{\intercal}                                                                 \\
     & \vec{b}=\vec{M} \vec{\mu}
\end{align}
Thus the marginalized likelihood is
\begin{equation}
    p(\vec{y}\lvert \vec{\theta}) = \mathcal{N}(\vec{y} \lvert \vec{b}, \vec{B})
    \label{eq:likelihood_lin_params_marginalized}
\end{equation}

\subsubsection{Application to microlensing}
Let's apply this trick to the microlensing models
(Equation~\ref{eq:flux_dominik}). In the general case, the dataset consists of
$J$ flux vectors $\vec{f}_j$, each of lengths $N_j$ where $j$ indexes different
spectral bands, often from different observatories as well.
Equation~\ref{eq:flux_dominik} can then be written in vector form as
\begin{equation}
    \vec{f}_j=\vec{M}_j\vec{w}_j
    \label{eq:microlensing_flux_general}
\end{equation}
where the matrix $\vec{M}$ is defined as
\begin{equation}
    \vec{M}_j\equiv \begin{pmatrix}
        \tilde{A}(t_1;\vec{\theta})     & 1      \\
        \tilde{A}(t_2;\vec{\theta})     & 1      \\
        \vdots                          & \vdots \\
        \tilde{A}(t_{N_j};\vec{\theta}) & 1
    \end{pmatrix}
\end{equation}
and $t=t_1,\ldots, t_{N_j}$ are the times of observations in band $j$,
$\tilde{A}(t_i) \equiv (A(t_i)-1) /(A\left(t_{0}\right)-1)$, $\vec{\theta}$ are the
nonlinear parameters, and the weights vector $\vec{w}_j$ is defined as
\begin{equation}
    \vec{w}_j\equiv    \left( \Delta F_j\; F_{\mathrm{base},j}\right)^\intercal
\end{equation}
Equation~\ref{eq:microlensing_flux_general} is general enough to encompass
either single or multiple lens microlensing models. Assuming independence of
noise properties between observations in different spectral bands\footnote{This
    is certainly true if we have observations from different telescopes. It is not
    strictly true if the observations are from the same telescope in different
    spectral bands but it is often a good enough assumption.} the likelihood for
the complete dataset
$\mathcal{D}=\left\{f_{j}\right\}_{j=1}^{J}$ factorizes as
\begin{equation}
    p(\mathcal{D}\lvert \vec{w}, \vec{\theta})=\prod_{j=1}^J p(\vec{f}_j\lvert \vec{w}_j, \vec{\theta})
\end{equation}
Using Equation~\ref{eq:likelihood_lin_params_marginalized} we can then write down the
negative log-likelihood (ignoring constants) for the $j$-th band as
\begin{align}
    \operatorname{NLL}(\vec{\theta}) & =\frac{1}{2}\left( \vec{f}_j-\vec{M}_j\vec{\mu}\right)^\intercal\left(\vec{C}_j + \vec{M}_j\vec{\Lambda}\vec{M}_j^\intercal\right)^{-1}
    \left( \vec{f}_j-\vec{M}_j\vec{\mu}\right)                                                                                                                                 \\
                                     & + \frac{1}{2}\ln\left| \vec{C}_j + \vec{M}_j\vec{\Lambda}\vec{M}_j^\intercal\right|
    \label{eq:ll_marginalized_microlensing}
\end{align}
where we have assumed a common mean $\vec{\mu}$ and covariance $\vec{\Lambda}$ for the
prior for the linear flux parameters.
To avoid computing an inverse and a determinant of $J$ matrices with shape $N_j\times N_j$,
we can simplify the terms in Equation~\ref{eq:ll_marginalized_microlensing} using the
matrix inversion lemma \citep[see for example Appendix A3 in][]{rasmussen2006}
\begin{align}
     & \left(\mathbf{C}_j+\vec{M}_j \boldsymbol{\Lambda} \vec{M}_j^{\intercal}\right)^{-1}=\mathbf{C}_j^{-1}-\mathbf{C}_j^{-1} \vec{M}_j\left(\boldsymbol{\Lambda}^{-1}+\vec{M}_j^{\intercal} \mathbf{C}^{-1} \vec{M_j}\right)^{-1} \vec{M}_j^{\intercal} \mathbf{C}_j^{-1} \\
     & \ln\left|\mathbf{C}_j+\vec{M}_j \vec{\Lambda} \vec{M}_j^{\intercal}\right|=\ln|\mathbf{C}_j| +\ln|\vec{\Lambda}| + \ln\left|\boldsymbol{\Lambda}^{-1}+\vec{M}_j^{\intercal} \mathbf{C}_j^{-1} \vec{M}_j\right|
\end{align}
In most cases we can assume that $\vec{\mu}=\vec{0}$, $\vec{\Lambda}$ is diagonal, and
in absence of correlated noise all matrices $\vec{C}_j$ are also diagonal so the expression
in Equation~\ref{eq:ll_marginalized_microlensing} isn't usually expensive to compute.

Although Equation~\ref{eq:ll_marginalized_microlensing} is the formally correct
way to marginalize the linear parameters of a general microlensing model, it is
not used in the microlensing literature. What most people do instead is using
MLE to maximize the joint likelihood $p(\vec{f}\lvert\vec{w},\vec{\theta})$
with respect to the linear parameters by solving a least squares problem at
every MCMC step in the nonlinear parameters (we'll cover MCMC in the next
section). This procedure is approximately equivalent to using the marginalized
likelihood from Equation~\ref{eq:ll_marginalized_microlensing} with two
important caveats. The first it that one shouldn't optimize for the maximum
likelihood flux parameters conditional on nonlinear parameters, but rather the
MAP values of those parameters under Gaussian priors (although, the difference
between the MLE and MAP estimates is negligible under broad priors). The second
caveat is that the optimization can be cast as a linear least squares problem
under the assumption that the covariance matrix $\vec{C}$ for the data is
diagonal. This is a fairly restrictive assumptions because it implies that
there is no correlated noise in the light curve which isn't true for most real
world microlensing datasets.

Although we have marginalized the likelihood over the linear parameters which
substantially reduces the complexity of the inference problem we can still
compute the posterior distribution for those parameters \emph{conditional on
    the value of the nonlinear parameters}:
\begin{equation}
    p(\vec{w}\lvert \vec{y}, \vec{\theta})=\mathcal{N}(\vec{w}\lvert \vec{y}, \vec{\theta})
\end{equation}
which we can use to obtain the full posterior over the linear parameters if we happen
to have samples from the posterior over the nonlinear parameters $p(\vec{\theta}|\vec{y})$
(obtained using MCMC for example).

\subsection{Gaussian processes}
% https://peterroelants.github.io/posts/gaussian-process-tutorial/
\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/gp_samples.pdf}
        \caption{A few samples from a Gaussian process prior with a squared exponential
            kernel function for two different values of the length scale parameter $l$.}
        \label{fig:gp_samples}
    \end{centering}
\end{figure}
A final topic that we need to cover in this section is that of \textsl{Gaussian
    Processes}. In Section~\ref{ssec:least_squares} we have discussed linear
regression with $p$ components (basis functions) and we have mentioned that one
fit many more components than there are data points and that these models can
be quite useful. A Gaussian process (GP) is what happens when let the number of
components $p$ go to infinity\footnote{See \citet{arXiv:2101.07256} for a
    demonstration of this point.} which is why they belong to a class of methods
called \textsl{nonparametric regression} methods, although a better name would
perhaps be infinite-parameter regression methods. This is not the only
formulation of GPs. GPs can also be seen as \emph{prior distributions over the
    space of functions} such that each sample from that GP prior is a function.
More formally, \emph{a Gaussian process is a collection of random variables,
    any finite number of which have a joint multivariate Gaussian
    distribution} \citep{rasmussen2006}.

GPs are completely specified by a \textsl{mean function} $\mu (x)$ and a
positive-definite \textsl{covariance function} $k(x,x^\prime)$ which are
functions of the input locations $x\in \mathbb{R}^D$ (for example, a time
coordinate):
\begin{align}
    \mu (x)        & =\mathbb{E}[f(x)]                                                    \\
    k(x, x^\prime) & =\mathbb{E}\left[(f(x) -\mu (x))(f(x^\prime) -\mu (x^\prime))\right]
\end{align}
where $x$ and $x^\prime$ are any two different points in the input space.
We write
\begin{equation}
    f(x) \sim \mathcal{G} \mathcal{P}\left(\mu(x), k\left(x, x^{\prime}\right)\right)
\end{equation}
Usually the mean function $\mu(x)$  is taken to be zero for simplicity and all the
magic happens in the covariance function $k(x,x^\prime)$ which describes
\emph{how  any two points in the input space related to each other}.

One very popular choice for the kernel function is the \textsl{squared
    exponential} kernel:
\begin{equation}
    k(x, x^\prime) = \exp\left(-\frac{1}{2}\frac{\left|x-x^\prime\right|^2}{l^2}\right)\hquad .
    \label{eq:sqexp}
\end{equation}
The above choice of a covariance function implies that the covariance of the outputs
$f(x)$ evaluated at any two points  will decay exponentially with the square of
the distance between the two points.
In other words, function values evaluated at points close to each tend to go hand in hand
while function values evaluated at points far away from each other are practically
independent.
The parameters $l$ sets the length scale which determines how quickly the covariance decays
to zero with increasing distance between the two points.
Figure~\ref{fig:gp_samples} shows samples from a GP prior with the squared exponential
kernel function in the one-dimensional case for two choices of the length sale parameter
$l$. One special property of this kernel is that it is \textsl{stationary}, meaning
that the mean and the variance of the samples from the GP prior don't depend on the
absolute value of the input coordinate $x$, they are the same everywhere because
the covariance only depends on the difference between any two points $x$ and
$x^\prime$\footnote{Stationarity is also a general property of time series.
    A stationary  time series is one whose properties (mean, variance) do not depend
    on the time at which the series is observed. A GP with a stationary kernel is a very flexible
    model for stationary time series.}. Not all kernels are stationary. An example of
a non-stationary kernel would be a kernel similar to the squared exponential but with
a variance growing linearly with time.
Kernels can be combined in any number of ways through for example addition and
multiplication and the result is also a valid kernel.
\href{https://www.cs.toronto.edu/~duvenaud/cookbook/}{This} pages provides a
nice visualization of different kernels and their combinations.

The key property of a GP is that for any finite subset
$\vec{X}=\left\{\mathbf{x}_{1} \ldots \mathbf{x}_{n}\right\}$ of the domain of
$x$ (that is, any marginal distribution of $x$) is a multivariate Gaussian:
\begin{equation}
    f(\vec{x})\sim \mathcal{N}\,(\vec{\mu}(\vec{X}), \vec K(\vec{X}, \vec{X}))
\end{equation}
where $\vec K$ is a covariance matrix of shape $n\times n$ which is constructed by
evaluating  a kernel such as the one defined in Equation~\ref{eq:sqexp} elementwise.
This is precisely how we plotted
the functions in Figure~\ref{fig:gp_samples}, we simply chose a dense number of linearly
spaced grid points, computed the covariance matrix $K$ and then sampled the multivariate
Gaussian distribution.
\subsubsection{Predictions from posterior distribution}
\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/gp_samples_conditioned.pdf}
        \caption{Samples from a Gaussian process with squared exponential kernel $l=1$
            conditioned on observations simulated from a simple sinusoid.
            The left panel shows the case when we assume that
            the observations are noiseless in which case the GP perfectly interpolates the data.
            The right panel shows the case when we assume that the observations are generated
            from the GP with the addition of some white Gaussian noise with variance $0.1^2$.}
        \label{fig:gp_samples_conditioned}
    \end{centering}
\end{figure}
We can go a step beyond drawing samples from the GP prior and condition the GP
on a set of observations in order fit the GP to data. Let $\vec{f}(\vec{X})$ be
the vector of observed data points evaluated at $\vec{X}$ with length $n_1$. If
we are interested in predicting the function values $\vec{f}^*(\vec{X}^*)$ at a
new vector of data points $\vec{X}^*$ with length $n_2$ (so we could, for
example, make a plot), then we need to compute the posterior distribution
$p(\vec{f}^*\lvert \vec{f}, \vec{X},\vec{X}^*)$. In order to compute the
conditional distribution, we start with the join distribution over $\vec{f}$
and $\vec{f}^*$. Since $\vec{f}$ and $\vec{f}^*$ both come from the same
multivariate Gaussian distribution, the joint distribution $p(\vec{f},
    \vec{f}^*)$ is also a multivariate Gaussian:
\begin{equation}
    \begin{pmatrix}
        \vec{f} \\
        \vec{f}^*
    \end{pmatrix} \sim \mathcal{N}\left[\begin{pmatrix}
            \vec{\mu} \\
            \vec{\mu}^*
        \end{pmatrix},\begin{pmatrix}
            \vec K(\vec{X},\vec{X})    & \vec K(\vec{X},\vec{X}^*)   \\
            \vec K(\vec{X}^*, \vec{X}) & \vec K(\vec{X}^*,\vec{X}^*)
        \end{pmatrix}\right]
\end{equation}
From this joint distribution, we can obtain the conditional  distribution
\citep[see appendix A2 in ][]{rasmussen2006} which is also a multivariate Gaussian\footnote{
    This is one of many special properties of Gaussians.
}:
\begin{align}
    p(\mathbf{f}^{*} \lvert \vec X^{*}, \vec X, \mathbf{f} )\sim \mathcal{N}( & \vec K\left(\vec X^*, \vec X\right) \vec K(\vec X, \vec X)^{-1} \mathbf{f}                                                          \\
                                                                              & \vec K\left(\vec X^*, \vec X^*\right)  -\vec K\left(\vec X^*, \vec X \right) \vec K(\vec X, \vec X)^{-1} \vec K (\vec X, \vec X^*))
    \label{eq:gp_posterior}
\end{align}

Samples from this posterior GP distribution given by
Equation~\ref{eq:gp_posterior} with a squared exponential kernel conditioned on
a set of simulated observations $\vec{f}$ generated from a simple sinusoid are
shown in the left panel of Figure~\ref{fig:gp_samples_conditioned}. Each draw
from the distribution perfectly interpolates all the data points. The variance
is naturally larger where the data is sparse and the variance of the
predictions grows rapidly in the extrapolative regime outside of the range of
the data. The squared exponential kernel is not a great choice here because the
covariance rapidly decays to zero with the distance from the first and last
data point so the predictions will look flat futher away from the range of the
data. A much better choice would be a kernel with a periodic covariance
structure which would not have this problem.

The predictions shown in the left panel assume that the observations are
noisless which is why each sample perfectly interpolates the data. It is more
common to assume that there is an additional white Gaussian noise component in
the observed data. In that case we add a diagonal matrix (because white noise
is independent Gaussian noise) to the covariance matrix
$\vec{K}(\vec{X},\vec{X})$ and we have
\begin{equation}
    \vec{K}(\vec{X},\vec{X}) + \sigma^2\vec{I}
\end{equation}
where $\vec{I}$ is the identity matrix. Predictions from a GP with this additional
noise are shown in the right panel of Figure~\ref{fig:gp_samples_conditioned}.

\subsubsection{Fitting for the parameters of a GP kernel}
We shown how to sample a GP prior and how to compute predictions from a GP
conditioned on a set of observations but we haven't discssed the parameters
which define the kernel function, such as the length scale $l$ in the squared
exponential kernel. These parameters are often called the
\textsl{hyperparameters} of the kernel. Given a set of observations $\vec{f}$
evaluated at $\vec{X}$ with length $n$, the likelihood of a GP model for the
kernel hyperparameters $\vec{\theta}$ is
\begin{equation}
    p(\vec{f}\lvert \vec{\theta})\sim\,
    \mathcal{N}(\vec{f}\lvert \vec{\mu}(\vec{\theta}),
    \vec{K}(\vec X, \vec X; \vec\theta))
    \label{eq:gp_marginal_likelihood}
\end{equation}
where $\vec{\theta}$ is a set of parameters which define the kernel function. We
can treat these kernel parameters as we would any other parameters, for example we
can optimize the likelihood in Equation~\ref{eq:gp_marginal_likelihood} and find the
MLE parameters $\hat{\vec{\theta}}_\mathrm{MLE}$. The likelihood is not linear in these
hyperaparamters and the the optimization problem is generally not convex.

The major challenge with GPs and the reason they still aren't as popular as for
instance linear regression is that sampling the GP prior or computing the
marginal likelihood (Equation~\ref{gp:marginal_likelihood}) requires an
inversion of a dense $n\times n$ covariance matrix, an operation which scales
as $\mathcal{O}(n^3)$. This operation becomes prohibitively expensive for
datasets with a few thousand data points. Thankfully, there has been a lot of
work on faster algorithms for inverting GP covariance matrices. Most of these
algorithms rely on the assumption of some special structure for the covariance
matrix which can be exploited to speed up the linear algebra operations. The
most notable example is the \textsf{celerite} algorithm
\citep{2017AJ....154..220F} which enables the use of 1D GPs with linear
$\mathcal{O}(n)$ scaling as long as we restrict ourselved to kernels which are
sums of complex exponential functions. These kernels are sufficiently flexible
for modeling physical time-series and the sums of complex exponentials also
have a physical interpretation as mixtures of stochastically drive, dampled
simple harmonic oscillators. Thanks to \textsf{celerite} it is possible to fit
GPs to astronomical light curves with thousands of datapoints.

\subsubsection{Use of GPs in astronomy}
The use of Gaussian process models within astronomy has skyrocketed in recent
years. GPs within astronomy are by far most commonly used to model correlated
noise in astronomical light curves. Practically, this is a small step from just
assuming independent Gaussian noise. All we have to do is to replace a
likelihood with a diagonal covariance matrix whose entries are given by the
variance estimates for individual data points (``errorbars'') with a dense
covariance matrix whose generated using some kernel function (while still
including the independent noise term). The mean of the multivariate Gaussian
likelihood is usually an astrophysical model such as a transit, a microlensing
flux prediction or a secondary eclipse prediction. In some cases, the mean is
set to zero and the hyperparameters of the kernel have physical meaning. For
instance, GPs have been used to model stellar rotation periods using a
quasi-periodic kernel specified with multiple parameters which describe the
stochastic variability of the star \citep{2018MNRAS.474.2094A}.
\citet{2021AJ....162..124L} went a step beyond, starting with properties of
distributions of stellar spots expressed in the \textsf{starry} formalism they
derived a closed-form expression for a GP which describes a light curve of a
rotating, evolving stellar surface conditioned on a distribution of stellar
spot sizes, contrasts and latitudes.

The advantage of GP models relative to alternatives such as linear regression
with a fixed number of basis functions is that they are far more flexible and
allow us to make less rigid assumptions about the phenomenon of interest. This
section was just a brief summary of their key properties and there is much more
to GPs that we haven't discussed (for example, fitting GPs to multi-dimensional
data). In Chapter~\ref{ch:mapping_exoplanets} we will show two very different
use cases of GPs in the context of occultation mapping.

\section{Statistical inference - computation}
\label{sec:inference_practice}
In the previous section I briefly summarized some key aspects of probability theory
and Bayesian statistics, I discussed maximum likelihood estimation and
linear models and GPs for which inference can be done analytically.
In most most cases, models of real world interest are not so well behaved and maximum
likelihood estimation or obtaining samples from the posterior
distribution becomes very challenging because of complex likelihood and posterior
landscapes. In those cases computation needs goes hand-in-hand with model building and it
is very important to understand how the strucuture of the model and the data maps to the
structure of the likelihood.
I start by introducing some very fundamental
concepts such as the \emph{curse of dimensionality} and the \emph{no free lunch theorems}.
Next, I discuss various computational inference methods such
as optimization methods, Markov chain Monte Carlo and Variational Inference.
Finally, I discuss model comparison through Bayes factors and cross validation.

\subsection{The curse of dimensionality and the no-free-lunch theorem}
The \emph{curse of dimensionality} refers to a phenomenon where the
\emph{volume} of \emph{possible} explanations for the data grows exponentially
with increasing dimension of the problem. Consider the following
example\footnote{ Taken from the excellent paper by \citet{arXiv:2104.00008}.
}. There are images consisting of $n$ pixels where each of the pixels is either
a black (0) or white (1). The number of possible images is $2^n$ which grows
exponentially. If each image out of the $2^n$ possible images is described by
one of two labels $A$, or $B$ then the total number of possible ways of
labeling the set of images is $2^{2^n}$ which is a stupidly large number. If
$n=9$, the number of ways to classify the set of all 9-bit black and white
images is greater than the number of atoms in the observable universe. If the
labels don't correlate with the properties of the images then one strategy for
solving this problem would be the lookup table approach -- memorize each image
with its label. The famous \emph{no-free-lunch theorem} (NFL) proved by the
mathematician David Wolpert \citep{wolpert1996} demonstrates that it is
impossible to construct an algorithm which does better than the lookup table
approach.

Of course the assumption that the labels aren't correlated to features in the
images is silly for any meaningful problem. For instance, all images of cats
have certain things in common as do images of galaxies. The no-free-lunch
theorem doesn't say that it is impossible to construct an algorithm to classify
images of cats, it only says that there is no best algorithm \emph{if we
    average over all possible inputs to the problem}. To construct an algorithm
which works in practice we need to impose structure on the problem, what
machine learning researchers call \emph{inductive bias}. A set of assumptions
which work in one domain may not work well in another domain and no single
model will work well in all domains.

\subsection{Sampling vs. optimization}
%When fitting a model to a scientific dataset we are usually interested in
%finding some ``best'' estimate of the parameters of that model\footnote{This is
%    opposed to machine learning where in the vast majority of cases the goal is
%    \emph{prediction} and the parameters of the model (for instance, weights of a
%    neural network) are not of interest.}. In science, simply having an estimate of
%the parameters is not sufficient, we also require estimates of the
%\emph{uncertainty} associated with those values. In the Bayesian formalism this 
%means estimating the posterior distribution $p(\vec \theta \lvert \mathcal{D})$.

Broadly speaking, in statistical inference we can differentiate between two
classes of methods for fitting models to data. Optimization methods cast the
problem of estimating the likelihood or the posterior as an (generally
non-convex) optimization problem. The goal in optimization problems is to find
a single point which minimizes or maximizes some scalar function called a
\textsl{cost function} or an \textsl{objective function}. The space of possible
cost functions is infinite but in probabilitic inference these will be (usually
unnormalized) probability distributions such as likelihood and posteriors. An
example would be MLE and MAP estimation, the Laplace approximation and
variational inference.

Sampling methods are a fundamentally different class of algorithms whose
purpose is accurately computing multidimensional integrals such as with the
goal of numerically computing \emph{integrals} such as expectation values over
the distribution $p(\vec{\theta})$:
\begin{equation}
    \mu\equiv \mathbb{E}_{p(\vec \theta)}[g(\vec \theta)]=\int g(\vec \theta) p(\vec \theta) \ud \vec \theta\hquad ,
    \label{eq:general_expectation}
\end{equation}
where $g(\vec \theta)$ is some arbitrary function such as the mean.
These integrals are intractable when the dimension of $\vec{\theta}$ is greater than
a few because  the volume of the parameter space grows exponentially and finding
the the regions in parameter space where the integral is nonzero becomes ever harder.
We can get around this problem by generating samples from the probability distribution
$\vec{\theta}^{(s)}\sim p(\vec{\theta})$ and computing the integral using
\textsl{Monte Carlo Integration} in regions where there is non-negligible probability:
\begin{equation}
    \hat{\mu}_\mathrm{MC}=\mathbb{E}_{p(\vec \theta)}[g(\vec \theta)] \approx \frac{1}{S} \sum_{i=1}^{S} g\left(\vec \theta^{(s)}\right)\hquad .
    \label{eq:monte_carlo_estimator}
\end{equation}
where $\hat{\mu}_\mathrm{MC}$ is the simple Monte Carlo estimator of the expectation
$\mu$.
It can be shown that if the expectation $\mu$ exists, the estimator
$\hat{\mu}_\mathrm{MC}$ is consistent (it asymptotically converges toward $\mu$ with
increasing $S$) and unbiased (the mean of the sampling distribution of the estimator
is equal to $\mu$). If the expectation of $g^2$ is also finite the central limit
theorem holds and the error of the simple  Monte Carlo estimator decreases as
$\sqrt{S}$. This error is independent of the dimension!
The major challenge is how to generate those samples from $p(\vec{\theta})$. This
is usually accomplished
sampling numbers from probability distributions that are easy to sample from
(most commonly multivariate normal distributions) and then reweighting those
samples at each iteration of the algorithm so that they match the target
distribution. Examples of sampling methods include Markov chain Monte Carlo
(MCMC), importance sampling and rejection sampling.

%The difference between optimization and sampling algorithms can often be fuzzy
%because sampling problems can sometimes be cast as optimization problems. In
%both cases the NFL theorem holds, meaning that no sampling or optimization
%method works best across all problems. The fundamental problem in both cases as
%that we can only evaluate the target density some finite number of times and
%the challenge is to find high probability regions in a multi-dimensional
%parameter space. Even in moderately high-dimensional spaces (on the order of 10
%dimensions) this is an extremely challening problem which is why methods which
%work well in low dimensions often fail spectacularly in high dimensions. Both
%classes of methods often use information about the gradient of target
%distribution to aid the exploration of the parameter space. In fact, optimizing
%or sampling high dimensional distributions is often impossible without access
%to (exacy) gradients of the target density.
%
\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/typical_sets.pdf}
        \caption{Histograms showing the distance from the MAP point for a set of 
        samples from $D$-dimensional multivariate normal distribution with 
        zero mean and unit variance. With increasing number of dimensions the 
        probability mass concentrates in a thin shell centered at the MAP point 
        called the typical set. For $D=50$ the posterior samples are coming from 
        a region more than 5 sigma away from the MAP point.}     
               \label{fig:typical_sets}
    \end{centering}
\end{figure}
Contrary to what we might intuitively think, in all but the lowest dimensional models
samples from the posterior distribution are not located anywhere 
near a point estimate such as the MAP point in the parameter space, despite the fact 
that this is where the posterior density is highest.
This happens because expectation values  depend not only on the density of the pdf 
$p(\vec\theta)$ but also on the volume term 
$\ud \vec\theta$ and the value of the integral is non-negligible only when both are 
jointly maximized. In high dimensions the volume grows exponentially and most 
of the volume ends up being concentrated far away from the mode so that 
the product of the density and the  volume, the \textsl{posterior mass}, ends up 
being concentrated in a thin shell around the mode called the \textsl{typical set} 
\citep[for a good overview of this issue see][]{arXiv:1701.02434}.
This point is illustrated in Figure~\ref{fig:typical_sets} using samples from 
a $D$-dimensional multivariate normal distribution with zero mean and unit 
variance. 
The reason this happens is that  although the most likely points are located 
in the neighbourhood of the mode, 
there are many more points further away in the typical set and those are the 
ones that contribute to the integral involved in the calculation of expectation 
values.
For a toy example shown in Figure~\ref{fig:typical_sets} the fact this does not 
matter much because the posterior has spherical symmetry but  for non Gaussian
posteriors we can get very different results  when computing the mean (via 
sampling) and the MAP point (via optimization).


\subsection{Optimization methods}
In this section we provide a brief overview of two most relevant methods for
estimating the posterior distribution $p(\vec{\theta}\lvert \mathcal{D})$ using
optimization algorithms.

\subsubsection{The Laplace approximation}
One of the simplest way of approximating a posterior distribution is the
\textsl{Laplace approximation}. We start with the general posterior density of
the form
\begin{equation}
    p(\vec{\theta}\lvert\mathcal{D})=\frac{1}{\mathcal{Z}}e^{-\mathcal{E}(\vec{\theta})}
\end{equation}
where $\mathcal{E}(\vec{\theta}) \equiv \ln\,p(\vec{\theta}, \mathcal{D})$
is called an \textsl{energy function} and $\mathcal{Z}$ is the fully marginalized
likelihood. Following \citet{murphy_book_2022}, we Taylor expand $\mathcal{E}(\vec{\theta})$ around the mode
$\hat{\vec{\theta}}$ as follows
\begin{equation}
    \mathcal{E}(\vec{\theta}) =\ln\,p(\vec{\theta}, \mathcal{D})\approx \mathcal{E}(\hat{\vec{\theta}})+(\vec{\theta}-\hat{\vec{\theta}})^{\intercal} \vec{g}+\frac{1}{2}(\vec{\theta}-\hat{\vec{\theta}})^{\intercal} \mathbf{H}(\vec{\theta}-\hat{\vec{\theta}})
\end{equation}
where $\vec{g}$ is the gradient at the mode (the Jacobian matrix), and $\vec{H}$
is the \textsl{Hessian} evaluated at the mode. The gradient term by definition
vanishes at the mode $\hat{\vec{\theta}}$ so we are left with
\begin{equation}
    \hat{p}(\vec{\theta}, \mathcal{D})=e^{-\mathcal{E}(\hat{\vec{\theta}})} \exp \left[-\frac{1}{2}(\boldsymbol{\theta}-\hat{\vec{\theta}})^{\intercal} \mathbf{H}(\vec{\theta}-\hat{\vec{\theta}})\right]
\end{equation}
from which it follows that
\begin{equation}
    \hat{p}(\vec{\theta} \lvert\mathcal{D})=\frac{1}{\mathcal{Z}}\, \hat{p}(\vec{\theta}, \mathcal{D})=\mathcal{N}\left(\vec{\theta} \lvert\hat{\vec{\theta}}, \mathbf{H}^{-1}\right)\hquad .
\end{equation}
Thus, we obtain the well known result that a Taylor expansion of an arbitrary
distribution  $p(\vec{\theta} \lvert\mathcal{D})$ around the mode $\hat{\vec{\theta}}$
truncated at the first non-vanishing term is simply a multivariate normal distribution
centered at the mode whose covariance matrix is given by the inverse Hessian (which specifies
the \emph{curvature} of the distribution) evaluated at the mode.

Laplace approximation works very well with posteriors which are well described
by a multivariate normal distribution. It is straightforward to compute because
we can use an optimizer to find the MAP point and then estimate the Hessian
numerically using finite difference approximation for the gradient or use
automatic differentiation to obtain the exact Hessian (we'll cover automatic
differentiation in the next section).

\subsubsection{Variational inference}
Another method which uses optimization to obtain an approximation to the true
posterior is \emph{Variational inference} (VI). The advantage of VI over the
Laplace approximation is that we can fit any distribution we like to the
posterior. The key idea is to model an intractable density $p(\vec{\theta}
    \lvert\mathcal{D})$ with some off-the-shelf distribution $q(\vec{\theta})$ such
that \emph{we minimize a measure of discrepancy between the two distributions}.
We optimize for the parameters of the distribution $q(\vec{\theta})$
$\vec{\psi}$ which are called the \textsl{variational parameters}. The most
common discrepancy measure is the KL divergence
(Equation~\ref{eq:kl_divergence})from $q$ to $p$. Thus, the optimization
problem is to find the parameters $\vec{\psi}^*$ which minimize the KL
divergence:
\begin{equation}
    \boldsymbol{\psi}^{*}=\underset{\boldsymbol{\psi}}{\operatorname{argmin}}\,D_{\mathbb{K} \mathbb{L}}(q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})\, \|\, p(\boldsymbol{\theta} \lvert\mathcal{D}))
    \label{eq:variational_inference}
\end{equation}
Making use of Equation~\ref{eq:kl_divergence}, we can expand the above equation as \citep{murphy_book_2022}
\begin{align}
    \vec{\psi}^* & =\underset{\vec{\psi}}{\operatorname{argmin}}\, \mathbb{E}_{q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})}\left[\ln q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})-\ln \left(\frac{p(\mathcal{D} \lvert\boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{D})}\right)\right]                           \\
                 & =\underset{\vec{\psi}}{\operatorname{argmin}} \,\underbrace{\mathbb{E}_{q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})}[-\ln p(\mathcal{D} \lvert\boldsymbol{\theta})-\ln p(\boldsymbol{\theta})+\ln q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})]}_{-\mathrm{ELBO}(\boldsymbol{\psi})}+\ln p(\mathcal{D})
\end{align}
Since the evidence $\ln\,p(\mathcal{D})$ is not a function of the variational parameters
$\vec{\psi}$, the problem of solving Equation~\ref{eq:variational_inference} reduces
to maximizing the term
\begin{equation}
    \mathrm{ELBO}(\boldsymbol{\psi}) \equiv \mathbb{E}_{q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})}[\ln p(\mathcal{D} \lvert\boldsymbol{\theta})+\ln p(\boldsymbol{\theta})-\ln q(\boldsymbol{\theta} \lvert\boldsymbol{\psi})]
\end{equation}
which is called the \textsl{evidence lower bound} (ELBO) because
$D_{\mathbb{K} \mathbb{L}}(q \| p) \geq 0$ implies that
$\mathrm{ELBO}(\boldsymbol{\psi}) \leq \ln p(\mathcal{D})$.
The most common choice for the distribution $q$ is, unsurprisingly, a multivariate
Gaussian. VI with a multivariate Gaussian differs from  the Laplace aproximation
because  in VI we are optimizing for both the mean and the covariance matrix rather
than equating the covariance matrix to the inverse Hessian at the MAP point.
VI is most commonly used with very large datasets and high dimensional parameter
spaces because it is computationally cheaper than MCMC (although VI and MCMC are not
doing the same thing). It has seen little use within astronomy.

\subsection{Sampling methods}
\label{ssec:sampling_methods}
\subsubsection{Rejection sampling}
Rejection sampling is one of the simplest algorithms for generating samples
from a probability distribution. Consider a normalized pdf
\begin{equation}
    p(\vec{\theta})=\frac{\tilde{p}(\vec{\theta})}{\mathcal{Z}}
\end{equation}
where $\tilde{\vec{\theta}}$ is the unnormalized pdf and $\mathcal{Z}$ is normalization
constant which isn't necessarily known.
If a proposal distribution $q(\vec{\theta})$ (which we know how to sample from)
satisfies the inequality
\begin{equation}
    Cq(\vec{\theta})\geq \tilde{p}(\vec{\theta})
    \label{eq:rejection_sampling_constraint}
\end{equation}
for some constant $C$, the function $Cq(\vec{\theta})$ provides an upper envelope for
$\tilde{p}$. We can use the proposal distribution to generate samples from the target
distribution using the following algorithm:
\begin{itemize}
    \item Sample $\vec{\theta}_0\sim q(\vec{\theta})$ from the proposal distribution.
    \item Sample $u_0\sim \mathrm{Unif}(0, Cq(\vec{\theta}_0))$ which corresponds to
          uniformly drawing a height under the envelope.
    \item If $u_0>\tilde{p}(\vec{\theta}_0)$ reject the sample, otherwise accept
\end{itemize}
See \citet{murphy_book_2023} for the proof that this procedure works.
If $\tilde{p}$ is a normalized target distribution the acceptence probability at
each iteration is  $1/C$ (TODO: show this) so  $C$ should be as small as possible
while satisfying the constraint in Equation~\ref{eq:rejection_sampling_constraint}.
In practice, if the distribution $p(\vec{\theta})$ represents a posterior distribution
$p(\vec{\theta}\lvert \mathcal{D})$ rejection sampling can be accomplished by using
the prior as the proposal distribution, sampling a very large number of parameter
vectors from the prior, computing the value of the likeliood
$p(\mathcal{D} \lvert\vec{\theta})$ for each and then setting $C$ to be equal to the
maximum value of the likelihood across all samples.

The major drawback of rejection sampling is that it scales badly with
increasing dimensionality of the target distribution. For instance, if
$p(\vec{\theta})=\mathcal{N}(\vec{0}, \sigma_p^2\vec{I})$ and
$q(\vec{\theta})=\mathcal{N}(\vec{0}, \sigma_q^2\vec{I})$ we need to have
$\sigma_q^2\sigma_p^2>\sigma_p^2$ to satisfy the bound. In $D$ dimensions the
optimal value of $C$ is $C=(\sigma_q/\sigma_p)^D$ and since the acceptance
probability is $1/C$ this decreases exponentially fast with dimension $D$. If
the the proposal distribution $q(\vec{\theta})$ approaches $0$ in the tails at
a faster rate than $p(\vec{\theta})$ as $\vec{\theta}\rightarrow \infty$ than
their ratio $p(\vec{\theta})/q(\vec{\theta})$ approaches $\infty$ and the
constraint in Equation~\ref{eq:rejection_sampling_constraint} will not
satisfied. Thus, we must ensure that the target distribution does not have
heavier tails than the proposal distribution.

Even though it is the simplest sampling algorithm, rejection sampling can be
very useful for problems with small numbers of parameters and small data sets
because it can deal with very pathological distributions (for example highly
multi-modal distributions). The most notable application of rejection sampling
in astronomy is the \textsf{The Joker} sampler \citep{2017ApJ...837...20P} for
sampling posterior pdfs for with very sparse binary-star and exoplanet radial
velocity measurements. \citet{2020ApJ...895....2P} apply this sampler to obtain
posterior pdfs in Keplerian parameters for $232 495$ sources from the APOGEE
catalog. Becuase these measurements are very sparse (a few data points per
star), the posterior pdf over the period prameter is highly multi-modal. The
brute-force approach of rejection sampling works very well and produces
independent samples from the distribution. For any individual object this pdf
doesn't tell us much, however, when the analysis is repeated for thousands of
objects in the framework of \textsl{hierarchical Bayesian inference} it is
nevertheless possible to obtain very tight constrain on the population-level
parameters. This is because even a highly multi-modal posterior pdf represents
a major contraction of the prior parameter space and thus excludes certain
parameter values.

The radial veclocity model in \citet{2017ApJ...837...20P} interesting because
structurally it very closely models the single lens microlensing model with
parallax effects. Both models have few parameters 7 default parameters, two of
which are linear and in both cases the posterior can be multi-modal (see
Chapter~\ref{ch:microlensing} for more on this point). In
\citet{2017ApJ...837...20P} they use the marginalized likelihood over the
linear parameters and perform rejection sampling with the dimensionally reduced
model. However, this requires an extremely large number of prior samples, on
the order of hundreds of millions! Evaluating the likelihood this many times is
much more computationally expensive in the microlensing case primarily because
microlensing light curves have a lot more data points than stellar RV
measurements. Neverthless, as I will show in Chapter~\ref{ch:microlensing}
rejection sampling is intractable with the parallax microlensing model but it
is tractable with the classic Paczynski model with 3 nonlinear parameters
$(t_0, t_E, u_0)$.

\subsubsection{Importance sampling}
\textsl{Importance sampling} is not a an algorithm for generating samples from
$p(\vec{\theta})$ but rather a method for estimating expectations
$\mathbb{E}_p[g(\vec{\theta})]$ given an existing set of samples from
some other distribution $q(\vec{\theta})$. We start by rewriting the integral 
in Equation~\ref{eq:general_expectation} as 
\begin{equation}
    \mu = \mathbb{E}_{p}[g(\vec \theta)]=\int g(\vec \theta)\,\frac{p(\vec \theta)}{q(\vec \theta)}\,q(\vec \theta) \ud \vec \theta\hquad ,
\end{equation}
where we have introduced some other distribution $q(\vec \theta)$.
The imporance sampling estimator of $\mu$ is then defined as
\begin{align}
    \hat{\mu}_{\mathrm{IS}}&=\frac{1}{S} \sum_{s=1}^{S} \frac{p\left(\boldsymbol{\theta}^{(s)}\right)}{q\left(\boldsymbol{\theta}^{(s)}\right)} g\left(\boldsymbol{\theta}^{(s)}\right)\\
    &=\frac{1}{S} \sum_{s=1}^{S} w^{(s)} g\left(\boldsymbol{\theta}^{(s)}\right)
\end{align}
where  $\boldsymbol{\theta}^{(s)} \sim q(\boldsymbol{\theta})$ are samples from the 
proposal distribution $q$ and $w^{(s)}$ are the \textsl{importance weights}. 
$w^{(s)}$  are the ratios between the target distribution $p(\vec \theta)$ and 
the proposal distribution $q(\vec \theta)$ evaluated at each $\vec{\theta}^{(s)}$.
As in the case of rejection sampling, the requirement for the proposal distribution $q$
is that it has support (is nonzero) wherever $p(\boldsymbol{\theta}) g(\boldsymbol{\theta})$
is nonzero.
Similarly to $\hat{\mu}_{\mathrm{MC}}$,  $\hat{\mu}_{\mathrm{IS}}$ is also a 
consistent and  unbiased estimator of $\mu$ but its variance is heavily dependent on 
how well the proposal distribution $q$ matches $p$. 
If we do not know the normalization constants of $p$ and $q$ we can instead use the 
self-normalized importance sampling (SNIS)  estimator 
\begin{equation}
    \hat{\mu}_{\mathrm{SNIS}}=\frac{\sum_{s=1}^{S} w^{(s)} h\left(\boldsymbol{\theta}^{(s)}\right)}{\sum_{s=1}^{S} w^{(s)}}
\label{eq:importance_sampling_estimator}
\end{equation}
which is also consistent but it has a small bias of order $\mathcal{O}(1/S)$.
There are also adaptive versions of importance sampling  which all rely on iteratively 
adapting the poposal distribution $q$ based on the importance weights at each iteration. 

When the proposal distribution $q$ is a poor approximation of $p$ the distribution of 
importance weights can have a heavy right tail which results in a large and 
large and possibly infinite variance of $\hat{\mu}_{\mathrm{IS}}$.
This is especially true in high dimensions when the importance weights $w^{(s)}$ can 
vary by several orders of magnitude and the estimator in 
Equation~\ref{eq:importance_sampling_estimator} becomes dominated by a few draws from 
proposal distribution. One solution to this  problem is to fit a generalized 
Pareto distribution\footnote{Pareto distributions are commonly used in \textsl{Extreme 
Value Analysis} (EVA). EVA is a branch of statistics dealing with heavy and fat-tailed 
distributions, a regime where many classical methods built around Gaussian assumptions 
fall apart.} 
to the distribution of importance weights and then use that distribution to produce 
a uniformly spaced set of importance weights. This procedure is called 
\textsl{Pareto Smoothed Importance Sampling} (PSIS) \citep{arXiv:1507.02646}.
PSIS forms the basis of a very popular \textsf{R} package \textsf{loo} for 
high-dimensional leave-one-out cross validation \citep{arXiv:1507.04544}.
The PSIS algorithm also reports the estimate of the smoothness parameter in the
generalized Pareto distribution (\textsl{Pareto $\hat{k}$}). $\hat{k}$ acts as a 
diagnostic  because the PSIS estimate is likely to be unstable  if $\hat{k}$ is 
too large \citep{arXiv:1507.02646}.

What if we are not just interested in computing expectations but also want to obtain 
posterior samples from $p(\vec{\theta})$ in order to, say, plot a historgram? We can 
use \textsl{importance resampling} which works by resampling the original samples 
$\vec{\theta}^{(1)} \dots \vec{\theta}^{(S)}$
with replacament with probabilities proportional to  the importance weights.
Practically, this meanse drawing a set of indices $i_1, \dots, i_S$ from a 
multinomial distribution  where the event probabilities are the normalized importance 
weights $\tilde{w}_{i}=w_{i} / \sum_{j} w_{j}$:
\begin{equation}
i_{1}, \ldots, i_{S} \sim \operatorname{Multinomial}\left(1, \tilde{w}_{1}, \ldots, \tilde{w}_{S}\right)\hquad .
\end{equation}
Samples with larger weights will appear more often than those with smaller weights. 
We can use Pareto smoothing when computing the weights. 

In the context of astronomy, one  interesting (and relevant for the work presented 
in thesis) application of importance sampling is for simplifying Bayesian hierarchical 
inference. \citet{2010ApJ...725.2166H} showed that given posterior samples for a 
collection of objects (obtained using MCMC for example) which were obtained with some 
set of vague priors on the parameters of interest, we can re-weight those samples using 
importance resampling under a different hierarchical prior which depends on some 
population level parameters and in the process obtain a posterior over the population 
level parameters. \citet{2010ApJ...725.2166H} applied this method to infer parameters 
describing the eccentricity  distribution for a catalog of $N$ exoplanets given 
posterior samples for the eccentricity for each individual planet. I will describe 
this method in more detail in Chapter~\ref{ch:microlensing} and show how it can be
applied to model the distribution of the $t_E$ parameter for a catalog of microlensing 
events.

\subsubsection{Rosenbluth-Metropolis-Hastings algorithm}
The most popular class of algorithms used for generating samples 
from a general pdf are 
\textsl{Markov Chain Monte Carlo} algorithms or MCMC for short. The simplest 
MCMC algorithm and the very first one that was developed is the
\textsl{Rosenbluth-Metropolis-Hasting algorithm} (RMH) (also known as the 
\textsl{Metropolis algorithm}). The RMH algorithm is one of 
the most famous algorithms developed in the 20th century. The original algorithm was 
published in \citet{1953JChPh..21.1087M} and it was extended by 
\citet{1970Bimka..57...97H}. The original version is extremely simple. 
Given a most recent sample $\vec\theta^{(s)}$, to generate the next sample
\begin{itemize}
    \item Draw a sample $\vec{\theta}^\prime$ from a proposal distribution $q(\bm\theta^\prime\lvert\vec\theta^{(s)})$ 
    \item Draw a uniform random number $r\sim\mathcal{U}(0,1)$
    \item If $\frac{p(\bm\theta^\prime)}{p(\vec\theta^{(k)})}$ then $\bm\theta^{(k+1)}\leftarrow \bm\theta^\prime$, else $\bm\theta^{(k+1)}\leftarrow\bm\theta^{(k)}$
\end{itemize}
The main characteristic of this algorithm is that it defines a biased random walk
through the parameter space (usually moving in only one parameter at the time) 
in such that the amount of time spent at a particular location $\vec\theta$ is 
proportional to the target density $p(\vec\theta)$.
These samples can then be used to approximate some expectation value of interest 
by means of Equation~\ref{eq:monte_carlo_estimator}. The samples are always correlated 
to en extent so estimates of the expectations don't converge as quickly with increasing 
number of samples as they would if the samples were independent.

The reason this algorithm works is because it defines a process called a 
\textsl{Markov Chain} in which the probability of moving from a current state to a
a successive state in the chain depends only on the current state and not any
past states.
Markov chains have stationary probability distributions over states and the stationary
distribution for the Markov chain defined by the RMH algorithm turns out to be
the target distribution $p(\bm\theta)$.
For this to happen, the proposal distribution $q$ must satisfied a property
called \textsl{detailed balance} which is defined as
\begin{equation}
    q(\bm\theta^\prime\lvert\bm\theta)=q(\bm\theta\lvert\bm\theta^\prime)\hquad .
\end{equation}
That is, the proposal has to be reversible so that the probability of going in either 
direction is the same. 

The proposal distribution $q$ is one of the main tunable parameters
of the RMH algoritm.
The simplest choice for $q$ is a multivariate Gaussian distribution
 $q(\bm\theta^\prime\lvert\bm\theta)\sim\mathcal{N}(\bm\theta,\vec \Sigma)$.
 The covariance matrix $\vec \Sigma$ sets a characteristic 
\textsl{step size} in the parameter space. If the step size is
too large most proposals will get rejected and the chain might miss regions with high
probability mass.
If on the other hand the step size is too small, most steps are likely to be accepted
but we might not explore the parameter space fully.
RMH (and MCMC samplers in general) don't converge to the stationary distribution  
of the target distribution immediately, instead there is always an initial exploration 
phase before the chain reaches the relevant parts of the parameter space 
(typical set).  The time it 
takes to reach stationarity is called the \textsl{mixing time} of the sampler 
or the \textsl{burn-in} period.

All MCMC algorithms have the same goal, namely, generating samples from a
target pdf such that the least number of samples $S$ are needed to accurately
approximate the integral in Equation~\ref{eq:general_expectation}. Metrics
used to judge the quality of different samplers include 
\begin{itemize}
    \item Efficiency -- what is the acceptance rate of the proposals?
    \item Coverage -- is the target distribution explored completely?
    \item Correlation -- how correlated are individual samples with past samples?
    \item High dimensions -- can the sampler handle high dimensional spaces?
    \item Derivatives -- does the sampler require gradients of the target density? 
\end{itemize}
In real world problems MCMC samplers require very careful tuning and initalization 
to work well. Although RMH and other MCMC samplers should in principle converge to the posterior distribution 
independent of the initialization point in the parameter space and the particular 
choice of parametrization; in practice intialization, careful tuning of the proposal 
distribution and how we parametrize the model is crucial for the performance 
of the sampler. It is not an usual that a minor reparametrization of a given model leads 
to an increase in sampling efficiency of several orders of magnitude\footnote{For this
reason it is often much better to invest time into making a given model more friendly 
to MCMC than to improve the evaluation time of the model by a factor of a few.}.


\subsubsection{Hamiltonian Monte Carlo}
Hamiltonian Monte Carlo (HMC) \citep{1987PhLB..195..216D,arXiv:1206.1901} is a 
variant of  Markov Chain Monte Carlo which uses the gradient of the target 
density to substantially  improve the sampling efficiency compared to 
gradient-free samplers such as the RMH sampler, especially in high dimensions.
The gradient information is especially useful for high-dimensional densities 
because it enables the sampler to better 
explore the typical set of parameter space by making proposals along the 
the thin shell comprising the typical set\footnote{In high dimensions and 
petrurbation away from the thin shell of the typical sets is likely to be 
outside the tpical set which is why samplers which are not aware of the 
geometry of the target density such as RMH work very poorly in high dimension.}.

HMC works by transforming the sampling problem to a classical mechanics 
problem expressed using the theory of Hamiltonian mechanics. 
We start by transforming the parameter space $\boldsymbol{\theta}$ into phase space by 
introducing a set of momentum parameters $\vec{p}$ (thus doubling the number of 
parameters) and define a \textsl{Hamiltonian} of the system to be the negative logarithm 
of the  joint density over $p(\vec{\theta}, \vec{p})$:
\begin{equation}
    \mathcal{H}(\boldsymbol{\theta},\mathbf p)\equiv -\ln p(\boldsymbol{\theta},\mathbf p)=-\ln p(\boldsymbol{\theta})-\ln p(\mathbf p\lvert\boldsymbol{\theta})\hquad .
\end{equation}
We can rewrite this Hamiltonian as
\begin{equation}
    \mathcal{H}(\boldsymbol{\theta},\mathbf p)= \mathcal{U}(\boldsymbol{\theta})  + 
    \mathcal{K}(\boldsymbol{\theta},\mathbf p)
\end{equation}
where $\mathcal{U}(\boldsymbol{\theta})=-\ln p(\vec \theta)$ is a potential energy term,
 $\mathcal{K}(\boldsymbol{\theta},\vec p)$ is a kinetic energy term and 
 $p(\vec\theta)$ is the target  density.
By mapping a posterior density to a physical system, we can use the machinery of 
classical mechanics by first sampling the momenta conditional on the parameters 
$\mathbf{p}\sim p(\mathbf{p}\lvert\boldsymbol{\theta})$
 and then solving Hamilton's equations
\begin{align}
  \dot{\theta}_i&=\frac{\partial\mathcal{H}}{\partial p_i}=\frac{\partial\mathcal{K}}{\partial p_i}\\
  \dot{p}_i&=-\frac{\partial\mathcal{H}}{\partial q_i}=-\frac{\partial\mathcal{U}}{\partial q_i} - 
  \frac{\partial\mathcal{K}}{\partial q_i}
\end{align}
\textsl{Symplectic integrators} (integrators which respect Liouville's theorem of 
classical mechanics) are used to integrate Hamilton's equation because of their 
high accuracy. The most common choice for an integrator is the symplectic 
(and reversible) \textsl{Leapfrog integrator}, which is defined by 
\begin{align}
\vec{p}_{t+1 / 2} &=\vec{p}_{t}-\frac{\eta}{2} \frac{\partial \mathcal{U}\left(\boldsymbol{\theta}_{t}\right)}{\partial \boldsymbol{\theta}} \\
\boldsymbol{\theta}_{t+1} &=\boldsymbol{\theta}_{t}+\eta \frac{\partial \mathcal{K}\left(\boldsymbol{p}_{t+1 / 2}\right)}{\partial \boldsymbol{p}} \\
\vec{p}_{t+1} &=\boldsymbol{p}_{t+1 / 2}-\frac{\eta}{2} \frac{\partial \mathcal{U}\left(\boldsymbol{\theta}_{t+1}\right)}{\partial \boldsymbol{\theta}}
\end{align}
where $\eta$ is the \textsl{step size}. The Leapfrog integrator works by performing a
half-update of the momentum, followed by a full update of position and another half-update 
of momentum.
Because no integrator t preserve energy perfectly an RMH acceptence criterion of the form 
$\alpha=\mathrm{min}\{1,\exp (\mathcal{H}(\boldsymbol{\theta}, \mathbf p) -\mathcal{H}(\boldsymbol{\theta}^\prime, \vec p^\prime))\}$ 
is needed at the end of the inegration step in order to restore the detailed balance 
property of Markov chains.
For the pseudocode of a complete algorithm using the Leapfrog integrator see 
for example \citet{murphy_book_2023}.


The most challenging part of implementing HMC is tuning the various hyperaparamters of 
the algorithm, namely, the number of leapfrog integration steps $L$, the step size
$\eta$ and the kinetic energy term $\mathcal{K}$.
The most common choice for the kinetic energy term is the Euclidian-Gaussian kinetic 
energy distribution where  the conditional momentum distribution is simply a 
multivariate normal distribution independent of the position:
\begin{equation}
\mathcal{K}(\vec\theta, \vec p) = \mathcal{N}(\mathbf p ; 0, \mathbf M)
\end{equation}
where the covariance matrix $\mathbf M$ is usually called the \textsl{mass matrix}.
The choice of the mass matrix $\mathbf{M}$ is important for efficient sampling because 
every transformation of the parameter space induces an inverse transformation on 
momentum space \citep{arXiv:1701.02434}. Consider for example a a cigar shaped target 
density.
We can either transform the posterior such that it looks more Gaussian and make it
 easier to sample from, or equivalently we can choose the momentum distribution such 
 that the momenta are aligned with the shape of the cigar by having higher velocity 
 proposals along the longer axis.
The optimal choice for $\mathbf{M}$ is to choose the matrix to be as close as 
possible to the covariance matrix of the parameter space.
The mass matrix is usually estimated empirically from the covariance matrix of the 
parameters during a burn-in phase of the sampling. 

As for the leapfrog parameters, we want to choose the number of leapfrog steps $L$ to be 
large enough so that the algorithm explores large swathes of the parameter space.
The most popular variant of HMC, the No U-Turn Sampler (NUTS) algorithm 
\citep{arXiv:1111.4246} adaptively chooses $L$ such that Leapfrog integration is 
automatically halted when the trajectory starts going back to its starting point (the
the momentum vector points in the direction of the starting point).
This trick ensures that parameter space updates are as large as possible and it 
reduces autocorrelation between successive samples.
In addition to automatically selecting $L$, \citet{arXiv:1111.4246} also propose a 
scheme for adapting the step size $\eta$ on the fly. Adding the automatic adaptation
of the mass matrix by running short chains during the burn-in period means that 
NUTS removes any need to tuning. The lack of complicated tuning parameters and 
the fact that NUTS works very well across a large set of problems are one of the 
reasons why NUTS (and its derivations) is the work-horse algorithm 
in many popular statistical modeling packages such as \textsf{Stan}
\footnote{\url{https://mc-stan.org/}} \citep{2017JSS....76....1C}, 
\textsf{PyMC}\footnote{\url{https://www.pymc.io/welcome.html}}, \textsf{numpyro}
\footnote{\url{https://num.pyro.ai/}} and \textsf{TensorFlow Probability}
\footnote{\url{https://www.tensorflow.org/probability}}. 

How well MCMC works depends almost entirely on the geometry of the target density
(which is parametrization dependent). If the target density is non-Gaussian 
HMC requires lots of leapfrog integration steps per iteration. It is entirely 
possible to have a 3 dimensional density with poor geometry (for example 
a multi-modal density with well seperated modes) for which HMC will 
fail but if the geometry of the distribution is well behaved HMC can scale to 
even millions of parameters.


One other notable variant of Hamiltonian Monte Carlo which works better for 
densities with highly complex geometries is 
Riemannian Manifold Hamiltonian Monte Carlo (RMHMC) \citep{girolami2011}.
RMHMC uses a position dependent kinetic energy term $\mathcal{K}$ and second order 
gradient information (quantifying the curvature of the target density) to improve 
sampling efficiency with highly correlated densities by adapting to the local geometry 
of the target density at every point. RMHMC is very rarely used and there are no 
good open-source implementations of the algorithm. The reason for this is that 
it is  difficult to construct numerically stable integrators for RMHMC
\footnote{\href{https://discourse.mc-stan.org/t/riemann-manifold-hmc-in-stan/19466/5}{This}
discussion on the \textsf{Stan} forums is particularly illuminating}.
Instead of using RMHMC, we can find better parametrizations for the target density
and use NUTS to sample it which can end up being equivalent to RMHMC 
\citep{arXiv:1910.09407}.

For an excellent visualization of different MCMC samplers see 
\href{http://chi-feng.github.io/mcmc-demo/app.html}{here}.


\subsubsection{Diagnosing MCMC chains}
How can we tell that our MCMC chains have converged to target density?
Unfortunately there is no point at which we can say that the chain definitively
converged to the target density and that we are sampling from the typical set of 
the posterior density faithfully. At best, we can
tell when the chain has not converged. The prime diagnostic for the convergence
of MCMC chains is the \textsl{integrated autocorrelation time}.
Given $S$ independent samples, the variance $\sigma^2$ for a Monte Carlo 
estimate of an expectation value $g(\vec\theta)$  is 
\begin{equation}
    \sigma^2= \frac{1}{S} \mathbb{V}_{p}\left[g(\vec\theta)\right]
    \label{eq:mcmc_error_indep}
\end{equation}
and the standard deviation of this estimator will decrease
$\sim 1/\sqrt{S}$ with the number of samples.
However, samples from $p(\vec\theta)$ produced by MCMC are not truly independent.
The way we measure this correlation is by computing the integrated autocorrelation
time $\tau_f$. If the samples are correlated, the the variance in 
Equation~\ref{eq:mcmc_error_indep} becomes
\begin{equation}
    \sigma^2= \frac{\tau_f}{S} \mathbb{V}_{p}\left[g(\vec\theta)\right]
    \label{eq:mcmc_error}
\end{equation}
where $\tau_f$ is the average number of steps needed before the chain ``forgets''
its initial position. It is defined as
\begin{equation}
    \tau_f = \sum_{\tau=-\infty}^\infty\rho_f(\tau)
\end{equation}
where $\rho_f(\tau)$ is the normalized \textsl{autocorrelation function} (ACF) of 
the stochastic process which  generated the chain for
$f$ \footnote{See this blog post by Daniel Foreman-Mackey for more details on 
autocorrelation functions: \url{https://dfm.io/posts/autocorr/}.}.
Estimating integrated autocorrelation times is difficult and in general
requires long chains because the estimate of $\tau_f$ is slow to converge.

Using an estimate of integrated autocorrelation time  we can compute the 
\textsl{Effective Sample Size} (ESS) for a given chain \citep{sokal1997}
\begin{equation}
\mathrm{ESS}\equiv\frac{S}{\tau_f}
\end{equation}
The integrated autocorrelation time (and hence also the ESS) depends on the function 
$g(\vec\theta)$ and will be different depending which expectation value we 
are interested in.
For example if we are interested in calculating the mean and require a certain
fractional precision we need to know the integrated autocorrelation time to estimate 
how many effective samples are necessary to achieve this precision.
\citet{arXiv:1903.08008} suggest computing ESS for both the bulk of the distribution 
(captured by the median) and \textsl{tail-ESS} which they define to be the minimum 
of the ESS for the 5\% and 95\% quantiles of the distribution. Tail-ESS will generally 
be smaller  than the ESS for the median because MCMC converges at a slower rate 
in the tail of the distribution.

Since MCMC samples are correlated, some users discard all except every $n-th$ sample in a process called
\textsl{thinning} the chains. Except for reasons of storage in memory, this
procedure should be discouraged because discarding information never helps with
better estimates and the Monte Carlo error in Equation~\ref{eq:mcmc_error} 
already incorporates the fact that the samples are correlated.

Another popular statistic for the diagnostic of MCMC chains is the
\textsl{Gelman-Rubin} $\hat R$ statistic \citep{1992StaSc...7..457G} which uses 
information pooled from multiple chains to judge convergence. It is 
defined as
\begin{equation}
    \hat R= \frac{\hat V}{W}
\end{equation}
where $W$ is the sample variance within a single chain, and $\hat V$ is the
variance estimate pooled across several independent chains.
The $\hat R$ statistic converges to 1 when each of the chains accurately 
sample the target density. A value greater than 1 indicates that one or more 
chains have not yet converged.

Finally, the most sophisticated diagnostic for the convergence of MCMC chains
attempts to detect the the breakdown of the geometric ergodicity property of
MCMC. Geometric ergodicity is a necessary condition for MCMC estimators to
follow the central limit theorem ensures that the estimator in 
Equation~\ref{eq:monte_carlo_estimator} is consistent and unbiased.
HMC algorithms enable the detection of so called
\textsl{divergences} 
\footnote{\url{http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html}.}.
 in the parameter space -- points in the parameter space 
where the target density has very large gradient indicating 
a suspected  breakdown of geometric ergodicity.
The presence of a large number of divergences in a model can lead to 
biased estimators. 
Modern statistical modeling packages include functions for computing ESS 
estimators,  $\hat R$ and divergences. Without the use of these diagnostic 
tools we cannot be sure that the MCMC chains have converged and that our results 
are correct.



\subsubsection{Failure modes for samplers}
MCMC methods work really well for distributions with relatively well-behaved 
geometries but tend to fail with more complex distributions, particularlly 
multi-modal distributions with well seperated modes. 
 \citet{arXiv:2101.09675} lists 
key properties of densities for which MCMC methods tend to fail:
\begin{itemize}
    \item \textbf{Highly non-Gaussian distributions}: For example banana shaped
    distributions, distributions with heavy tails.
    \item \textbf{Multimodal distributions}: If there are multiple well 
    separated  modes in the target densities, even if each mode is individually 
    well described by a Gaussian MCMC methods will tend to get stuck in one of
    the modes and fail to jump between modes.
    \item \textbf{High dimensionality}: High dimensional problems can break pretty 
    much every algorithm, they are however less of a problem for HMC like algorithms.
    \item \textbf{Highly informative distributions}: If the data is particularly 
    informative the posterior occupies an extremely small portion of the total 
    prior volume and it may be challenging to discover those regions. Even HMC 
    can fail here if the posterior mass is concentrated  in a very narrow 
    ``canyon'' if the momentum of the proposals is too large.
    \item \textbf{Phase transitions}: These are abrupt changes in the structure 
    of the likelihood as we move up constant likelihod contours.
\end{itemize}
The first and second failure modes are particularly important for this thesis 
because likelihoods for microlensing models are highly non-Gaussian and 
multimodal. In the next section we cover a class of algorithms which (on paper!)
addresses almost all of these challenges.

\subsubsection{Nested Sampling}
\textsl{Nested sampling} \citep{2004AIPC..735..395S}  is fundamentally different 
from MCMC algorithms  because it was designed for the purpose  of estimating the
Bayesian evidence  $\mathcal{Z}$ (Equation~\ref{eq:evidence}) instead of drawing samples 
from  a target distribution.
Since $\mathcal{Z}$ is just a normalization constant, it is usually ignored 
in traditional MCMC algorithms such as RMH which don't require the target 
density to be properly normalized.
However, $\mathcal{Z}$  becomes important in the context of model comparison 
using \textsl{Bayes factors} (see Section~\ref{ssec:model_comparison}) which
are proportional to the ratios of the evidences of two models. 

In addition to estimating $\mathcal{Z}$ Nested Sampling produces samples 
from posterior $p(\vec\theta\lvert \mathcal{D})$ as a sort of byproduct. 
The major advantage of NS over MCMC methods is that acts as a sort of 
global search algorithm working from the outside-in which improves its 
performance on highly multi-modal distributions\footnote{
    The extent to which this claim is true will be investigated in 
    Chapter~\ref{ch:microlensing}.
}. Nested Sampling has been extensively used in cosmology in particular, mostly 
thanks to the popularity of the \textsf{MultiNest} package \citep{arXiv:0809.3437}. 
\citet{arXiv:2205.15570} is great introductory review to these methods while 
\citet{arXiv:2101.09675} goes into a lot more depth.

We start by rewriting the evience $\mathcal{Z}$ from Equation~\ref{eq:evidence}
as 
\begin{equation}
\mathcal{Z}=\int \mathcal{L}(\vec\theta) \pi(\vec \theta)\ud\vec\theta
\label{eq:evidence2}
\end{equation}
where  $\mathcal{L}\equiv p(\mathcal{D}\lvert\vec \theta)$ is the likelihood 
and $\pi(\vec \theta)$ is the prior. The key idea behind NS is that it is possible 
to replacec the high-dimensional integral in Equation~\ref{eq:evidence2} with 
a one-dimensional integral over the entire prior volume: 
\begin{equation}
    \mathcal{Z}=\int_{0}^{1} \mathcal{L}(X) \ud X
    \label{eq:nested_sampling}
\end{equation}
where $\ud X$ is the volume element of points in the parameters space which 
share the same likelihood $\mathcal{L}(X)$ weighted by the prior $\pi(\vec\theta)$:
\begin{equation}
X\left(\mathcal{L}^{\star}\right)=\int_{\mathcal{L}>\mathcal{L}^{\star}} \pi(\vec\theta) \ud \vec\theta\hquad .
\end{equation}
$\mathcal{L}=\mathcal{L}^\star$ defines a contour of constant likelihood and 
$X(\mathcal{L}^\star)$ is the prior volume enclosed by that contour.
\emph{Nested sampling thus replaces the original problem of sampling from the posterior 
to  sampling from prior with a likelihood constraint}.
For a derivation of Equation~\ref{eq:nested_sampling} see \citet{arXiv:2205.15570}.

The standard version of the NS algorithm works by drawing $K$ \textsl{live points} 
(points in the parameter space) from the prior at each iteration $i$, removing 
the point with the lowest likelihood value $\mathcal{L}_i$ and replacing it 
with a new live point sampled from the prior and subject to the 
constraint $\mathcal{L}_{i+1}>\mathcal{L}_i$ (see \citet{arXiv:2205.15570} 
for details). This process continuously shrinks the volume of the prior by an 
approximately constant factor, effectively scanning the parameter space from 
contours with a lowest likelihood to contours with a highest likelihood.
The algorithm terminates when some fractional error tolerance on the estimation 
of the evidence is reached. 
At the end of the iterative procedure we are left with 
estimates of the volume $X(\mathcal{L}^\star)$ at each iteration and we can 
estimate the integral in Equation~\ref{eq:nested_sampling} using the trapezoidal 
rule:
\begin{equation}
\mathcal{Z}=\sum_{i=1}^{K}w_i\,\mathcal{L}_i^\star\hquad ,
\end{equation}
where the weights $w_i$ are 
\begin{equation}
w_{i}=\frac{1}{2}\left(X_{i-1}-X_{i+1}\right)\hquad .
\end{equation}
The posterior samples can be obtained from the from the discarded live points 
by assigning weights $\hat{w}_i$ to each live point:
\begin{equation}
    \hat{w}_i\equiv \frac{\mathcal{L}_i\,w_i}{\hat{\mathcal{Z}}}
\end{equation}
To compute the ESS for these samples, we can use the exectation value dependent 
estimator provided by \citet{arXiv:1809.04129}:
\begin{equation}
    \widehat{\mathrm{ESS}}=\frac{1}{\sum_{s=1}^S\tilde{w}_i^2}
\end{equation}
where
\begin{equation}
\tilde{w}_{i}\equiv\frac{\left|g\left(\vec{\theta}^{(s)}\right)\right| \hat w_{s}}{\sum_{i=1}^{S}\left|g\left(\vec{\theta}^{(i)}\right)\right| \hat w_{i}}
\end{equation}


\cite{arXiv:2101.09675} identifies three distinct phases in the behavior 
of the NS algorithm. In the first phase the prior volume shrinks towards the 
bulk of the posterior (the typical set) and during this phase the live points 
vary in likelihood by many orders of magnitude. All of the probability mass is 
associated with a single live point because the live points are all associated 
with an equal volume. 
The typical set is resolved in the second phase and the live points have 
comparable wieghts.  In the final stage NS reaches the region around the MLE 
with large values of the likelihood but very small volume.
Unlike MCMC, NS has a well defined termination criterion and running the algorithm 
for a longer time does not produce more posterior samples (there are other ways 
of producing more samples after an NS run has terminated).
NS is different compared to HMC in that HMC explores level sets of energy where 
the Hamiltonian is constant while NS scans through a range of energy levels. 
It's this feature that enables it to discover multiple modes in the density.


Historically NS has been  extensively used by physicists and almost completely 
ignored by statisticians. This has been changing in recent years and there 
is now more work from both communities aimed at understanding and improving the 
algorithm and better diagnosing its failure modes (see for example 
\citet{ arXiv:1407.5459,arXiv:1804.06406,arXiv:1704.03459,arXiv:2101.09675}).
For example, a recent paper by \citet{arXiv:1805.03924}  reframes the NS 
algorithm as a special case of \textsl{Sequantial Monte Carlo}, a class of 
algorithms that have been extensively studied by statisticians for decades.
There are also lots of modern open-source implementations of NS such as 
\textsf{dynesty} \citep{arXiv:1904.02180}, \textsf{JAXNS} \citep{arXiv:2012.15286}
and \textsf{UltraNest} \citep{2021JOSS....6.3001B}. 
\textsf{UltraNest} in particular is the most developed modeling framework 
focused on correctness of posterior inferences. Ultimately, the only way 
to check whether NS is doing what it promises to do is to apply it to a real 
world problem. In Chapter~\ref{ch:microlensing} we do just that and compare 
the results and performance of NS to HMC.

\subsubsection{Ensemble samplers with affine invariance}
By far the most popular sampler in physics and astronomy is an MCMC method 
called \textsl{affine invariant ensamble sampler} (AIES)
\citep{2010CAMCS...5...65G} which was popularized by an excellent \textsf{Python}
implementation of the algorithm \textsf{emcee} 
\citep{arXiv:1202.3665,arXiv:1911.07688}. The method is similar 
to RMH  in the sense that it does not use gradient information and it requires 
only that the evaluation of the log-probability function. The special property 
of affine invariant MCMC is that it is invariante to 
\textsl{affine transformations} (linear transformations such as 
scale transformations and rotations) of the parameter space. It uses multiple 
MCMC chains (called \textsl{walkers}) to explore the parameter space which 
are correlated by construction. 

\textsf{emcee} is so popular that the 
original paper has been cited almost 8000 times so far.  
The success of \textsf{emcee} has less to do with its actual suitability 
to a wide range  of problems and more to do with the fact that it is very easy to use with the 
kinds of models astronomers and physicists work with. \textsf{emcee}  works with
black-box physics simulators and it is also easily parallelizable. 
This feature also makes it easy to misuse and apply to problems for which it is 
not suitable. For example, \citet{arXiv:1509.02230} showed that \textsf{emcee} 
does not work well in high dimensions (greater than about 10) and it can appear 
as if the sampler converged when in reality it has not. Nevertheless 
\textsf{emcee} can still be very useful and fast for very low dimensional 
problems. As with other MCMC samplers, it will fail with multi-modal 
likelihoods.


\subsubsection{Other methods}
That there are many many algorithms in the  literature besides the ones I've covered here. 
The vast majority of these  algorithms are variants of RHMC, HMC or  
NS. Most of these algorithms don't have well tested opens-source implementations, 
the examples presented in the papers are usually low-dimensional toy 
problems, and the algorithms require very careful tuning to work at all. This 
is why the development of general purpose statistical modeling 
(probabilistic programming) packages such as \textsf{Stan} and 
\textsf{PyMC} has been so important. They demonstrated that methods such as 
NUTS work well for a wide variety of problems.

\subsection{Model validation and comparison}
\label{ssec:model_comparison}

So far I have discussed inference of model parameters using either optimization 
methods such as maximum likelihood or sampling methods such as MCMC and Nested 
Sampling.
Using these methods we can obtain some estimates of the parameters of a specific  
model but this  does not tell us if the model is any good in the first place.
Every time we are doing parameter inference we assume that the particular model
is true.
A major challenge in statistical inference is comparing different 
models which can explain the same dataset, so called \textsl{model comparison}.
What we mean by different models is somewhat fuzzy. We could for example have 
a model specified by the same likelihood function and a set of continous 
parameters and then call different realizations of those parameters different 
models. This does not make much sense in the Bayesian context. In most cases
when a model comparison problem can be cast as a parameter inference problem it 
is easier to treat it as a parameter inference problem\footnote{Although, as we
will see in Chapter~\ref{ch:microlensing} it may be advantageous  to use model 
comparison methods when comparing different modes in a multi-modal posterior.}.
This happens when two \emph{models are nested}, meaning they share one or 
several parameters.
For example, let's say we are fitting a microlensing model and trying to decide 
whether  or not to include the finite source effect parametrized by a continuous 
parameter $\rho_\star$.
We could fit two separate models, a model which uses a point source 
aproximation for the magnification and one which includes the finite source 
effects via the $\rho_\star$ parameter and then decide which model better 
describes the data. The alternative is to simply fit the more complex model, 
obtain a posterior for the $\rho_\star$ parameter and check if it differs from 
zero in a meaningful way. Although the former approach is more elegant sometimes
it is practically intractable because we might end up having trouble sampling 
the posterior if the additional parameter is not well constrained by the data\footnote{In those
cases is may nake sense to put a strong prior pushing the parameter to zero which 
helps with inference. If the parameter is well constrained by the data the 
likelihood will push the posterior away from zero.} or if the model with the 
extra parameter is much more computationally expensive. 
If the models are nested but the paramer which differentiates between models is 
discrete it is sometimes still possible to cast the comparison problem as a parameter 
inference problem using \emph{dimension jumping} methods such as 
\textsl{Reversible-jump MCMC} (RJ-MCMC)
\citep[See][for an application of a method similar to RJ-MCMC to an 
astronomy problem]{2015MNRAS.448.3206B}.
However, these methods are computationally very difficult to implement.

When the models are not nested we have no choice but to do model comparison. 
An example would be fitting a single lens microlensing model vs. a binary lens 
or a binary source star model. Statistics alone does not tell us what to put in the 
abstract of a paper after we have completed the analysis for multiple competing 
models\footnote{This is the subject of decision theory which relies on 
specifying \textsl{utility functions} (which depend on our posterior beliefs) 
to quantify expected loss given a certain decision}. Best we can hope for is 
assigning probabilities to each of the models taking into account how well they 
fit the data and how parsimonious they are. Model 
comparison is one of the key problems with analyzing microlensing light curves 
because there are often multiple very different models which make near-identical 
predictions  
\citep[see][for examples from microlensing]{ 2017AJ....153..129J,2020AJ....160...17H,2021AJ....162...59R}.
Especially problematic is the fact that when multiple models equally well 
describe the data, \emph{some models will be more interesting than others}.
For example one model might be a triple lens model implying the discovery of 
an \textsl{exomoon} in a planetary system. The other model could be a binary 
source star model. The major issue here is that the incentives of researchers 
are not always aligned with truth-seeking and the outcome of a model 
comparison test can be a strong function of how much effort was invested into 
a given model\footnote{This is one of the causes of the replication crisis in 
many scientific disciplines.}. In the extreme case, if we only consider one interesting model then
there is no chance we can conclude that  a more mundane model describes the 
data equally well. This is why it is very important to have a clearly defined 
framework for assesing different models.

\subsubsection{Frequentist model comparison (hypothesis testing)}
The question of model comparison is different depending on whether
one uses the frequentist or Bayesian statistics. In classical frequentist
statistics, one has to form the \textsl{null hypothesis} $\textrm H_0$ which 
represents a baseline (null) model and an \textsl{alternative hypothesis} which is 
the hypothesis of interest. For instance, the two hypotheses could be
\begin{align}
    \textrm H_0: & \; \textrm{there is no planet in the data} \notag\\
    \textrm H_1: & \;\textrm{there is a planet in the data}\notag
\end{align}
We then form a \textsl{hypothesis test} under some \textsl{test statistic} 
and based on the outcome of that hypothesis test we either reject the null 
hypothesis or we don't, depending on an arbitrary threshold called the 
\textsl{p-value}.

The p-value is the probability of obtaining data equal to or more extreme than 
what is observed, \emph{assuming that the null hypothesis $\textrm H_0$ is true}.
It is the probability for obtaining the observed
dataset in an infinite set of draws of the data, \emph{it is not a statement 
about the probability of the hypothesis}\footnote{The definition of the p-value
is so confusing that even some statistics textbooks get it wrong.}.
All hypothesis testing does is answering the following question:
assuming there's nothing interesting in the data, how likely was I to observe 
the data I was given.
If a certain threshold p-value is reached ($0.05$ in most scientific disciplines, 
substantially less in physics and astronomy), the null hypothesis hypothesis 
is rejected.
The fact the null hypothesis is rejected cannot be interpreted as evidence in
favour of the alternative hypothesis because such a statement would involve
using Bayes' theorem to invert the probabilities. However, it is almost always
(mis)interpreted in that way.
Given a set of models $\{\mathcal{M}_1,\mathcal{M}_2,\dots,\mathcal{M}_n\}$, all
we are formally allowed to do is to  reject some of those models based on 
a p-value, and keep the rest.


One might wonder if there is as an absolute way of judging the quality of a
single isolated model. In astronomical literature, this usually falls under the term of
a \textsl{goodness of fit} statistic. These statistics implicitly use the 
hypothesis testing procedure described above, rejecting the null hypothesis 
under some p-value. Popular choices are the \textsl{Chi-squared test} 
(not to be confused with the $\chi^2$ loss function),
\textsl{Kolmogorov-Smirnov test}, \textsl{Anderson-Darling test}, etc.
These tests rely on strong assumptions about the data generating process 
and they are often misused. 
It is often preferable (if possible) to just use visual checks on the predictions 
of the model 
\citep[see][for a visualization guide in the Bayesian paradigm]{arXiv:1709.01449}. 
For example plotting the predictions of the model in data space, plotting 
the residuals, etc.  Probabilistic modeling is almost always an iterative 
process and visualization helps us spot where the model is failing to describe 
the data adequatelly.

\subsubsection{Bayesian model comparison using Bayes factors}
In the Bayesian paradigm, the most natural way of comparing different 
models is by making use of  Bayes' theorem. We can assign the probability of a 
given model $\mathcal{M}$ as 
\begin{equation}
    p(\mathcal{M}\lvert\mathcal D)\propto p(\mathcal{M})\,p(\mathcal D\lvert\mathcal{M})
\end{equation}
where $p(\mathcal{M})$ is the prior probability of the model $\mathcal{M}$ and
$p(\mathcal D\lvert\mathcal{M})$ is the Bayesian evidence. If we are comparing two 
models, $\mathcal{M}_o$ with a set of parameters $\vec\theta_0$ and
and $\mathcal{M}_1$ with a set of parameters $\vec\theta_1$, we can form the ratio of 
the their posterior probabilities given the data
\begin{equation}
    \frac{p(\mathcal{M}_0\lvert\mathcal D)}{p(\mathcal{M}_1\lvert\mathcal D)} =
    \frac{\mathcal{Z}_0}{\mathcal{Z}_1} \, \frac{p(\mathcal{M}_0)}{p(\mathcal{M}_1)}\hquad .
\end{equation}
The ratio of the two evidences on the right hand side is called the 
\textsl{Bayes factor}, and the ratio of prior probabilities is the 
\textsl{prior odds}.
Prior odds can be set to unity under the assmuption that both models are equally 
likely a priori.
A large Bayes factor $\textrm{B}_{01}\equiv \mathcal{Z}_0/\mathcal{Z}_1$ should 
be interpreted as evidence in favour of model 0 versus model 1, given the 
observed data $\mathcal D$.
As in the case of hypothesis testing, one can then define arbitrary criteria for
the strength of the evidence in favour of model 0 conditional on the specific value
of the Bayes factor.
The popular choice is using Jeffreys scale \citep{1939thpr.book.....J}.
Unlike the p-value, Bayes factors (multiplied by prior odds)
are actual probabilities for the models
themselves, rather than probabilities for the data in an infinite set of trials.
One useful aspect of Bayesian model comparison is the fact that it automatically
implements \emph{Occam's razor}, penalizing overly complicated models if there
is a simpler alternative \citep[see Chapter 28 of][]{2003itil.book.....M}.
The goal when using Bayes factor is to either select a single ``best'' model
$\mathcal{M}_i$ or, the more Bayesian option, to average over a set of 
multiple models using their  posterior probabilities 
$p(\mathcal{M}_i\lvert\mathcal D)$.

There are two major problems with Bayes' factor. The first one is the fact that
whereas the Bayesian evidence $\mathcal{Z}$ could often be ignored when inferring the 
parameters of the models, it is a crucial quantity in Bayesian model comparison. 
Computing $\mathcal{Z}$ is very computationally expensive, error prone, and it
 requires methods such as Nested Sampling.
The other problem with Bayes
factors is that the estimates of $\mathcal{Z}$ are particularly sensitive to the 
width and the shape of te prior distribution for the parameters. For example, if 
the prior for parameter $\theta_k$
in model $\mathcal{M}_0$ is $\theta_k\sim \mathcal{N}(0, s^2)$, where $s$ is
very large, the evidence will scale as $\mathcal{Z}\sim 1/s$ practically
independently of the data. This kind of dependence of the evidences on the
parameter priors is not unexpected, it means that models which ``waste'' prior
parameter space will be penalized, but it is often undesirable\footnote{See the 
excellent essay on Bayes factors by \citet{navarro_2020}.}. 

Thus, in order to use Bayes factors for model comparison, one has to be able to
compute evidences, and think very carefully about the choice of priors. Despite
these issues, Bayes factors have been used extensively in staistics and the 
natural sciences, most notably in cosmology. For example,\citet{arXiv:1312.3529} 
used Bayesian model comparison to estimate Bayes factors of 193 different 
inflation models using the Cosmic Microwave Background
data from the Planck satellite and Higgs inflation as the reference model. 

\subsubsection{Cross validation}
The more tractable and robust alternative to estimating Bayes factors is using 
\textsl{cross validation}. The key idea behind cross validation (CV) is to judge 
different models based on how well they predict unseen data\footnote{This is 
principle method of model comparison in machine learning. In machine learning 
it is customary to split the original dataset into a \textsl{training dataset} 
and a \textsl{validation dataset}. The training dataset is used to optimize 
the main parameters of the model and the loss function evaluated on the 
validation dataset is used to compare different models or set the hyperparameters
of a given model.}. 
CV can be used to assess the predictive power of a single model (as a sort of 
goodness of fit metric) to compare or average multiple models. 
The idea behind CV is to split the dataset into $K$ parts or \textsl{folds}, 
re-fit the model including $K-1$ folds, evaluate the objective function (log 
likelihood or posterior) and then repeat the procedure for each fold. We 
can then for example add up the $K$ values and compute an estimate 
the predictive performance of the model. 
If we set $K$ to be equal to the number of data points $N$, then we 
are talking about \textsl{leave-one-out cross validation} (LOO-CV).

One measure of the predictive performance of a given model $\mathcal{M}$ which has 
good theoretical properties is ther so-called 
\textsl{exptected log predictive density} (ELPD) 
\citep[see for example][]{10.1214/12-SS102}. It is defined as 
\begin{equation}
\operatorname{ELPD}(\mathcal{M})\equiv \int \ln p\left(\tilde{\vec y}_{i} \lvert \vec y, \mathcal{M}\right)\,p_{t}\left(\tilde{\vec y}_{i}\right) \ud \tilde{\vec y}_{i}
\end{equation}
where 
\begin{equation}
p(\tilde{\vec y}_i\lvert\vec y)=\int p\left(\tilde{\vec y}_{i} \lvert \vec \theta,\mathcal{M}\right)\,p(\vec\theta \lvert \vec y) \ud \vec\theta
\end{equation}
is the posterior predictive distribution for a new observation $\tilde{\vec y}_i$ that 
was generated from some unknown true distribution $p_t(\tilde{\vec y}_i)$.
ELPD captures the expected predictive performance of the model for some future data
$\tilde{\vec y}$.  It tells us how well the model will generalize to new data.

One method for estimating ELPD is using LOO-CV. In LOO-CV we compute $N$ leave-one-out 
posterior distributions $p(\vec\theta\lvert \vec{y}_{-i})$ where $\vec y_{-i}$ 
denotes all of the observations except the $i$-th observation. Using these 
posterior estimates we can estimate the ELPD as 
 \begin{align}
\mathrm{ELPD}_{\mathrm{LOO}} &\approx\frac{1}{N} \sum_{i=1}^{N} \ln p\left(\vec y_{i} \lvert \vec y_{-i},\mathcal{M}\right) \\
\label{eq:elpd_loo}
&=\frac{1}{N} \sum_{i=1}^{L} \ln\int p\left(\vec y_{i} \lvert \vec \theta, \mathcal{M}\right) p\left(\vec\theta \lvert \vec y_{-i},\mathcal{M}\right) \ud \vec\theta \\
\end{align}
where $p(\vec y_i\lvert \vec \theta)$ is the likelihood for the $i$-th observation 
and $p(\vec\theta\lvert \vec y_{-i},\mathcal{M})$ is the posterior for $\vec\theta$ 
obtained by fitting the full dataset except the held-out observation $\vec y_{-i}$.

A naive implementation LOO-CV is very computationally expensive because it 
involves re-fitting the model $N$ times. Fortunately, there are reliable methods for 
approximating it. 
The most popular approximation uses importance sampling such that the target 
distribution is the LOO posterior and the proposal distribution is the full
posterior. Given $S$ draws from the full posterior 
$\vec\theta\sim p(\vec\theta|\vec y, \mathcal{M})$ we can estimate 
$p(\vec y_i\lvert \vec y_{-i},\mathcal{M})$ as 
\begin{align}
\ln p\left(\vec y_{i} \lvert \vec y_{-i},\mathcal{M}\right)&\approx\ln\left(\frac{\frac{1}{S} \sum_{s=1}^{S} p\left(\vec y_{i} \lvert\vec \theta^{(s)},\mathcal{M}\right) w\left(\vec\theta^{(s)}\right)}{\frac{1}{S} \sum_{s=1}^{S} w\left(\vec\theta^{(s)}\right)}\right) \\
\label{eq:prob_loocv_pointwise}
w\left(\vec\theta_{s}\right)&=\frac{p\left(\vec\theta^{(s)} \lvert \vec y_{-i},\mathcal{M}\right)}{p\left(\vec\theta^{(s)} \lvert\vec y,\mathcal{M}\right)} \propto \frac{1}{p\left(\vec y_{i} \lvert\vec\theta^{(s)},\mathcal{M}\right)}
\end{align}
If particular observations are highly influential (removing one data point 
noticeably changes the results) the target LOO posterior can differ 
from the full posterior which is why Pareto smoothed importance sampling (PSIS) 
(see Section~\ref{ssec:sampling_methods}) is used to improve the estimate. 
Using Equation~\ref{eq:prob_loocv_pointwise} and Equation~\ref{eq:elpd_loo}
we define \citep{2015arXiv150704544V} 
\begin{equation}
\mathrm{ELPD}_\mathrm{PSIS-LOO}\equiv\sum_{i=1}^N\ln\left(\frac{\frac{1}{S} \sum_{s=1}^{S} p\left(\vec y_{i} \lvert\vec \theta^{(s)},\mathcal{M}\right) w\left(\vec\theta^{(s)}\right)}{\frac{1}{S} \sum_{s=1}^{S} w\left(\vec\theta^{(s)}\right)}\right) \hquad .
\label{eq:elpd_psis_loo}
\end{equation}
We can use the value of the Pareto $\hat k$ from PSIS to determine if the 
PSIS-LOO ELPD estimator of $p(\vec\theta\lvert\vec y_{-i})$ for the $i$-th data 
point potentially do a full re-fit of the model only for the most problematic 
points. \citet{arXiv:1004.2316} showed that LOO is a consistent and unbiased estimator of 
the ELPD.

Equation~\ref{eq:elpd_psis_loo} thus enables us to estimate the expected log pointwise 
predictive density for unseen data using only posterior samples without having 
to re-fit the model (unless Paretko $\hat k$ is large for certain data points). 
This equation forms the basis for the widely popular \textsf{loo} statistical package 
\citep{loo_package}.
$\mathrm{ELPD}_\mathrm{PSIS-LOO}$ is closely related to the WAIC information 
criterion \citep{arXiv:1004.2316} which is an alternative approach to estimating 
ELPD. LOO and WAIC are asymptotically equivalent but LOO is a more robust and less 
biased estimator with finite data \citep{arXiv:1507.04544}.
Other information criteria such as AIC, BIC and DIC are inferior to LOO and WAIC 
estimates of ELPD because they use point estimates (maximum likelihood) to assign 
a score to a model and they attempt to quantify model complexity in an ad-hoc 
way (by including a term proportional to the number of parameters in the model). 
LOO-CV is also less sensitive to the choice of priors and often more reliable 
for model comparison than Bayes factors \citep{10.1214/17-BA1091,arXiv:2003.04026}.

What if our model includes a Gaussian Process and the likelihood is not 
factorizable? In that case we cannot use Equation~\ref{eq:elpd_psis_loo} but 
\citet{arXiv:1810.10559} showed how to evaluate 
$\ln p(\vec y_i\lvert\vec y_{-i},\vec\theta,\mathcal{M})$ if the likelihood is 
multivariate normal with a dense covariance or a Student-t likelihood so it is 
still possible to use LOO-CV cheaply. 
If we are dealing with time-series data and we are only interested in predicting 
the next few points conditional on the previous data points then it may not 
much sense to use LOO-CV because we are using future data points when evaluating 
predictions on past data points. \textsl{leave-future-out cross validation} 
(LFO-CV) \citep{arXiv:1902.06281} is a generalization of LOO which takes into 
account the time-ordering of the data points in a time-series. 

For a more information about Cross Validation and its approximations see the excellent
\href{https://avehtari.github.io/modelselection/CV-FAQ.html}{Cross-validation FAQ}
by Aki Vehtari.
In Chapter~\ref{ch:microlensing} I will demonstrate the utility of these methods by applying 
them to very practical problems in the analysis of microlensing light curves.


\subsection{What about machine learning?}
%% mention that even if we have a model focused on inference, it might be best if we judge 
%% its quality through its predictive performance
%
\section{Automatic differentiation}
%\section{Putting it all together -- \emph{probabilistic programming}}
%\label{sec:programming}
%\subsection{The success of machine learning}
%% that quote from Yann LeCun on differentiable code 
%\subsection{Automatic differentiation}
%\subsubsection{Forward mode autodiff}
%\subsubsection{Reverse mode autodiff or backpropagation}
%\subsubsection{Jacobian vector products}
%\subsection{JAX}
%\subsection{Numpyro}

% CHAPTER 3: Modeling microlensing events 
%% PLOT POINTWISE LOO 
\chapter{Modeling microlensing events}
\label{ch:microlensing}
\section{Single lens events}
\section{Modeling single lens events}
\section{Numerical solutions to the lens equation}
\section{\textsf{caustics} -- computing the magnification of an extended limb-darkened source}
\section{Comparison with previous work}
\section{Modeling binary lens events}
\section{Population-level inference using hierarchical modeling}

% CHAPTER 4: Mapping the surface of Io 
\chapter{Mapping the surface of Io}
\label{ch:mapping_io}
\section{The data}
\section{Information content}
\section{Pixel sampling}
\section{A static map model}
\section{A dynamic map model}

% CHAPTER 5: Mapping the surfaces of exoplanets
\chapter{Mapping the surfaces of exoplanets}
\label{ch:mapping_exoplanets}
\section{Previous work}
\section{What can we learn using eclipse mapping?}
\section{Time dependent maps}

% CHAPTER 6: Conclusion
\chapter{Conclusion}
\section{Summary of the thesis}
\subsection{Microlensing}
\subsection{Occultation mapping}
\section{Future work}
\subsection{Microlensing in the era of the Roman telescope}
\subsection{Mapping volcanic activity on Io}
\subsection{Mapping exoplanets with JWST and beyond}

\appendix

\chapter{Complex polynomial coefficients}
\label{app:complex_poly}
\section{Lens equation}
In this appendix we (partially) derive the coefficients of the complex
polynomial derived from the lens equation
(Equation~\ref{eq:lens_equation_complex}). Taking the complex conjugate of
Equation~\ref{eq:lens_equation_complex}, have
\begin{equation}
    w=z-\sum_{i=1}^{N} \frac{\epsilon_{i}}{\bar{z}-\bar{z}_{i}}
\end{equation}
Let's define $z_i\equiv z - z_j$ and two polynomials $G$ and $H$
\begin{align}
    G & =\sum_{k=0}^{N-1} G_{k} z^{k} =\sum_{j=1}^{N} \epsilon_{j} \prod_{i=1, i \neq j}^{N} z_{i} \\
    H & =\sum_{k=0}^{N} H_{k} z^{k}   =\prod_{i=1}^{N} z_{i}
\end{align}
it follows that
\begin{equation}
    \frac{G}{H}=\frac{\sum_{j=1}^{N} \epsilon_{j}
        \prod_{i \neq j} z_{i}}{\prod_{j=1}^{N} z_{j}}=\sum_{j=1}^{N} \frac{\epsilon_{j}}{z_{j}}
\end{equation}
and
\begin{equation}
    \bar{z}=\bar{w}+G / H
\end{equation}
Defining $\omega_{j} \equiv \bar{r}_{j}-\bar{w}_{i}$, we obtain
\begin{equation}
    z-w=\sum_{j=1}^{N} \frac{\epsilon_{j}}{G / H+\bar{w}-\bar{r}_{j}}
\end{equation}
Multiplying the above equation with $\prod_{j=1}^{N}\left(G-\varpi_{j} H\right)$
results in
\begin{equation}
    0=(z-w) \prod_{j=1}^{N}\left(G-\varpi_{j} H\right)-
    H \sum_{j=1}^{N}\left[\epsilon_{j} \prod_{i=1, j \neq i}^{N}\left(G-\varpi_{i} H\right)\right]
    \label{eq:complex_poly}
\end{equation}
Equation~\ref{eq:complex_poly} is complex polynomial of degree $N^2+1$. To obtain coefficients of this
polynomial in terms of fundamental parameters we use a computer algebra software package \textsf{SymPy}\footnote{
    \url{https://www.sympy.org}
}\citep{10.7717/peerj-cs.103}. A Jupyter notebook with the complete derivation is available
\href{https://github.com/fbartolic/caustics/blob/main/notebooks/ComplexPolynomialCoefficients.ipynb}{here}.
I have managed to derive the coefficient for the binary and triple lens case but \textsf{SymPy} fails to
converge for the quadruple lens case because the number of coefficients becomes very large.
It is possible that an alternative computer algebra system such as Wolfram Mathematica would work.

\section{Critical curve equation}
\label{app:complex_poly_crit}
Similarly to the lens equation, we can derive the coefficients of the complex
polynomial equation whose roots are points on the critical curve. To solve for
these points we need to solve the following equation
\citep{1990A&A...236..311W}:
\begin{equation}
    \sum_{i=1}^{N} \frac{\epsilon_{i}}{\left(\bar{z}-\bar{r}_{i}\right)^{2}}=e^{i \phi}
\end{equation}
for each value of the parameter $\phi\in[0,2\pi]$.
Taking the complex conjugate of the above equation we have
\begin{equation}
    \sum_{i=1}^{N} \frac{\epsilon_{i}}{\left(z-r_{i}\right)^{2}}=e^{-i \phi}
\end{equation}
We define polynomials $G'$ and $H'$
\begin{align}
    G' & = \sum_{j=1}^N\epsilon_j\prod_{i=1,\;i\neq j}^Nz^2_i \\
    H' & = \prod_{i=1}^Nz^2_i
\end{align}
and
\begin{equation}
    e^{-i\phi}=\frac{G'}{H'}
\end{equation}
Finally, we obtain the polynomial
\begin{equation}
    e^{-i\phi}H' - G' = 0
    \label{eq:complex_poly_critical}
\end{equation}
Equation~\ref{eq:complex_poly_critical} is a complex polynomial of degree $2N$. We can
obtain its coefficients with \textsf{SymPy} as before and their values are available
\href{https://github.com/fbartolic/caustics/blob/main/notebooks/ComplexPolynomialCoefficients.ipynb}{here}.

\bibliography{bib.bib}

\end{document}